{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PyTorch_uintopressure.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Apmaranca/NCMnaST/blob/gh-pages/PyTorch_uintopressure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXmCHcwKs6rd"
      },
      "source": [
        "# PyTorch method for tranform Lung pressure to uin fft\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs4RGzTw-gFJ"
      },
      "source": [
        "My first idea for approaching the problem posed in Google Brain - Ventilator Pressure Prediction was to find a way to circunvent the problem of trying to solve a tipical dynamic problem with static models, not time dependent. As I have a past in thermodynamics, it's natural to think about this ventilating system as a series of ordinary differential equations on pressure, evolving on time. \n",
        "To make things worst, the data set showed many series of pressure along time as independent values. That's exactly whats what we should do for a database, but it's very strange for a dynamical system. \n",
        "Thinking about the real problem of ventilating, we must know what pressure to apply on each moment, but it's not true that we don't know the entire pressure curve. We know the points until that moment and certainly it's not much to ask a first breath to calibrate the respirator, since it could get all the dymamical characteristics of the lung. \n",
        "If we approach the lung as a live functional lung, there as air entering the lung in the inspiration and there is no pressure increase in the lung, since it gets bigger exactly to make the air enter. \n",
        "Once we have a respirator, we pressupose some effort in letting the air in, that we could think divided in two energy expenditures. One elastic and one plastic deformation of the lung, using these terms losely, just to indicate that one fraction will be elastic, like a rubber ballon, ans will exert some work expanding, like a expanding spring, obeying a two dimmentional Hooke Law, that won't be very easy to model, but certainly have a dynamic character. This elastic work will be used to expel the air plus carbon dioxide out of the lungs. \n",
        "Another part of the energy used in the inspiration will be used to expand the lungs inelastic and may not be recovered in the expiration, neither will be described by any two dimmensional Hooke law. \n",
        "Anyway, for a physical model, there will be a very complex and dynamical model.\n",
        "Since we don't see any preocupation in controling a low pressure outlet for the expiration, we pressupose that the elastic fraction of the lungs expansion is dominant and just the inspiration is critical.  \n",
        "### From de dynamical lung behavior to the Fast Fourier Transform\n",
        "If one analises all the given pressure curves in the inspiration, we perceive that there are several types of typical behavior. This is state even in the competition description. There are several types of lungs, ages, sizes and elastic coeficients that results in very different curves. \n",
        "This was largely explored by the coleagues in clusterings of the curves. \n",
        "I don't think, however, that it's adequated to cluster the curves per se, the pressure along time curves, but its dynamical characteristics. \n",
        "Not all the dynamical characteristics was though as relevant too. We used a fast Fourier Transform to get all the frequencies from this pressure curves but we declared interest just in the low and medium frequency range. There are a lot of oscilations and vibrations of high frequency that we considered to be irrelevant for the ventilation, just catch frequencies of the order of tenths of minutes or less. Zero frequency, os constant pressure, is not of interest too. \n",
        "We buit this model here: https://www.kaggle.com/alfredomaranca/lightautoml-fft-powered-for-the-respirator as a first step for this approach. \n",
        "\n",
        "### How to use this fft approach\n",
        "\n",
        "There is a caveat in this approach for one to use it in this competition, since in the test data we don't have a pressure dinamics of the lungs, just the uin curve. We should have a curve of pressure for another respirator cycle of the same lung, but we have just uin data. \n",
        "The uin data, however, translates very well the dynamics of the pressure of the lung and there a good correlation of pressure fft and uin fft. \n",
        "Since we are data science freaks, let us translate uin fft for this lung (that we have) to pressure fft. Once given a data set, we get the entire uin curve, calculate it's fft and use this neural network to evaluate a pressure fft curve, that could be used in the model. \n",
        "In this notebook we present this fft transformation. \n",
        "\n",
        "### What is to be made. \n",
        "\n",
        "We need, now, to translate the test data using this neural network and incrementing the model with the low and medium frequencies that characterise the dynamical response of the lungs.\n",
        "I'm not sure I'll have time to do this, since I have another duties and spent already much time on this, but you are welcome to try and share the results. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzCCniVwNTdp"
      },
      "source": [
        "# Setting seeds to try and ensure we have the same results - this is not guaranteed across PyTorch releases.\n",
        "import torch\n",
        "import pandas as pd\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQLW-HL7_0pT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c433eeac-c80b-4522-bd15-7a218f4872fe"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PXrJ5JYpq34"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NQfgixJT27B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46945e27-937e-4f10-b09f-a058196a02ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCJzXv0OK1Bs"
      },
      "source": [
        "\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader , TensorDataset\n",
        "\n",
        "mean, std = (0.5,), (0.5,)\n",
        "\n",
        "# Create a transform and normalise data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                              ])\n",
        "\n",
        "# Download FMNIST training dataset and load training data\n",
        "images_pd = pd.read_csv('/content/drive/MyDrive/kaggle/input/ventilator-pressure-prediction/uin_sign.csv')\n",
        "labels_pd = pd.read_csv('/content/drive/MyDrive/kaggle/input/ventilator-pressure-prediction/pressure_sign.csv')\n",
        "X = images_pd[images_pd.sum(axis=1) != 0]\n",
        "y = labels_pd[labels_pd.sum(axis=1) != 0]\n",
        "results_to_learn = X.shape[0]\n",
        "tmpX = torch.tensor(X.values.astype(np.float32))\n",
        "tmpy = torch.tensor(y.values.astype(np.float32)) #dtype=torch.long, device=device\n",
        "dataset = TensorDataset(tmpX , tmpy)\n",
        "trainloader = DataLoader(dataset , shuffle=True) #batch_size = 16, desactivated\n",
        "#testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OfNTrTs3O4tB",
        "outputId": "e332f72c-988e-4032-93ae-abef58860d38"
      },
      "source": [
        "y\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.408682</td>\n",
              "      <td>0.258872</td>\n",
              "      <td>0.148300</td>\n",
              "      <td>0.089062</td>\n",
              "      <td>0.058699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.289840</td>\n",
              "      <td>0.142355</td>\n",
              "      <td>0.114043</td>\n",
              "      <td>0.071609</td>\n",
              "      <td>0.049044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.972773</td>\n",
              "      <td>0.958388</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.372466</td>\n",
              "      <td>0.226257</td>\n",
              "      <td>0.011190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.484864</td>\n",
              "      <td>0.120219</td>\n",
              "      <td>0.033307</td>\n",
              "      <td>0.012502</td>\n",
              "      <td>0.021616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.213998</td>\n",
              "      <td>0.105382</td>\n",
              "      <td>0.086002</td>\n",
              "      <td>0.040437</td>\n",
              "      <td>0.034681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.904156</td>\n",
              "      <td>0.524202</td>\n",
              "      <td>0.235788</td>\n",
              "      <td>0.116240</td>\n",
              "      <td>0.042056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.330925</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.120792</td>\n",
              "      <td>0.004581</td>\n",
              "      <td>0.059932</td>\n",
              "      <td>0.072586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.315690</td>\n",
              "      <td>0.122006</td>\n",
              "      <td>0.107037</td>\n",
              "      <td>0.072876</td>\n",
              "      <td>0.034781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.936061</td>\n",
              "      <td>0.588108</td>\n",
              "      <td>0.348418</td>\n",
              "      <td>0.108830</td>\n",
              "      <td>0.045605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.140950</td>\n",
              "      <td>0.078140</td>\n",
              "      <td>0.145025</td>\n",
              "      <td>0.376780</td>\n",
              "      <td>0.201388</td>\n",
              "      <td>0.885130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.269814</td>\n",
              "      <td>0.149951</td>\n",
              "      <td>0.082894</td>\n",
              "      <td>0.052120</td>\n",
              "      <td>0.045420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.251515</td>\n",
              "      <td>0.130205</td>\n",
              "      <td>0.078059</td>\n",
              "      <td>0.056047</td>\n",
              "      <td>0.047212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.355735</td>\n",
              "      <td>0.216602</td>\n",
              "      <td>0.169397</td>\n",
              "      <td>0.136152</td>\n",
              "      <td>0.092994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.294308</td>\n",
              "      <td>0.103292</td>\n",
              "      <td>0.221308</td>\n",
              "      <td>0.106779</td>\n",
              "      <td>0.028369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.201685</td>\n",
              "      <td>0.142083</td>\n",
              "      <td>0.061814</td>\n",
              "      <td>0.044769</td>\n",
              "      <td>0.030738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.652627</td>\n",
              "      <td>0.457578</td>\n",
              "      <td>0.338808</td>\n",
              "      <td>0.292626</td>\n",
              "      <td>0.204890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.338864</td>\n",
              "      <td>0.119985</td>\n",
              "      <td>0.096638</td>\n",
              "      <td>0.053549</td>\n",
              "      <td>0.090197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.164137</td>\n",
              "      <td>0.081706</td>\n",
              "      <td>0.146290</td>\n",
              "      <td>0.235060</td>\n",
              "      <td>0.006451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.286892</td>\n",
              "      <td>0.121507</td>\n",
              "      <td>0.066179</td>\n",
              "      <td>0.041920</td>\n",
              "      <td>0.037114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.262226</td>\n",
              "      <td>0.025621</td>\n",
              "      <td>0.033046</td>\n",
              "      <td>0.035734</td>\n",
              "      <td>0.024567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.363469</td>\n",
              "      <td>0.127646</td>\n",
              "      <td>0.058892</td>\n",
              "      <td>0.074532</td>\n",
              "      <td>0.051541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.184501</td>\n",
              "      <td>0.264119</td>\n",
              "      <td>0.113273</td>\n",
              "      <td>0.049636</td>\n",
              "      <td>0.266480</td>\n",
              "      <td>0.774213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.790142</td>\n",
              "      <td>0.688711</td>\n",
              "      <td>0.484812</td>\n",
              "      <td>0.257149</td>\n",
              "      <td>0.201798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.830830</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.525458</td>\n",
              "      <td>0.037386</td>\n",
              "      <td>0.042603</td>\n",
              "      <td>0.019789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.336901</td>\n",
              "      <td>0.203202</td>\n",
              "      <td>0.120170</td>\n",
              "      <td>0.086866</td>\n",
              "      <td>0.051956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.118675</td>\n",
              "      <td>0.008608</td>\n",
              "      <td>0.178481</td>\n",
              "      <td>0.113044</td>\n",
              "      <td>0.062554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.373060</td>\n",
              "      <td>0.234766</td>\n",
              "      <td>0.152276</td>\n",
              "      <td>0.111812</td>\n",
              "      <td>0.076294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.469613</td>\n",
              "      <td>0.321263</td>\n",
              "      <td>0.223824</td>\n",
              "      <td>0.162916</td>\n",
              "      <td>0.112268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.445890</td>\n",
              "      <td>0.346038</td>\n",
              "      <td>0.351638</td>\n",
              "      <td>0.421312</td>\n",
              "      <td>0.417451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.339090</td>\n",
              "      <td>0.282009</td>\n",
              "      <td>0.240096</td>\n",
              "      <td>0.302689</td>\n",
              "      <td>0.379496</td>\n",
              "      <td>0.667967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.319349</td>\n",
              "      <td>0.170037</td>\n",
              "      <td>0.096237</td>\n",
              "      <td>0.057835</td>\n",
              "      <td>0.047474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.076097</td>\n",
              "      <td>0.114144</td>\n",
              "      <td>0.105587</td>\n",
              "      <td>0.150434</td>\n",
              "      <td>0.238011</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.437098</td>\n",
              "      <td>0.179259</td>\n",
              "      <td>0.117867</td>\n",
              "      <td>0.084564</td>\n",
              "      <td>0.063585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.118262</td>\n",
              "      <td>0.014422</td>\n",
              "      <td>0.061227</td>\n",
              "      <td>0.010457</td>\n",
              "      <td>0.022817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.474498</td>\n",
              "      <td>0.837705</td>\n",
              "      <td>0.078221</td>\n",
              "      <td>0.245598</td>\n",
              "      <td>0.026975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.003519</td>\n",
              "      <td>0.060165</td>\n",
              "      <td>0.011374</td>\n",
              "      <td>0.007156</td>\n",
              "      <td>0.026601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.237186</td>\n",
              "      <td>0.072900</td>\n",
              "      <td>0.052678</td>\n",
              "      <td>0.037608</td>\n",
              "      <td>0.021355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.172753</td>\n",
              "      <td>0.113946</td>\n",
              "      <td>0.305234</td>\n",
              "      <td>0.350149</td>\n",
              "      <td>0.310903</td>\n",
              "      <td>0.557422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.340294</td>\n",
              "      <td>0.130723</td>\n",
              "      <td>0.068664</td>\n",
              "      <td>0.043536</td>\n",
              "      <td>0.038929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.899045</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.286489</td>\n",
              "      <td>0.046172</td>\n",
              "      <td>0.060580</td>\n",
              "      <td>0.055913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.347245</td>\n",
              "      <td>0.205609</td>\n",
              "      <td>0.221476</td>\n",
              "      <td>0.194266</td>\n",
              "      <td>0.166865</td>\n",
              "      <td>0.544508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.732623</td>\n",
              "      <td>0.619816</td>\n",
              "      <td>0.391930</td>\n",
              "      <td>0.244630</td>\n",
              "      <td>0.153897</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2         3         4         5\n",
              "1   1.000000  0.408682  0.258872  0.148300  0.089062  0.058699\n",
              "2   1.000000  0.289840  0.142355  0.114043  0.071609  0.049044\n",
              "3   0.972773  0.958388  1.000000  0.372466  0.226257  0.011190\n",
              "4   1.000000  0.484864  0.120219  0.033307  0.012502  0.021616\n",
              "5   1.000000  0.213998  0.105382  0.086002  0.040437  0.034681\n",
              "6   1.000000  0.904156  0.524202  0.235788  0.116240  0.042056\n",
              "7   0.330925  1.000000  0.120792  0.004581  0.059932  0.072586\n",
              "9   1.000000  0.315690  0.122006  0.107037  0.072876  0.034781\n",
              "10  1.000000  0.936061  0.588108  0.348418  0.108830  0.045605\n",
              "12  0.140950  0.078140  0.145025  0.376780  0.201388  0.885130\n",
              "13  1.000000  0.269814  0.149951  0.082894  0.052120  0.045420\n",
              "14  1.000000  0.251515  0.130205  0.078059  0.056047  0.047212\n",
              "15  1.000000  0.355735  0.216602  0.169397  0.136152  0.092994\n",
              "16  1.000000  0.294308  0.103292  0.221308  0.106779  0.028369\n",
              "17  1.000000  0.201685  0.142083  0.061814  0.044769  0.030738\n",
              "18  1.000000  0.652627  0.457578  0.338808  0.292626  0.204890\n",
              "19  1.000000  0.338864  0.119985  0.096638  0.053549  0.090197\n",
              "20  1.000000  0.164137  0.081706  0.146290  0.235060  0.006451\n",
              "21  1.000000  0.286892  0.121507  0.066179  0.041920  0.037114\n",
              "22  1.000000  0.262226  0.025621  0.033046  0.035734  0.024567\n",
              "23  1.000000  0.363469  0.127646  0.058892  0.074532  0.051541\n",
              "25  0.184501  0.264119  0.113273  0.049636  0.266480  0.774213\n",
              "26  1.000000  0.790142  0.688711  0.484812  0.257149  0.201798\n",
              "27  0.830830  1.000000  0.525458  0.037386  0.042603  0.019789\n",
              "28  1.000000  0.336901  0.203202  0.120170  0.086866  0.051956\n",
              "29  1.000000  0.118675  0.008608  0.178481  0.113044  0.062554\n",
              "30  1.000000  0.373060  0.234766  0.152276  0.111812  0.076294\n",
              "32  1.000000  0.469613  0.321263  0.223824  0.162916  0.112268\n",
              "34  1.000000  0.445890  0.346038  0.351638  0.421312  0.417451\n",
              "35  0.339090  0.282009  0.240096  0.302689  0.379496  0.667967\n",
              "36  1.000000  0.319349  0.170037  0.096237  0.057835  0.047474\n",
              "37  0.076097  0.114144  0.105587  0.150434  0.238011  1.000000\n",
              "39  1.000000  0.437098  0.179259  0.117867  0.084564  0.063585\n",
              "40  1.000000  0.118262  0.014422  0.061227  0.010457  0.022817\n",
              "41  1.000000  0.474498  0.837705  0.078221  0.245598  0.026975\n",
              "42  1.000000  0.003519  0.060165  0.011374  0.007156  0.026601\n",
              "43  1.000000  0.237186  0.072900  0.052678  0.037608  0.021355\n",
              "44  0.172753  0.113946  0.305234  0.350149  0.310903  0.557422\n",
              "46  1.000000  0.340294  0.130723  0.068664  0.043536  0.038929\n",
              "47  0.899045  1.000000  0.286489  0.046172  0.060580  0.055913\n",
              "48  0.347245  0.205609  0.221476  0.194266  0.166865  0.544508\n",
              "49  1.000000  0.732623  0.619816  0.391930  0.244630  0.153897"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5XPehWi75Df",
        "outputId": "a3aee966-aaf2-468a-a815-f9f53b5dae7e"
      },
      "source": [
        "trainloader\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f5578277b50>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ktL3Z3bu16g"
      },
      "source": [
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmVOCozI1IlF"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqMqFbIVrbFH"
      },
      "source": [
        "class FMNIST(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(6, 128)\n",
        "    self.fc2 = nn.Linear(128,64)\n",
        "    self.fc3 = nn.Linear(64,6)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    \n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    \n",
        "    return x\n",
        "\n",
        "model = FMNIST()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRtN96IBAK6k"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67eZUNEM5b7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32c4e67-c477-4525-812d-926d725550ce"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FMNIST(\n",
              "  (fc1): Linear(in_features=6, out_features=128, bias=True)\n",
              "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (fc3): Linear(in_features=64, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPdDu7KfWEfW"
      },
      "source": [
        "- The only change we have made to the code is that we are going to track the training loss, the testing loss and the accuracy across the 30 epochs.\n",
        "- We'll print out the train loss, the test loss and the accuracy after each epoch.\n",
        "- Because we are running this over 30 epochs this will take a bit longer to run - approx 15 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJLzWi0UqGWm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1863ab78-d909-47f9-e307-ecf6b9eeb86a"
      },
      "source": [
        "from torch import optim\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 3000\n",
        "train_tracker, test_tracker, accuracy_tracker = [], [], []\n",
        "\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    cum_loss = 0\n",
        "    \n",
        "    for batch, (images, labels) in enumerate(trainloader,1):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        #print(output)\n",
        "        #print(labels)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        cum_loss += loss.item()\n",
        "    \n",
        "    train_tracker.append(cum_loss/len(trainloader))\n",
        "    print(f\"Epoch({i+1}/{num_epochs}) | Training loss: {cum_loss/len(trainloader)} | \",end='')\n",
        "    \n",
        "    test_loss = 0\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch(1/3000) | Training loss: 2.122975394839332 | Epoch(2/3000) | Training loss: 2.1224980439458574 | Epoch(3/3000) | Training loss: 2.1221636675653004 | Epoch(4/3000) | Training loss: 2.121930102507273 | Epoch(5/3000) | Training loss: 2.1217665331704274 | Epoch(6/3000) | Training loss: 2.1216505765914917 | Epoch(7/3000) | Training loss: 2.1215680156435286 | Epoch(8/3000) | Training loss: 2.1215092710086276 | Epoch(9/3000) | Training loss: 2.121467351913452 | Epoch(10/3000) | Training loss: 2.121437191963196 | Epoch(11/3000) | Training loss: 2.121415300028665 | Epoch(12/3000) | Training loss: 2.1213991869063604 | Epoch(13/3000) | Training loss: 2.121387322743734 | Epoch(14/3000) | Training loss: 2.12137838772365 | Epoch(15/3000) | Training loss: 2.121371473584856 | Epoch(16/3000) | Training loss: 2.12136618580137 | Epoch(17/3000) | Training loss: 2.121361928326743 | Epoch(18/3000) | Training loss: 2.1213584599040805 | Epoch(19/3000) | Training loss: 2.121355584689549 | Epoch(20/3000) | Training loss: 2.1213531522523787 | Epoch(21/3000) | Training loss: 2.1213509837786355 | Epoch(22/3000) | Training loss: 2.12134907075337 | Epoch(23/3000) | Training loss: 2.1213473847934177 | Epoch(24/3000) | Training loss: 2.121345783982958 | Epoch(25/3000) | Training loss: 2.121344342118218 | Epoch(26/3000) | Training loss: 2.121342959858122 | Epoch(27/3000) | Training loss: 2.1213416173344566 | Epoch(28/3000) | Training loss: 2.1213403855051314 | Epoch(29/3000) | Training loss: 2.1213391366459073 | Epoch(30/3000) | Training loss: 2.1213379757744923 | Epoch(31/3000) | Training loss: 2.1213368404479254 | Epoch(32/3000) | Training loss: 2.1213357278278897 | Epoch(33/3000) | Training loss: 2.121334598177955 | Epoch(34/3000) | Training loss: 2.1213335934139432 | Epoch(35/3000) | Training loss: 2.121332509177072 | Epoch(36/3000) | Training loss: 2.1213314533233643 | Epoch(37/3000) | Training loss: 2.121330425852821 | Epoch(38/3000) | Training loss: 2.121329489208403 | Epoch(39/3000) | Training loss: 2.1213284759294417 | Epoch(40/3000) | Training loss: 2.121327519416809 | Epoch(41/3000) | Training loss: 2.1213265515509105 | Epoch(42/3000) | Training loss: 2.1213256149064925 | Epoch(43/3000) | Training loss: 2.121324692453657 | Epoch(44/3000) | Training loss: 2.1213237387793407 | Epoch(45/3000) | Training loss: 2.121322813488188 | Epoch(46/3000) | Training loss: 2.1213219534783136 | Epoch(47/3000) | Training loss: 2.1213210991450717 | Epoch(48/3000) | Training loss: 2.1213201852071855 | Epoch(49/3000) | Training loss: 2.121319367772057 | Epoch(50/3000) | Training loss: 2.121318470864069 | Epoch(51/3000) | Training loss: 2.121317664782206 | Epoch(52/3000) | Training loss: 2.121316830317179 | Epoch(53/3000) | Training loss: 2.1213160441035317 | Epoch(54/3000) | Training loss: 2.12131530046463 | Epoch(55/3000) | Training loss: 2.121314522765932 | Epoch(56/3000) | Training loss: 2.121313784803663 | Epoch(57/3000) | Training loss: 2.1213130411647616 | Epoch(58/3000) | Training loss: 2.1213123628071378 | Epoch(59/3000) | Training loss: 2.1213116333598183 | Epoch(60/3000) | Training loss: 2.1213108953975497 | Epoch(61/3000) | Training loss: 2.1213102227165583 | Epoch(62/3000) | Training loss: 2.121309586933681 | Epoch(63/3000) | Training loss: 2.12130891425269 | Epoch(64/3000) | Training loss: 2.1213082472483316 | Epoch(65/3000) | Training loss: 2.1213075802439736 | Epoch(66/3000) | Training loss: 2.1213069558143616 | Epoch(67/3000) | Training loss: 2.12130629164832 | Epoch(68/3000) | Training loss: 2.1213057097934542 | Epoch(69/3000) | Training loss: 2.121305125100272 | Epoch(70/3000) | Training loss: 2.1213045233771917 | Epoch(71/3000) | Training loss: 2.1213039386840093 | Epoch(72/3000) | Training loss: 2.121303317092714 | Epoch(73/3000) | Training loss: 2.121302780650911 | Epoch(74/3000) | Training loss: 2.1213022073109946 | Epoch(75/3000) | Training loss: 2.121301676545824 | Epoch(76/3000) | Training loss: 2.121301074822744 | Epoch(77/3000) | Training loss: 2.1213005554108393 | Epoch(78/3000) | Training loss: 2.1213000274839855 | Epoch(79/3000) | Training loss: 2.1212994882038663 | Epoch(80/3000) | Training loss: 2.1212989631153287 | Epoch(81/3000) | Training loss: 2.1212984806015376 | Epoch(82/3000) | Training loss: 2.1212979895727977 | Epoch(83/3000) | Training loss: 2.1212974843524752 | Epoch(84/3000) | Training loss: 2.121296981970469 | Epoch(85/3000) | Training loss: 2.1212964937800454 | Epoch(86/3000) | Training loss: 2.121296002751305 | Epoch(87/3000) | Training loss: 2.1212955060459318 | Epoch(88/3000) | Training loss: 2.121295017855508 | Epoch(89/3000) | Training loss: 2.12129461197626 | Epoch(90/3000) | Training loss: 2.1212941095942544 | Epoch(91/3000) | Training loss: 2.1212936639785767 | Epoch(92/3000) | Training loss: 2.121293218362899 | Epoch(93/3000) | Training loss: 2.1212927954537526 | Epoch(94/3000) | Training loss: 2.121292312939962 | Epoch(95/3000) | Training loss: 2.1212918957074485 | Epoch(96/3000) | Training loss: 2.1212914898282005 | Epoch(97/3000) | Training loss: 2.1212910413742065 | Epoch(98/3000) | Training loss: 2.121290672393072 | Epoch(99/3000) | Training loss: 2.1212902267773948 | Epoch(100/3000) | Training loss: 2.1212898208981468 | Epoch(101/3000) | Training loss: 2.1212894519170127 | Epoch(102/3000) | Training loss: 2.121289020492917 | Epoch(103/3000) | Training loss: 2.1212886628650485 | Epoch(104/3000) | Training loss: 2.121288237117586 | Epoch(105/3000) | Training loss: 2.1212878681364513 | Epoch(106/3000) | Training loss: 2.1212874821254184 | Epoch(107/3000) | Training loss: 2.121287138689132 | Epoch(108/3000) | Training loss: 2.1212867299715676 | Epoch(109/3000) | Training loss: 2.121286383696965 | Epoch(110/3000) | Training loss: 2.121286017554147 | Epoch(111/3000) | Training loss: 2.1212856684412276 | Epoch(112/3000) | Training loss: 2.121285313651675 | Epoch(113/3000) | Training loss: 2.1212849645387557 | Epoch(114/3000) | Training loss: 2.121284632455735 | Epoch(115/3000) | Training loss: 2.121284320240929 | Epoch(116/3000) | Training loss: 2.121283959774744 | Epoch(117/3000) | Training loss: 2.121283599308559 | Epoch(118/3000) | Training loss: 2.1212832842554366 | Epoch(119/3000) | Training loss: 2.1212829379808333 | Epoch(120/3000) | Training loss: 2.1212826569875083 | Epoch(121/3000) | Training loss: 2.1212822936830067 | Epoch(122/3000) | Training loss: 2.1212819928214666 | Epoch(123/3000) | Training loss: 2.121281672091711 | Epoch(124/3000) | Training loss: 2.121281393936702 | Epoch(125/3000) | Training loss: 2.121281067530314 | Epoch(126/3000) | Training loss: 2.1212807354472933 | Epoch(127/3000) | Training loss: 2.1212804175558544 | Epoch(128/3000) | Training loss: 2.1212801450774785 | Epoch(129/3000) | Training loss: 2.1212798186710904 | Epoch(130/3000) | Training loss: 2.121279532001132 | Epoch(131/3000) | Training loss: 2.121279191403162 | Epoch(132/3000) | Training loss: 2.121278938793001 | Epoch(133/3000) | Training loss: 2.121278615224929 | Epoch(134/3000) | Training loss: 2.1212783313932873 | Epoch(135/3000) | Training loss: 2.121278010663532 | Epoch(136/3000) | Training loss: 2.1212777353468395 | Epoch(137/3000) | Training loss: 2.121277468545096 | Epoch(138/3000) | Training loss: 2.1212771903900873 | Epoch(139/3000) | Training loss: 2.1212769122350785 | Epoch(140/3000) | Training loss: 2.12127662556512 | Epoch(141/3000) | Training loss: 2.1212763474101113 | Epoch(142/3000) | Training loss: 2.1212761061532155 | Epoch(143/3000) | Training loss: 2.1212758336748396 | Epoch(144/3000) | Training loss: 2.1212755895796276 | Epoch(145/3000) | Training loss: 2.1212752915564037 | Epoch(146/3000) | Training loss: 2.1212750502995084 | Epoch(147/3000) | Training loss: 2.1212748147192455 | Epoch(148/3000) | Training loss: 2.121274550755819 | Epoch(149/3000) | Training loss: 2.1212742924690247 | Epoch(150/3000) | Training loss: 2.1212740341822305 | Epoch(151/3000) | Training loss: 2.1212738298234486 | Epoch(152/3000) | Training loss: 2.1212735828899203 | Epoch(153/3000) | Training loss: 2.1212732905433294 | Epoch(154/3000) | Training loss: 2.1212730634780157 | Epoch(155/3000) | Training loss: 2.121272853442601 | Epoch(156/3000) | Training loss: 2.121272623538971 | Epoch(157/3000) | Training loss: 2.121272379443759 | Epoch(158/3000) | Training loss: 2.121272155216762 | Epoch(159/3000) | Training loss: 2.121271939504714 | Epoch(160/3000) | Training loss: 2.1212716755412875 | Epoch(161/3000) | Training loss: 2.1212714399610246 | Epoch(162/3000) | Training loss: 2.1212712128957114 | Epoch(163/3000) | Training loss: 2.121271005698613 | Epoch(164/3000) | Training loss: 2.121270755926768 | Epoch(165/3000) | Training loss: 2.121270560082935 | Epoch(166/3000) | Training loss: 2.1212703727540516 | Epoch(167/3000) | Training loss: 2.1212701229822066 | Epoch(168/3000) | Training loss: 2.121269907270159 | Epoch(169/3000) | Training loss: 2.1212697199412753 | Epoch(170/3000) | Training loss: 2.121269532612392 | Epoch(171/3000) | Training loss: 2.1212692941938127 | Epoch(172/3000) | Training loss: 2.121269078481765 | Epoch(173/3000) | Training loss: 2.1212688911528814 | Epoch(174/3000) | Training loss: 2.12126863854272 | Epoch(175/3000) | Training loss: 2.1212685023035323 | Epoch(176/3000) | Training loss: 2.1212682752382186 | Epoch(177/3000) | Training loss: 2.1212680623644875 | Epoch(178/3000) | Training loss: 2.121267889227186 | Epoch(179/3000) | Training loss: 2.1212676820300875 | Epoch(180/3000) | Training loss: 2.1212675174077353 | Epoch(181/3000) | Training loss: 2.1212673158872697 | Epoch(182/3000) | Training loss: 2.1212671257200695 | Epoch(183/3000) | Training loss: 2.1212669667743502 | Epoch(184/3000) | Training loss: 2.121266785122099 | Epoch(185/3000) | Training loss: 2.1212666176614308 | Epoch(186/3000) | Training loss: 2.121266416140965 | Epoch(187/3000) | Training loss: 2.121266260033562 | Epoch(188/3000) | Training loss: 2.121266081219628 | Epoch(189/3000) | Training loss: 2.1212658825374784 | Epoch(190/3000) | Training loss: 2.1212657548132396 | Epoch(191/3000) | Training loss: 2.121265519232977 | Epoch(192/3000) | Training loss: 2.1212653716405234 | Epoch(193/3000) | Training loss: 2.121265178635007 | Epoch(194/3000) | Training loss: 2.12126502536592 | Epoch(195/3000) | Training loss: 2.1212648437136696 | Epoch(196/3000) | Training loss: 2.121264656384786 | Epoch(197/3000) | Training loss: 2.121264471894219 | Epoch(198/3000) | Training loss: 2.121264321463449 | Epoch(199/3000) | Training loss: 2.1212641511644637 | Epoch(200/3000) | Training loss: 2.1212639723505293 | Epoch(201/3000) | Training loss: 2.1212637906982783 | Epoch(202/3000) | Training loss: 2.1212636686506725 | Epoch(203/3000) | Training loss: 2.1212635238965354 | Epoch(204/3000) | Training loss: 2.1212633223760697 | Epoch(205/3000) | Training loss: 2.1212631946518306 | Epoch(206/3000) | Training loss: 2.1212630413827442 | Epoch(207/3000) | Training loss: 2.1212628455389115 | Epoch(208/3000) | Training loss: 2.1212627178146723 | Epoch(209/3000) | Training loss: 2.121262539000738 | Epoch(210/3000) | Training loss: 2.1212623914082847 | Epoch(211/3000) | Training loss: 2.121262272198995 | Epoch(212/3000) | Training loss: 2.121262110414959 | Epoch(213/3000) | Training loss: 2.1212619855290367 | Epoch(214/3000) | Training loss: 2.1212618265833174 | Epoch(215/3000) | Training loss: 2.1212616903441295 | Epoch(216/3000) | Training loss: 2.121261554104941 | Epoch(217/3000) | Training loss: 2.1212614263807024 | Epoch(218/3000) | Training loss: 2.1212612504050847 | Epoch(219/3000) | Training loss: 2.1212610715911504 | Epoch(220/3000) | Training loss: 2.1212609977949235 | Epoch(221/3000) | Training loss: 2.1212608700706843 | Epoch(222/3000) | Training loss: 2.121260708286649 | Epoch(223/3000) | Training loss: 2.1212605578558787 | Epoch(224/3000) | Training loss: 2.1212604329699563 | Epoch(225/3000) | Training loss: 2.1212602882158187 | Epoch(226/3000) | Training loss: 2.121260169006529 | Epoch(227/3000) | Training loss: 2.121260086695353 | Epoch(228/3000) | Training loss: 2.121259902204786 | Epoch(229/3000) | Training loss: 2.121259771642231 | Epoch(230/3000) | Training loss: 2.1212596808161055 | Epoch(231/3000) | Training loss: 2.121259507678804 | Epoch(232/3000) | Training loss: 2.121259391307831 | Epoch(233/3000) | Training loss: 2.1212592862901234 | Epoch(234/3000) | Training loss: 2.121259167080834 | Epoch(235/3000) | Training loss: 2.1212590592248097 | Epoch(236/3000) | Training loss: 2.1212589343388877 | Epoch(237/3000) | Training loss: 2.1212588293211803 | Epoch(238/3000) | Training loss: 2.121258693081992 | Epoch(239/3000) | Training loss: 2.121258590902601 | Epoch(240/3000) | Training loss: 2.121258480208261 | Epoch(241/3000) | Training loss: 2.1212583751905534 | Epoch(242/3000) | Training loss: 2.1212582332747325 | Epoch(243/3000) | Training loss: 2.1212581254187084 | Epoch(244/3000) | Training loss: 2.1212580232393172 | Epoch(245/3000) | Training loss: 2.1212579295748757 | Epoch(246/3000) | Training loss: 2.121257784820738 | Epoch(247/3000) | Training loss: 2.1212576684497653 | Epoch(248/3000) | Training loss: 2.121257586138589 | Epoch(249/3000) | Training loss: 2.121257478282565 | Epoch(250/3000) | Training loss: 2.1212573590732755 | Epoch(251/3000) | Training loss: 2.1212572909536815 | Epoch(252/3000) | Training loss: 2.1212571745827082 | Epoch(253/3000) | Training loss: 2.121257069565001 | Epoch(254/3000) | Training loss: 2.1212569759005593 | Epoch(255/3000) | Training loss: 2.121256870882852 | Epoch(256/3000) | Training loss: 2.121256785733359 | Epoch(257/3000) | Training loss: 2.121256669362386 | Epoch(258/3000) | Training loss: 2.1212565842128934 | Epoch(259/3000) | Training loss: 2.1212564508120217 | Epoch(260/3000) | Training loss: 2.121256362824213 | Epoch(261/3000) | Training loss: 2.1212562833513533 | Epoch(262/3000) | Training loss: 2.121256184010279 | Epoch(263/3000) | Training loss: 2.121256093184153 | Epoch(264/3000) | Training loss: 2.1212560080346607 | Epoch(265/3000) | Training loss: 2.1212559200468517 | Epoch(266/3000) | Training loss: 2.1212558292207264 | Epoch(267/3000) | Training loss: 2.1212557213647023 | Epoch(268/3000) | Training loss: 2.121255622023628 | Epoch(269/3000) | Training loss: 2.121255559580667 | Epoch(270/3000) | Training loss: 2.121255460239592 | Epoch(271/3000) | Training loss: 2.1212553608985174 | Epoch(272/3000) | Training loss: 2.1212552842639743 | Epoch(273/3000) | Training loss: 2.121255196276165 | Epoch(274/3000) | Training loss: 2.1212550827435086 | Epoch(275/3000) | Training loss: 2.1212550089472817 | Epoch(276/3000) | Training loss: 2.121254923797789 | Epoch(277/3000) | Training loss: 2.1212548045884994 | Epoch(278/3000) | Training loss: 2.1212547364689054 | Epoch(279/3000) | Training loss: 2.121254656996046 | Epoch(280/3000) | Training loss: 2.121254551978338 | Epoch(281/3000) | Training loss: 2.1212544554755803 | Epoch(282/3000) | Training loss: 2.121254398709252 | Epoch(283/3000) | Training loss: 2.121254324913025 | Epoch(284/3000) | Training loss: 2.1212542085420516 | Epoch(285/3000) | Training loss: 2.1212541233925593 | Epoch(286/3000) | Training loss: 2.1212540694645474 | Epoch(287/3000) | Training loss: 2.121253972961789 | Epoch(288/3000) | Training loss: 2.1212538991655623 | Epoch(289/3000) | Training loss: 2.121253791309538 | Epoch(290/3000) | Training loss: 2.1212537033217296 | Epoch(291/3000) | Training loss: 2.121253640878768 | Epoch(292/3000) | Training loss: 2.1212535755974904 | Epoch(293/3000) | Training loss: 2.1212534450349354 | Epoch(294/3000) | Training loss: 2.1212534052985057 | Epoch(295/3000) | Training loss: 2.1212533229873296 | Epoch(296/3000) | Training loss: 2.1212533031191145 | Epoch(297/3000) | Training loss: 2.1212531867481412 | Epoch(298/3000) | Training loss: 2.1212530590239025 | Epoch(299/3000) | Training loss: 2.12125301361084 | Epoch(300/3000) | Training loss: 2.1212529171080816 | Epoch(301/3000) | Training loss: 2.12125286318007 | Epoch(302/3000) | Training loss: 2.121252795060476 | Epoch(303/3000) | Training loss: 2.121252718425932 | Epoch(304/3000) | Training loss: 2.121252641791389 | Epoch(305/3000) | Training loss: 2.1212525481269475 | Epoch(306/3000) | Training loss: 2.121252505552201 | Epoch(307/3000) | Training loss: 2.1212524204027083 | Epoch(308/3000) | Training loss: 2.1212523636363803 | Epoch(309/3000) | Training loss: 2.1212523125466847 | Epoch(310/3000) | Training loss: 2.1212521961757114 | Epoch(311/3000) | Training loss: 2.1212521479243325 | Epoch(312/3000) | Training loss: 2.1212520627748397 | Epoch(313/3000) | Training loss: 2.1212519861402965 | Epoch(314/3000) | Training loss: 2.1212519236973355 | Epoch(315/3000) | Training loss: 2.1212518839609054 | Epoch(316/3000) | Training loss: 2.1212518271945773 | Epoch(317/3000) | Training loss: 2.1212517136619207 | Epoch(318/3000) | Training loss: 2.1212516796021235 | Epoch(319/3000) | Training loss: 2.121251594452631 | Epoch(320/3000) | Training loss: 2.1212515717460994 | Epoch(321/3000) | Training loss: 2.1212514695667086 | Epoch(322/3000) | Training loss: 2.1212513759022666 | Epoch(323/3000) | Training loss: 2.1212513134593056 | Epoch(324/3000) | Training loss: 2.1212512850761414 | Epoch(325/3000) | Training loss: 2.1212512368247625 | Epoch(326/3000) | Training loss: 2.121251180058434 | Epoch(327/3000) | Training loss: 2.1212510353042964 | Epoch(328/3000) | Training loss: 2.1212510608491444 | Epoch(329/3000) | Training loss: 2.1212509529931203 | Epoch(330/3000) | Training loss: 2.1212509217716398 | Epoch(331/3000) | Training loss: 2.1212508337838307 | Epoch(332/3000) | Training loss: 2.121250808238983 | Epoch(333/3000) | Training loss: 2.121250720251174 | Epoch(334/3000) | Training loss: 2.121250646454947 | Epoch(335/3000) | Training loss: 2.12125058117367 | Epoch(336/3000) | Training loss: 2.1212505187307085 | Epoch(337/3000) | Training loss: 2.121250467641013 | Epoch(338/3000) | Training loss: 2.121250345593407 | Epoch(339/3000) | Training loss: 2.121250339916774 | Epoch(340/3000) | Training loss: 2.1212503001803444 | Epoch(341/3000) | Training loss: 2.1212502348990667 | Epoch(342/3000) | Training loss: 2.121250183809371 | Epoch(343/3000) | Training loss: 2.121250112851461 | Epoch(344/3000) | Training loss: 2.1212500504084995 | Epoch(345/3000) | Training loss: 2.121249999318804 | Epoch(346/3000) | Training loss: 2.1212499624206904 | Epoch(347/3000) | Training loss: 2.1212499226842607 | Epoch(348/3000) | Training loss: 2.121249857402983 | Epoch(349/3000) | Training loss: 2.121249814828237 | Epoch(350/3000) | Training loss: 2.121249735355377 | Epoch(351/3000) | Training loss: 2.121249692780631 | Epoch(352/3000) | Training loss: 2.121249644529252 | Epoch(353/3000) | Training loss: 2.121249562218076 | Epoch(354/3000) | Training loss: 2.1212495309965953 | Epoch(355/3000) | Training loss: 2.121249496936798 | Epoch(356/3000) | Training loss: 2.1212493975957236 | Epoch(357/3000) | Training loss: 2.1212493635359264 | Epoch(358/3000) | Training loss: 2.1212492954163324 | Epoch(359/3000) | Training loss: 2.121249244326637 | Epoch(360/3000) | Training loss: 2.1212492159434726 | Epoch(361/3000) | Training loss: 2.121249196075258 | Epoch(362/3000) | Training loss: 2.121249136470613 | Epoch(363/3000) | Training loss: 2.1212490655127025 | Epoch(364/3000) | Training loss: 2.12124902009964 | Epoch(365/3000) | Training loss: 2.1212489775248935 | Epoch(366/3000) | Training loss: 2.1212489150819325 | Epoch(367/3000) | Training loss: 2.1212488895370845 | Epoch(368/3000) | Training loss: 2.1212488185791742 | Epoch(369/3000) | Training loss: 2.1212487930343267 | Epoch(370/3000) | Training loss: 2.1212487731661116 | Epoch(371/3000) | Training loss: 2.1212486936932518 | Epoch(372/3000) | Training loss: 2.121248642603556 | Epoch(373/3000) | Training loss: 2.1212486085437594 | Epoch(374/3000) | Training loss: 2.1212485432624817 | Epoch(375/3000) | Training loss: 2.1212485006877353 | Epoch(376/3000) | Training loss: 2.1212484467597235 | Epoch(377/3000) | Training loss: 2.1212484070232938 | Epoch(378/3000) | Training loss: 2.121248395670028 | Epoch(379/3000) | Training loss: 2.121248355933598 | Epoch(380/3000) | Training loss: 2.1212482849756875 | Epoch(381/3000) | Training loss: 2.1212482565925237 | Epoch(382/3000) | Training loss: 2.121248233885992 | Epoch(383/3000) | Training loss: 2.121248171443031 | Epoch(384/3000) | Training loss: 2.1212481203533353 | Epoch(385/3000) | Training loss: 2.1212480550720576 | Epoch(386/3000) | Training loss: 2.121248023850577 | Epoch(387/3000) | Training loss: 2.1212479897907803 | Epoch(388/3000) | Training loss: 2.121247941539401 | Epoch(389/3000) | Training loss: 2.121247884773073 | Epoch(390/3000) | Training loss: 2.1212478365216936 | Epoch(391/3000) | Training loss: 2.1212478365216936 | Epoch(392/3000) | Training loss: 2.1212477882703147 | Epoch(393/3000) | Training loss: 2.121247722989037 | Epoch(394/3000) | Training loss: 2.121247731503986 | Epoch(395/3000) | Training loss: 2.1212476775759743 | Epoch(396/3000) | Training loss: 2.121247620809646 | Epoch(397/3000) | Training loss: 2.121247575396583 | Epoch(398/3000) | Training loss: 2.121247547013419 | Epoch(399/3000) | Training loss: 2.121247493085407 | Epoch(400/3000) | Training loss: 2.1212474476723444 | Epoch(401/3000) | Training loss: 2.121247430642446 | Epoch(402/3000) | Training loss: 2.121247405097598 | Epoch(403/3000) | Training loss: 2.1212473937443326 | Epoch(404/3000) | Training loss: 2.1212473256247386 | Epoch(405/3000) | Training loss: 2.1212472716967263 | Epoch(406/3000) | Training loss: 2.1212472518285117 | Epoch(407/3000) | Training loss: 2.121247166679019 | Epoch(408/3000) | Training loss: 2.1212472035771324 | Epoch(409/3000) | Training loss: 2.1212471439724876 | Epoch(410/3000) | Training loss: 2.12124707869121 | Epoch(411/3000) | Training loss: 2.1212470531463623 | Epoch(412/3000) | Training loss: 2.121247016248249 | Epoch(413/3000) | Training loss: 2.121246953805288 | Epoch(414/3000) | Training loss: 2.121246956643604 | Epoch(415/3000) | Training loss: 2.1212468998772755 | Epoch(416/3000) | Training loss: 2.121246894200643 | Epoch(417/3000) | Training loss: 2.1212468573025296 | Epoch(418/3000) | Training loss: 2.1212468090511503 | Epoch(419/3000) | Training loss: 2.1212467835063027 | Epoch(420/3000) | Training loss: 2.1212467664764043 | Epoch(421/3000) | Training loss: 2.121246712548392 | Epoch(422/3000) | Training loss: 2.1212466784885953 | Epoch(423/3000) | Training loss: 2.1212466188839505 | Epoch(424/3000) | Training loss: 2.1212465848241533 | Epoch(425/3000) | Training loss: 2.121246542249407 | Epoch(426/3000) | Training loss: 2.1212465223811923 | Epoch(427/3000) | Training loss: 2.1212464968363443 | Epoch(428/3000) | Training loss: 2.121246482644762 | Epoch(429/3000) | Training loss: 2.1212464287167503 | Epoch(430/3000) | Training loss: 2.121246386142004 | Epoch(431/3000) | Training loss: 2.1212463889803206 | Epoch(432/3000) | Training loss: 2.121246326537359 | Epoch(433/3000) | Training loss: 2.1212463151840937 | Epoch(434/3000) | Training loss: 2.1212462924775624 | Epoch(435/3000) | Training loss: 2.1212462243579684 | Epoch(436/3000) | Training loss: 2.1212462271962846 | Epoch(437/3000) | Training loss: 2.1212461846215382 | Epoch(438/3000) | Training loss: 2.1212461647533236 | Epoch(439/3000) | Training loss: 2.1212461165019443 | Epoch(440/3000) | Training loss: 2.121246099472046 | Epoch(441/3000) | Training loss: 2.1212460568972995 | Epoch(442/3000) | Training loss: 2.121246051220667 | Epoch(443/3000) | Training loss: 2.1212459859393893 | Epoch(444/3000) | Training loss: 2.1212459972926547 | Epoch(445/3000) | Training loss: 2.121245957556225 | Epoch(446/3000) | Training loss: 2.1212458979515803 | Epoch(447/3000) | Training loss: 2.1212459121431624 | Epoch(448/3000) | Training loss: 2.1212458582151505 | Epoch(449/3000) | Training loss: 2.1212458241553533 | Epoch(450/3000) | Training loss: 2.12124578725724 | Epoch(451/3000) | Training loss: 2.121245792933873 | Epoch(452/3000) | Training loss: 2.121245721975962 | Epoch(453/3000) | Training loss: 2.1212456822395325 | Epoch(454/3000) | Training loss: 2.121245704946064 | Epoch(455/3000) | Training loss: 2.121245653856368 | Epoch(456/3000) | Training loss: 2.121245639664786 | Epoch(457/3000) | Training loss: 2.121245611281622 | Epoch(458/3000) | Training loss: 2.1212455857367742 | Epoch(459/3000) | Training loss: 2.1212455204554965 | Epoch(460/3000) | Training loss: 2.121245475042434 | Epoch(461/3000) | Training loss: 2.121245494910649 | Epoch(462/3000) | Training loss: 2.121245475042434 | Epoch(463/3000) | Training loss: 2.121245440982637 | Epoch(464/3000) | Training loss: 2.12124540692284 | Epoch(465/3000) | Training loss: 2.12124540692284 | Epoch(466/3000) | Training loss: 2.12124536718641 | Epoch(467/3000) | Training loss: 2.1212453217733476 | Epoch(468/3000) | Training loss: 2.1212452933901833 | Epoch(469/3000) | Training loss: 2.121245284875234 | Epoch(470/3000) | Training loss: 2.121245236623855 | Epoch(471/3000) | Training loss: 2.121245182695843 | Epoch(472/3000) | Training loss: 2.121245185534159 | Epoch(473/3000) | Training loss: 2.12124517701921 | Epoch(474/3000) | Training loss: 2.121245123091198 | Epoch(475/3000) | Training loss: 2.121245100384667 | Epoch(476/3000) | Training loss: 2.1212450975463506 | Epoch(477/3000) | Training loss: 2.121245046456655 | Epoch(478/3000) | Training loss: 2.121245035103389 | Epoch(479/3000) | Training loss: 2.1212449925286427 | Epoch(480/3000) | Training loss: 2.1212449840136935 | Epoch(481/3000) | Training loss: 2.1212449754987444 | Epoch(482/3000) | Training loss: 2.121244932923998 | Epoch(483/3000) | Training loss: 2.1212449102174666 | Epoch(484/3000) | Training loss: 2.121244878995986 | Epoch(485/3000) | Training loss: 2.121244870481037 | Epoch(486/3000) | Training loss: 2.1212448250679743 | Epoch(487/3000) | Training loss: 2.121244810876392 | Epoch(488/3000) | Training loss: 2.121244802361443 | Epoch(489/3000) | Training loss: 2.1212447796549116 | Epoch(490/3000) | Training loss: 2.121244742756798 | Epoch(491/3000) | Training loss: 2.1212447370801653 | Epoch(492/3000) | Training loss: 2.121244734241849 | Epoch(493/3000) | Training loss: 2.121244694505419 | Epoch(494/3000) | Training loss: 2.1212446831521534 | Epoch(495/3000) | Training loss: 2.121244634900774 | Epoch(496/3000) | Training loss: 2.1212445752961293 | Epoch(497/3000) | Training loss: 2.121244592326028 | Epoch(498/3000) | Training loss: 2.1212445469129655 | Epoch(499/3000) | Training loss: 2.1212445327213834 | Epoch(500/3000) | Training loss: 2.121244524206434 | Epoch(501/3000) | Training loss: 2.12124449582327 | Epoch(502/3000) | Training loss: 2.1212444504102073 | Epoch(503/3000) | Training loss: 2.1212444589251565 | Epoch(504/3000) | Training loss: 2.1212444390569414 | Epoch(505/3000) | Training loss: 2.121244402158828 | Epoch(506/3000) | Training loss: 2.121244387967246 | Epoch(507/3000) | Training loss: 2.1212443453925 | Epoch(508/3000) | Training loss: 2.121244334039234 | Epoch(509/3000) | Training loss: 2.1212443170093356 | Epoch(510/3000) | Training loss: 2.1212443028177534 | Epoch(511/3000) | Training loss: 2.1212442914644876 | Epoch(512/3000) | Training loss: 2.1212442205065773 | Epoch(513/3000) | Training loss: 2.121244240374792 | Epoch(514/3000) | Training loss: 2.1212442176682607 | Epoch(515/3000) | Training loss: 2.1212441949617293 | Epoch(516/3000) | Training loss: 2.1212441523869834 | Epoch(517/3000) | Training loss: 2.1212441552252996 | Epoch(518/3000) | Training loss: 2.121244129680452 | Epoch(519/3000) | Training loss: 2.121244095620655 | Epoch(520/3000) | Training loss: 2.121244055884225 | Epoch(521/3000) | Training loss: 2.1212440615608577 | Epoch(522/3000) | Training loss: 2.1212439991178966 | Epoch(523/3000) | Training loss: 2.121243987764631 | Epoch(524/3000) | Training loss: 2.1212439820879982 | Epoch(525/3000) | Training loss: 2.121243967896416 | Epoch(526/3000) | Training loss: 2.1212439707347324 | Epoch(527/3000) | Training loss: 2.1212439281599864 | Epoch(528/3000) | Training loss: 2.121243894100189 | Epoch(529/3000) | Training loss: 2.1212439139684043 | Epoch(530/3000) | Training loss: 2.12124388558524 | Epoch(531/3000) | Training loss: 2.121243860040392 | Epoch(532/3000) | Training loss: 2.121243817465646 | Epoch(533/3000) | Training loss: 2.12124380611238 | Epoch(534/3000) | Training loss: 2.121243783405849 | Epoch(535/3000) | Training loss: 2.1212437606993175 | Epoch(536/3000) | Training loss: 2.12124380611238 | Epoch(537/3000) | Training loss: 2.12124373515447 | Epoch(538/3000) | Training loss: 2.1212436925797236 | Epoch(539/3000) | Training loss: 2.12124369541804 | Epoch(540/3000) | Training loss: 2.1212437124479386 | Epoch(541/3000) | Training loss: 2.1212436698731922 | Epoch(542/3000) | Training loss: 2.1212436443283442 | Epoch(543/3000) | Training loss: 2.1212436131068637 | Epoch(544/3000) | Training loss: 2.1212436187834967 | Epoch(545/3000) | Training loss: 2.121243607430231 | Epoch(546/3000) | Training loss: 2.1212435307956876 | Epoch(547/3000) | Training loss: 2.121243553502219 | Epoch(548/3000) | Training loss: 2.1212435421489535 | Epoch(549/3000) | Training loss: 2.1212434825443087 | Epoch(550/3000) | Training loss: 2.1212434740293595 | Epoch(551/3000) | Training loss: 2.1212434626760936 | Epoch(552/3000) | Training loss: 2.1212434428078786 | Epoch(553/3000) | Training loss: 2.1212434399695623 | Epoch(554/3000) | Training loss: 2.121243411586398 | Epoch(555/3000) | Training loss: 2.1212433803649176 | Epoch(556/3000) | Training loss: 2.1212433860415505 | Epoch(557/3000) | Training loss: 2.121243363335019 | Epoch(558/3000) | Training loss: 2.1212433661733354 | Epoch(559/3000) | Training loss: 2.1212433008920577 | Epoch(560/3000) | Training loss: 2.1212433008920577 | Epoch(561/3000) | Training loss: 2.12124327534721 | Epoch(562/3000) | Training loss: 2.1212432583173118 | Epoch(563/3000) | Training loss: 2.121243246964046 | Epoch(564/3000) | Training loss: 2.121243227095831 | Epoch(565/3000) | Training loss: 2.1212432299341475 | Epoch(566/3000) | Training loss: 2.1212432043892995 | Epoch(567/3000) | Training loss: 2.121243178844452 | Epoch(568/3000) | Training loss: 2.1212431532996043 | Epoch(569/3000) | Training loss: 2.121243105048225 | Epoch(570/3000) | Training loss: 2.1212431334313893 | Epoch(571/3000) | Training loss: 2.121243090856643 | Epoch(572/3000) | Training loss: 2.1212430681501115 | Epoch(573/3000) | Training loss: 2.121243056796846 | Epoch(574/3000) | Training loss: 2.121243051120213 | Epoch(575/3000) | Training loss: 2.121243025575365 | Epoch(576/3000) | Training loss: 2.121243017060416 | Epoch(577/3000) | Training loss: 2.1212429943538846 | Epoch(578/3000) | Training loss: 2.1212430085454668 | Epoch(579/3000) | Training loss: 2.1212429461025057 | Epoch(580/3000) | Training loss: 2.121242960294088 | Epoch(581/3000) | Training loss: 2.121242900689443 | Epoch(582/3000) | Training loss: 2.1212429375875566 | Epoch(583/3000) | Training loss: 2.12124289501281 | Epoch(584/3000) | Training loss: 2.1212428552763805 | Epoch(585/3000) | Training loss: 2.121242846761431 | Epoch(586/3000) | Training loss: 2.1212428495997475 | Epoch(587/3000) | Training loss: 2.1212428354081654 | Epoch(588/3000) | Training loss: 2.1212427758035206 | Epoch(589/3000) | Training loss: 2.121242792833419 | Epoch(590/3000) | Training loss: 2.1212427758035206 | Epoch(591/3000) | Training loss: 2.12124274458204 | Epoch(592/3000) | Training loss: 2.121242724713825 | Epoch(593/3000) | Training loss: 2.1212427417437234 | Epoch(594/3000) | Training loss: 2.121242738905407 | Epoch(595/3000) | Training loss: 2.1212426793007624 | Epoch(596/3000) | Training loss: 2.121242645240965 | Epoch(597/3000) | Training loss: 2.121242645240965 | Epoch(598/3000) | Training loss: 2.1212426679474965 | Epoch(599/3000) | Training loss: 2.121242645240965 | Epoch(600/3000) | Training loss: 2.1212425998279025 | Epoch(601/3000) | Training loss: 2.121242574283055 | Epoch(602/3000) | Training loss: 2.1212425856363204 | Epoch(603/3000) | Training loss: 2.1212425913129533 | Epoch(604/3000) | Training loss: 2.121242548738207 | Epoch(605/3000) | Training loss: 2.1212425317083086 | Epoch(606/3000) | Training loss: 2.1212425430615744 | Epoch(607/3000) | Training loss: 2.1212425033251443 | Epoch(608/3000) | Training loss: 2.1212425317083086 | Epoch(609/3000) | Training loss: 2.1212424550737654 | Epoch(610/3000) | Training loss: 2.1212424749419805 | Epoch(611/3000) | Training loss: 2.1212424635887146 | Epoch(612/3000) | Training loss: 2.1212424579120817 | Epoch(613/3000) | Training loss: 2.121242438043867 | Epoch(614/3000) | Training loss: 2.1212423756009056 | Epoch(615/3000) | Training loss: 2.1212423869541714 | Epoch(616/3000) | Training loss: 2.1212423841158548 | Epoch(617/3000) | Training loss: 2.1212423472177413 | Epoch(618/3000) | Training loss: 2.1212423756009056 | Epoch(619/3000) | Training loss: 2.1212423472177413 | Epoch(620/3000) | Training loss: 2.121242315996261 | Epoch(621/3000) | Training loss: 2.1212422847747803 | Epoch(622/3000) | Training loss: 2.1212422734215144 | Epoch(623/3000) | Training loss: 2.121242281936464 | Epoch(624/3000) | Training loss: 2.1212422734215144 | Epoch(625/3000) | Training loss: 2.1212422450383506 | Epoch(626/3000) | Training loss: 2.1212422166551863 | Epoch(627/3000) | Training loss: 2.121242208140237 | Epoch(628/3000) | Training loss: 2.1212421967869712 | Epoch(629/3000) | Training loss: 2.121242179757073 | Epoch(630/3000) | Training loss: 2.1212421570505415 | Epoch(631/3000) | Training loss: 2.1212421315056935 | Epoch(632/3000) | Training loss: 2.1212421371823265 | Epoch(633/3000) | Training loss: 2.121242108799162 | Epoch(634/3000) | Training loss: 2.121242086092631 | Epoch(635/3000) | Training loss: 2.1212421031225297 | Epoch(636/3000) | Training loss: 2.1212420804159984 | Epoch(637/3000) | Training loss: 2.1212420633860996 | Epoch(638/3000) | Training loss: 2.12124202364967 | Epoch(639/3000) | Training loss: 2.121242043517885 | Epoch(640/3000) | Training loss: 2.121242037841252 | Epoch(641/3000) | Training loss: 2.121242012296404 | Epoch(642/3000) | Training loss: 2.1212419924281893 | Epoch(643/3000) | Training loss: 2.121241955530076 | Epoch(644/3000) | Training loss: 2.121241961206709 | Epoch(645/3000) | Training loss: 2.121241935661861 | Epoch(646/3000) | Training loss: 2.121241929985228 | Epoch(647/3000) | Training loss: 2.1212419271469116 | Epoch(648/3000) | Training loss: 2.1212419271469116 | Epoch(649/3000) | Training loss: 2.121241881733849 | Epoch(650/3000) | Training loss: 2.121241867542267 | Epoch(651/3000) | Training loss: 2.1212418590273177 | Epoch(652/3000) | Training loss: 2.1212418249675205 | Epoch(653/3000) | Training loss: 2.1212418107759383 | Epoch(654/3000) | Training loss: 2.121241788069407 | Epoch(655/3000) | Training loss: 2.1212417653628757 | Epoch(656/3000) | Training loss: 2.1212417909077237 | Epoch(657/3000) | Training loss: 2.1212417625245594 | Epoch(658/3000) | Training loss: 2.1212417369797114 | Epoch(659/3000) | Training loss: 2.1212417426563444 | Epoch(660/3000) | Training loss: 2.1212417426563444 | Epoch(661/3000) | Training loss: 2.121241717111497 | Epoch(662/3000) | Training loss: 2.1212416858900163 | Epoch(663/3000) | Training loss: 2.1212417029199147 | Epoch(664/3000) | Training loss: 2.1212416858900163 | Epoch(665/3000) | Training loss: 2.1212416802133833 | Epoch(666/3000) | Training loss: 2.121241648991903 | Epoch(667/3000) | Training loss: 2.121241637638637 | Epoch(668/3000) | Training loss: 2.121241580872309 | Epoch(669/3000) | Training loss: 2.12124160357884 | Epoch(670/3000) | Training loss: 2.121241572357359 | Epoch(671/3000) | Training loss: 2.1212415922255743 | Epoch(672/3000) | Training loss: 2.121241583710625 | Epoch(673/3000) | Training loss: 2.1212415411358787 | Epoch(674/3000) | Training loss: 2.1212415326209295 | Epoch(675/3000) | Training loss: 2.1212414928845 | Epoch(676/3000) | Training loss: 2.121241507076082 | Epoch(677/3000) | Training loss: 2.1212414928845 | Epoch(678/3000) | Training loss: 2.121241467339652 | Epoch(679/3000) | Training loss: 2.1212414928845 | Epoch(680/3000) | Training loss: 2.1212414645013355 | Epoch(681/3000) | Training loss: 2.121241416249956 | Epoch(682/3000) | Training loss: 2.1212414361181713 | Epoch(683/3000) | Training loss: 2.1212414417948042 | Epoch(684/3000) | Training loss: 2.121241419088273 | Epoch(685/3000) | Training loss: 2.1212413878667924 | Epoch(686/3000) | Training loss: 2.121241365160261 | Epoch(687/3000) | Training loss: 2.121241365160261 | Epoch(688/3000) | Training loss: 2.1212413736752103 | Epoch(689/3000) | Training loss: 2.121241319747198 | Epoch(690/3000) | Training loss: 2.1212412942023504 | Epoch(691/3000) | Training loss: 2.1212413424537298 | Epoch(692/3000) | Training loss: 2.121241311232249 | Epoch(693/3000) | Training loss: 2.1212412856874012 | Epoch(694/3000) | Training loss: 2.1212412970406667 | Epoch(695/3000) | Training loss: 2.1212412800107683 | Epoch(696/3000) | Training loss: 2.121241282849085 | Epoch(697/3000) | Training loss: 2.1212412345977056 | Epoch(698/3000) | Training loss: 2.121241243112655 | Epoch(699/3000) | Training loss: 2.1212412204061235 | Epoch(700/3000) | Training loss: 2.121241257304237 | Epoch(701/3000) | Training loss: 2.1212412062145414 | Epoch(702/3000) | Training loss: 2.121241177831377 | Epoch(703/3000) | Training loss: 2.121241194861276 | Epoch(704/3000) | Training loss: 2.121241197699592 | Epoch(705/3000) | Training loss: 2.121241180669694 | Epoch(706/3000) | Training loss: 2.1212411409332637 | Epoch(707/3000) | Training loss: 2.1212411579631625 | Epoch(708/3000) | Training loss: 2.121241095520201 | Epoch(709/3000) | Training loss: 2.1212410784903026 | Epoch(710/3000) | Training loss: 2.1212411040351506 | Epoch(711/3000) | Training loss: 2.1212410756519864 | Epoch(712/3000) | Training loss: 2.1212410926818848 | Epoch(713/3000) | Training loss: 2.1212410756519864 | Epoch(714/3000) | Training loss: 2.1212410614604043 | Epoch(715/3000) | Training loss: 2.121241041592189 | Epoch(716/3000) | Training loss: 2.121241018885658 | Epoch(717/3000) | Training loss: 2.121241013209025 | Epoch(718/3000) | Training loss: 2.1212410075323924 | Epoch(719/3000) | Training loss: 2.1212410018557595 | Epoch(720/3000) | Training loss: 2.1212409621193293 | Epoch(721/3000) | Training loss: 2.1212409819875444 | Epoch(722/3000) | Training loss: 2.12124095360438 | Epoch(723/3000) | Training loss: 2.121240945089431 | Epoch(724/3000) | Training loss: 2.121240939412798 | Epoch(725/3000) | Training loss: 2.1212408939997354 | Epoch(726/3000) | Training loss: 2.1212409053530012 | Epoch(727/3000) | Training loss: 2.1212409621193293 | Epoch(728/3000) | Training loss: 2.12124088264647 | Epoch(729/3000) | Training loss: 2.1212409025146846 | Epoch(730/3000) | Training loss: 2.121240871293204 | Epoch(731/3000) | Training loss: 2.1212408457483565 | Epoch(732/3000) | Training loss: 2.121240868454888 | Epoch(733/3000) | Training loss: 2.1212408599399386 | Epoch(734/3000) | Training loss: 2.1212408315567743 | Epoch(735/3000) | Training loss: 2.1212408315567743 | Epoch(736/3000) | Training loss: 2.12124080317361 | Epoch(737/3000) | Training loss: 2.121240788982028 | Epoch(738/3000) | Training loss: 2.1212407804670788 | Epoch(739/3000) | Training loss: 2.1212407662754966 | Epoch(740/3000) | Training loss: 2.1212407662754966 | Epoch(741/3000) | Training loss: 2.121240746407282 | Epoch(742/3000) | Training loss: 2.1212407435689653 | Epoch(743/3000) | Training loss: 2.121240706670852 | Epoch(744/3000) | Training loss: 2.1212407378923324 | Epoch(745/3000) | Training loss: 2.1212406697727384 | Epoch(746/3000) | Training loss: 2.121240712347485 | Epoch(747/3000) | Training loss: 2.1212406953175864 | Epoch(748/3000) | Training loss: 2.121240672611055 | Epoch(749/3000) | Training loss: 2.1212406924792697 | Epoch(750/3000) | Training loss: 2.121240661257789 | Epoch(751/3000) | Training loss: 2.121240647066207 | Epoch(752/3000) | Training loss: 2.1212405959765115 | Epoch(753/3000) | Training loss: 2.121240632874625 | Epoch(754/3000) | Training loss: 2.1212406101680936 | Epoch(755/3000) | Training loss: 2.1212406357129416 | Epoch(756/3000) | Training loss: 2.1212406271979924 | Epoch(757/3000) | Training loss: 2.121240564755031 | Epoch(758/3000) | Training loss: 2.1212405817849294 | Epoch(759/3000) | Training loss: 2.121240564755031 | Epoch(760/3000) | Training loss: 2.121240578946613 | Epoch(761/3000) | Training loss: 2.121240564755031 | Epoch(762/3000) | Training loss: 2.1212405250186013 | Epoch(763/3000) | Training loss: 2.121240530695234 | Epoch(764/3000) | Training loss: 2.1212405392101834 | Epoch(765/3000) | Training loss: 2.1212404994737533 | Epoch(766/3000) | Training loss: 2.121240485282171 | Epoch(767/3000) | Training loss: 2.12124046257564 | Epoch(768/3000) | Training loss: 2.1212404994737533 | Epoch(769/3000) | Training loss: 2.121240510827019 | Epoch(770/3000) | Training loss: 2.1212404739289057 | Epoch(771/3000) | Training loss: 2.1212404654139565 | Epoch(772/3000) | Training loss: 2.1212404398691085 | Epoch(773/3000) | Training loss: 2.121240428515843 | Epoch(774/3000) | Training loss: 2.121240476767222 | Epoch(775/3000) | Training loss: 2.121240448384058 | Epoch(776/3000) | Training loss: 2.121240428515843 | Epoch(777/3000) | Training loss: 2.1212403717495145 | Epoch(778/3000) | Training loss: 2.1212404256775264 | Epoch(779/3000) | Training loss: 2.121240354719616 | Epoch(780/3000) | Training loss: 2.1212403887794133 | Epoch(781/3000) | Training loss: 2.121240354719616 | Epoch(782/3000) | Training loss: 2.121240360396249 | Epoch(783/3000) | Training loss: 2.1212403291747686 | Epoch(784/3000) | Training loss: 2.1212403660728816 | Epoch(785/3000) | Training loss: 2.121240332013085 | Epoch(786/3000) | Training loss: 2.121240258216858 | Epoch(787/3000) | Training loss: 2.1212402837617055 | Epoch(788/3000) | Training loss: 2.121240286600022 | Epoch(789/3000) | Training loss: 2.121240246863592 | Epoch(790/3000) | Training loss: 2.121240266731807 | Epoch(791/3000) | Training loss: 2.1212402809233892 | Epoch(792/3000) | Training loss: 2.1212402411869595 | Epoch(793/3000) | Training loss: 2.121240252540225 | Epoch(794/3000) | Training loss: 2.121240244025276 | Epoch(795/3000) | Training loss: 2.1212402014505294 | Epoch(796/3000) | Training loss: 2.1212402298336936 | Epoch(797/3000) | Training loss: 2.1212402298336936 | Epoch(798/3000) | Training loss: 2.1212402128037953 | Epoch(799/3000) | Training loss: 2.1212402355103266 | Epoch(800/3000) | Training loss: 2.121240184420631 | Epoch(801/3000) | Training loss: 2.121240184420631 | Epoch(802/3000) | Training loss: 2.1212401560374667 | Epoch(803/3000) | Training loss: 2.1212401617140997 | Epoch(804/3000) | Training loss: 2.121240130492619 | Epoch(805/3000) | Training loss: 2.1212401418458846 | Epoch(806/3000) | Training loss: 2.1212401248159862 | Epoch(807/3000) | Training loss: 2.121240110624404 | Epoch(808/3000) | Training loss: 2.1212401333309354 | Epoch(809/3000) | Training loss: 2.121240110624404 | Epoch(810/3000) | Training loss: 2.121240096432822 | Epoch(811/3000) | Training loss: 2.121240116301037 | Epoch(812/3000) | Training loss: 2.1212400879178728 | Epoch(813/3000) | Training loss: 2.121240053858076 | Epoch(814/3000) | Training loss: 2.121240102109455 | Epoch(815/3000) | Training loss: 2.1212400652113415 | Epoch(816/3000) | Training loss: 2.1212400453431264 | Epoch(817/3000) | Training loss: 2.121240019798279 | Epoch(818/3000) | Training loss: 2.1212399999300637 | Epoch(819/3000) | Training loss: 2.121240033989861 | Epoch(820/3000) | Training loss: 2.1212400169599626 | Epoch(821/3000) | Training loss: 2.121240033989861 | Epoch(822/3000) | Training loss: 2.1212400112833296 | Epoch(823/3000) | Training loss: 2.121240014121646 | Epoch(824/3000) | Training loss: 2.1212399970917475 | Epoch(825/3000) | Training loss: 2.1212400112833296 | Epoch(826/3000) | Training loss: 2.121239960193634 | Epoch(827/3000) | Training loss: 2.1212399829001654 | Epoch(828/3000) | Training loss: 2.1212399431637357 | Epoch(829/3000) | Training loss: 2.1212399374871027 | Epoch(830/3000) | Training loss: 2.1212399687085832 | Epoch(831/3000) | Training loss: 2.1212399374871027 | Epoch(832/3000) | Training loss: 2.1212399431637357 | Epoch(833/3000) | Training loss: 2.121239897750673 | Epoch(834/3000) | Training loss: 2.121239946002052 | Epoch(835/3000) | Training loss: 2.121239880720774 | Epoch(836/3000) | Training loss: 2.121239906265622 | Epoch(837/3000) | Training loss: 2.121239866529192 | Epoch(838/3000) | Training loss: 2.121239883559091 | Epoch(839/3000) | Training loss: 2.1212398466609774 | Epoch(840/3000) | Training loss: 2.121239886397407 | Epoch(841/3000) | Training loss: 2.1212398466609774 | Epoch(842/3000) | Training loss: 2.1212398494992937 | Epoch(843/3000) | Training loss: 2.1212398466609774 | Epoch(844/3000) | Training loss: 2.121239809762864 | Epoch(845/3000) | Training loss: 2.1212398267927624 | Epoch(846/3000) | Training loss: 2.121239795571282 | Epoch(847/3000) | Training loss: 2.121239798409598 | Epoch(848/3000) | Training loss: 2.121239815439497 | Epoch(849/3000) | Training loss: 2.121239789894649 | Epoch(850/3000) | Training loss: 2.1212397927329656 | Epoch(851/3000) | Training loss: 2.1212397643498013 | Epoch(852/3000) | Training loss: 2.1212397671881176 | Epoch(853/3000) | Training loss: 2.1212397671881176 | Epoch(854/3000) | Training loss: 2.12123974164327 | Epoch(855/3000) | Training loss: 2.121239770026434 | Epoch(856/3000) | Training loss: 2.121239710421789 | Epoch(857/3000) | Training loss: 2.121239724613371 | Epoch(858/3000) | Training loss: 2.121239730290004 | Epoch(859/3000) | Training loss: 2.1212396848769415 | Epoch(860/3000) | Training loss: 2.121239710421789 | Epoch(861/3000) | Training loss: 2.1212396933918907 | Epoch(862/3000) | Training loss: 2.121239667847043 | Epoch(863/3000) | Training loss: 2.1212396792003085 | Epoch(864/3000) | Training loss: 2.1212396792003085 | Epoch(865/3000) | Training loss: 2.12123966217041 | Epoch(866/3000) | Training loss: 2.1212396451405118 | Epoch(867/3000) | Training loss: 2.121239656493777 | Epoch(868/3000) | Training loss: 2.121239659332094 | Epoch(869/3000) | Training loss: 2.1212396252722967 | Epoch(870/3000) | Training loss: 2.121239656493777 | Epoch(871/3000) | Training loss: 2.1212396451405118 | Epoch(872/3000) | Training loss: 2.121239594050816 | Epoch(873/3000) | Training loss: 2.121239628110613 | Epoch(874/3000) | Training loss: 2.1212396054040816 | Epoch(875/3000) | Training loss: 2.121239628110613 | Epoch(876/3000) | Training loss: 2.121239565667652 | Epoch(877/3000) | Training loss: 2.1212395883741832 | Epoch(878/3000) | Training loss: 2.1212395883741832 | Epoch(879/3000) | Training loss: 2.121239577020918 | Epoch(880/3000) | Training loss: 2.121239559991019 | Epoch(881/3000) | Training loss: 2.1212395486377535 | Epoch(882/3000) | Training loss: 2.1212395571527027 | Epoch(883/3000) | Training loss: 2.1212395287695385 | Epoch(884/3000) | Training loss: 2.1212395486377535 | Epoch(885/3000) | Training loss: 2.1212395230929055 | Epoch(886/3000) | Training loss: 2.1212395316078547 | Epoch(887/3000) | Training loss: 2.12123951173964 | Epoch(888/3000) | Training loss: 2.1212395316078547 | Epoch(889/3000) | Training loss: 2.1212395202545893 | Epoch(890/3000) | Training loss: 2.1212395202545893 | Epoch(891/3000) | Training loss: 2.121239483356476 | Epoch(892/3000) | Training loss: 2.121239500386374 | Epoch(893/3000) | Training loss: 2.1212394805181596 | Epoch(894/3000) | Training loss: 2.1212394663265774 | Epoch(895/3000) | Training loss: 2.1212394748415266 | Epoch(896/3000) | Training loss: 2.1212394634882608 | Epoch(897/3000) | Training loss: 2.121239443620046 | Epoch(898/3000) | Training loss: 2.1212394663265774 | Epoch(899/3000) | Training loss: 2.121239415236882 | Epoch(900/3000) | Training loss: 2.121239423751831 | Epoch(901/3000) | Training loss: 2.1212394322667802 | Epoch(902/3000) | Training loss: 2.121239435105097 | Epoch(903/3000) | Training loss: 2.1212393982069835 | Epoch(904/3000) | Training loss: 2.121239389692034 | Epoch(905/3000) | Training loss: 2.121239403883616 | Epoch(906/3000) | Training loss: 2.1212393726621355 | Epoch(907/3000) | Training loss: 2.121239420913515 | Epoch(908/3000) | Training loss: 2.1212393840154014 | Epoch(909/3000) | Training loss: 2.121239375500452 | Epoch(910/3000) | Training loss: 2.121239355632237 | Epoch(911/3000) | Training loss: 2.121239335764022 | Epoch(912/3000) | Training loss: 2.121239327249073 | Epoch(913/3000) | Training loss: 2.1212393584705533 | Epoch(914/3000) | Training loss: 2.121239349955604 | Epoch(915/3000) | Training loss: 2.121239330087389 | Epoch(916/3000) | Training loss: 2.121239347117288 | Epoch(917/3000) | Training loss: 2.1212393017042253 | Epoch(918/3000) | Training loss: 2.1212393102191744 | Epoch(919/3000) | Training loss: 2.1212392903509594 | Epoch(920/3000) | Training loss: 2.1212392846743264 | Epoch(921/3000) | Training loss: 2.1212393017042253 | Epoch(922/3000) | Training loss: 2.1212392846743264 | Epoch(923/3000) | Training loss: 2.1212393102191744 | Epoch(924/3000) | Training loss: 2.1212393045425415 | Epoch(925/3000) | Training loss: 2.121239264806112 | Epoch(926/3000) | Training loss: 2.121239259129479 | Epoch(927/3000) | Training loss: 2.12123928183601 | Epoch(928/3000) | Training loss: 2.121239273321061 | Epoch(929/3000) | Training loss: 2.121239278997694 | Epoch(930/3000) | Training loss: 2.121239219393049 | Epoch(931/3000) | Training loss: 2.121239219393049 | Epoch(932/3000) | Training loss: 2.1212392420995805 | Epoch(933/3000) | Training loss: 2.121239193848201 | Epoch(934/3000) | Training loss: 2.121239191009885 | Epoch(935/3000) | Training loss: 2.1212392250696817 | Epoch(936/3000) | Training loss: 2.121239233584631 | Epoch(937/3000) | Training loss: 2.121239185333252 | Epoch(938/3000) | Training loss: 2.121239185333252 | Epoch(939/3000) | Training loss: 2.1212392023631503 | Epoch(940/3000) | Training loss: 2.121239193848201 | Epoch(941/3000) | Training loss: 2.121239191009885 | Epoch(942/3000) | Training loss: 2.121239188171568 | Epoch(943/3000) | Training loss: 2.1212391683033536 | Epoch(944/3000) | Training loss: 2.12123917114167 | Epoch(945/3000) | Training loss: 2.1212391399201893 | Epoch(946/3000) | Training loss: 2.1212391626267206 | Epoch(947/3000) | Training loss: 2.1212391399201893 | Epoch(948/3000) | Training loss: 2.1212391569500877 | Epoch(949/3000) | Training loss: 2.12123913140524 | Epoch(950/3000) | Training loss: 2.1212391285669234 | Epoch(951/3000) | Training loss: 2.1212390604473295 | Epoch(952/3000) | Training loss: 2.1212390945071267 | Epoch(953/3000) | Training loss: 2.121239117213658 | Epoch(954/3000) | Training loss: 2.1212390774772283 | Epoch(955/3000) | Training loss: 2.121239097345443 | Epoch(956/3000) | Training loss: 2.1212390859921775 | Epoch(957/3000) | Training loss: 2.121239029225849 | Epoch(958/3000) | Training loss: 2.1212390462557473 | Epoch(959/3000) | Training loss: 2.1212390746389116 | Epoch(960/3000) | Training loss: 2.121239023549216 | Epoch(961/3000) | Training loss: 2.1212390718005953 | Epoch(962/3000) | Training loss: 2.1212390519323803 | Epoch(963/3000) | Training loss: 2.121239057609013 | Epoch(964/3000) | Training loss: 2.121239009357634 | Epoch(965/3000) | Training loss: 2.121239032064165 | Epoch(966/3000) | Training loss: 2.121239032064165 | Epoch(967/3000) | Training loss: 2.121239015034267 | Epoch(968/3000) | Training loss: 2.121239043417431 | Epoch(969/3000) | Training loss: 2.1212389838127863 | Epoch(970/3000) | Training loss: 2.1212390121959506 | Epoch(971/3000) | Training loss: 2.1212389923277355 | Epoch(972/3000) | Training loss: 2.1212390065193176 | Epoch(973/3000) | Training loss: 2.1212389894894192 | Epoch(974/3000) | Training loss: 2.1212389951660517 | Epoch(975/3000) | Training loss: 2.1212389724595204 | Epoch(976/3000) | Training loss: 2.121238949752989 | Epoch(977/3000) | Training loss: 2.121238949752989 | Epoch(978/3000) | Training loss: 2.121238966782888 | Epoch(979/3000) | Training loss: 2.1212389440763566 | Epoch(980/3000) | Training loss: 2.1212389298847745 | Epoch(981/3000) | Training loss: 2.1212389440763566 | Epoch(982/3000) | Training loss: 2.1212389100165594 | Epoch(983/3000) | Training loss: 2.1212389298847745 | Epoch(984/3000) | Training loss: 2.1212389213698253 | Epoch(985/3000) | Training loss: 2.121238898663294 | Epoch(986/3000) | Training loss: 2.1212389213698253 | Epoch(987/3000) | Training loss: 2.121238887310028 | Epoch(988/3000) | Training loss: 2.1212389128548756 | Epoch(989/3000) | Training loss: 2.1212389043399265 | Epoch(990/3000) | Training loss: 2.121238907178243 | Epoch(991/3000) | Training loss: 2.1212388646034968 | Epoch(992/3000) | Training loss: 2.121238898663294 | Epoch(993/3000) | Training loss: 2.121238858926864 | Epoch(994/3000) | Training loss: 2.121238873118446 | Epoch(995/3000) | Training loss: 2.1212388418969654 | Epoch(996/3000) | Training loss: 2.121238853250231 | Epoch(997/3000) | Training loss: 2.1212388333820162 | Epoch(998/3000) | Training loss: 2.121238858926864 | Epoch(999/3000) | Training loss: 2.12123890150161 | Epoch(1000/3000) | Training loss: 2.1212388362203325 | Epoch(1001/3000) | Training loss: 2.121238799322219 | Epoch(1002/3000) | Training loss: 2.121238796483903 | Epoch(1003/3000) | Training loss: 2.1212388418969654 | Epoch(1004/3000) | Training loss: 2.1212387794540044 | Epoch(1005/3000) | Training loss: 2.1212388078371682 | Epoch(1006/3000) | Training loss: 2.121238804998852 | Epoch(1007/3000) | Training loss: 2.121238819190434 | Epoch(1008/3000) | Training loss: 2.1212387709390548 | Epoch(1009/3000) | Training loss: 2.1212387879689536 | Epoch(1010/3000) | Training loss: 2.1212388078371682 | Epoch(1011/3000) | Training loss: 2.121238799322219 | Epoch(1012/3000) | Training loss: 2.1212387567474726 | Epoch(1013/3000) | Training loss: 2.1212387766156877 | Epoch(1014/3000) | Training loss: 2.1212387141727267 | Epoch(1015/3000) | Training loss: 2.121238745394207 | Epoch(1016/3000) | Training loss: 2.121238742555891 | Epoch(1017/3000) | Training loss: 2.121238722687676 | Epoch(1018/3000) | Training loss: 2.1212387652624223 | Epoch(1019/3000) | Training loss: 2.1212387141727267 | Epoch(1020/3000) | Training loss: 2.121238722687676 | Epoch(1021/3000) | Training loss: 2.1212387539091564 | Epoch(1022/3000) | Training loss: 2.1212387397175743 | Epoch(1023/3000) | Training loss: 2.1212386801129295 | Epoch(1024/3000) | Training loss: 2.1212387198493596 | Epoch(1025/3000) | Training loss: 2.121238697142828 | Epoch(1026/3000) | Training loss: 2.121238731202625 | Epoch(1027/3000) | Training loss: 2.1212386914661954 | Epoch(1028/3000) | Training loss: 2.1212386914661954 | Epoch(1029/3000) | Training loss: 2.1212386517297652 | Epoch(1030/3000) | Training loss: 2.1212386744362965 | Epoch(1031/3000) | Training loss: 2.121238643214816 | Epoch(1032/3000) | Training loss: 2.1212386857895624 | Epoch(1033/3000) | Training loss: 2.121238637538183 | Epoch(1034/3000) | Training loss: 2.1212386659213474 | Epoch(1035/3000) | Training loss: 2.1212386517297652 | Epoch(1036/3000) | Training loss: 2.1212386119933355 | Epoch(1037/3000) | Training loss: 2.121238629023234 | Epoch(1038/3000) | Training loss: 2.1212386403765 | Epoch(1039/3000) | Training loss: 2.1212386261849177 | Epoch(1040/3000) | Training loss: 2.1212386176699685 | Epoch(1041/3000) | Training loss: 2.1212385978017534 | Epoch(1042/3000) | Training loss: 2.1212386176699685 | Epoch(1043/3000) | Training loss: 2.121238580771855 | Epoch(1044/3000) | Training loss: 2.1212386034783863 | Epoch(1045/3000) | Training loss: 2.1212386091550193 | Epoch(1046/3000) | Training loss: 2.1212386034783863 | Epoch(1047/3000) | Training loss: 2.1212386063167026 | Epoch(1048/3000) | Training loss: 2.121238572256906 | Epoch(1049/3000) | Training loss: 2.1212385296821594 | Epoch(1050/3000) | Training loss: 2.1212385637419566 | Epoch(1051/3000) | Training loss: 2.1212385495503745 | Epoch(1052/3000) | Training loss: 2.1212385438737416 | Epoch(1053/3000) | Training loss: 2.121238555227007 | Epoch(1054/3000) | Training loss: 2.1212385637419566 | Epoch(1055/3000) | Training loss: 2.1212385637419566 | Epoch(1056/3000) | Training loss: 2.1212385296821594 | Epoch(1057/3000) | Training loss: 2.1212385353587924 | Epoch(1058/3000) | Training loss: 2.1212385353587924 | Epoch(1059/3000) | Training loss: 2.1212385211672102 | Epoch(1060/3000) | Training loss: 2.1212385325204757 | Epoch(1061/3000) | Training loss: 2.1212385211672102 | Epoch(1062/3000) | Training loss: 2.1212385211672102 | Epoch(1063/3000) | Training loss: 2.121238546712058 | Epoch(1064/3000) | Training loss: 2.121238472915831 | Epoch(1065/3000) | Training loss: 2.1212384644008817 | Epoch(1066/3000) | Training loss: 2.121238506975628 | Epoch(1067/3000) | Training loss: 2.121238492784046 | Epoch(1068/3000) | Training loss: 2.121238472915831 | Epoch(1069/3000) | Training loss: 2.1212384502092996 | Epoch(1070/3000) | Training loss: 2.121238472915831 | Epoch(1071/3000) | Training loss: 2.1212384615625655 | Epoch(1072/3000) | Training loss: 2.121238498460679 | Epoch(1073/3000) | Training loss: 2.1212384644008817 | Epoch(1074/3000) | Training loss: 2.1212384558859325 | Epoch(1075/3000) | Training loss: 2.1212384275027683 | Epoch(1076/3000) | Training loss: 2.1212384473709833 | Epoch(1077/3000) | Training loss: 2.1212384587242488 | Epoch(1078/3000) | Training loss: 2.121238438856034 | Epoch(1079/3000) | Training loss: 2.121238444532667 | Epoch(1080/3000) | Training loss: 2.1212384076345536 | Epoch(1081/3000) | Training loss: 2.1212384218261358 | Epoch(1082/3000) | Training loss: 2.121238404796237 | Epoch(1083/3000) | Training loss: 2.1212384019579207 | Epoch(1084/3000) | Training loss: 2.1212384360177174 | Epoch(1085/3000) | Training loss: 2.1212384076345536 | Epoch(1086/3000) | Training loss: 2.1212383849280223 | Epoch(1087/3000) | Training loss: 2.1212383877663386 | Epoch(1088/3000) | Training loss: 2.1212383934429715 | Epoch(1089/3000) | Training loss: 2.121238418987819 | Epoch(1090/3000) | Training loss: 2.1212383792513894 | Epoch(1091/3000) | Training loss: 2.1212383877663386 | Epoch(1092/3000) | Training loss: 2.1212383849280223 | Epoch(1093/3000) | Training loss: 2.121238350868225 | Epoch(1094/3000) | Training loss: 2.1212383537065413 | Epoch(1095/3000) | Training loss: 2.1212383593831743 | Epoch(1096/3000) | Training loss: 2.121238356544858 | Epoch(1097/3000) | Training loss: 2.1212383253233775 | Epoch(1098/3000) | Training loss: 2.1212383792513894 | Epoch(1099/3000) | Training loss: 2.1212383253233775 | Epoch(1100/3000) | Training loss: 2.121238316808428 | Epoch(1101/3000) | Training loss: 2.1212383537065413 | Epoch(1102/3000) | Training loss: 2.121238339514959 | Epoch(1103/3000) | Training loss: 2.121238339514959 | Epoch(1104/3000) | Training loss: 2.121238302616846 | Epoch(1105/3000) | Training loss: 2.121238342353276 | Epoch(1106/3000) | Training loss: 2.1212383196467446 | Epoch(1107/3000) | Training loss: 2.1212383082934787 | Epoch(1108/3000) | Training loss: 2.1212383139701116 | Epoch(1109/3000) | Training loss: 2.121238302616846 | Epoch(1110/3000) | Training loss: 2.1212382657187328 | Epoch(1111/3000) | Training loss: 2.1212382713953652 | Epoch(1112/3000) | Training loss: 2.121238302616846 | Epoch(1113/3000) | Training loss: 2.1212382912635803 | Epoch(1114/3000) | Training loss: 2.1212382855869474 | Epoch(1115/3000) | Training loss: 2.121238268557049 | Epoch(1116/3000) | Training loss: 2.1212382373355685 | Epoch(1117/3000) | Training loss: 2.121238262880416 | Epoch(1118/3000) | Training loss: 2.121238234497252 | Epoch(1119/3000) | Training loss: 2.1212382458505177 | Epoch(1120/3000) | Training loss: 2.1212382458505177 | Epoch(1121/3000) | Training loss: 2.121238257203783 | Epoch(1122/3000) | Training loss: 2.121238206114088 | Epoch(1123/3000) | Training loss: 2.121238206114088 | Epoch(1124/3000) | Training loss: 2.121238234497252 | Epoch(1125/3000) | Training loss: 2.1212382174673534 | Epoch(1126/3000) | Training loss: 2.1212382174673534 | Epoch(1127/3000) | Training loss: 2.1212382089524042 | Epoch(1128/3000) | Training loss: 2.121238214629037 | Epoch(1129/3000) | Training loss: 2.121238206114088 | Epoch(1130/3000) | Training loss: 2.121238214629037 | Epoch(1131/3000) | Training loss: 2.12123818056924 | Epoch(1132/3000) | Training loss: 2.121238200437455 | Epoch(1133/3000) | Training loss: 2.1212382117907205 | Epoch(1134/3000) | Training loss: 2.121238186245873 | Epoch(1135/3000) | Training loss: 2.1212381351561773 | Epoch(1136/3000) | Training loss: 2.121238200437455 | Epoch(1137/3000) | Training loss: 2.1212381692159745 | Epoch(1138/3000) | Training loss: 2.121238200437455 | Epoch(1139/3000) | Training loss: 2.1212381975991383 | Epoch(1140/3000) | Training loss: 2.121238166377658 | Epoch(1141/3000) | Training loss: 2.1212381777309237 | Epoch(1142/3000) | Training loss: 2.1212381379944936 | Epoch(1143/3000) | Training loss: 2.1212381578627086 | Epoch(1144/3000) | Training loss: 2.12123818056924 | Epoch(1145/3000) | Training loss: 2.1212381578627086 | Epoch(1146/3000) | Training loss: 2.121238120964595 | Epoch(1147/3000) | Training loss: 2.121238123802912 | Epoch(1148/3000) | Training loss: 2.1212381152879622 | Epoch(1149/3000) | Training loss: 2.1212381379944936 | Epoch(1150/3000) | Training loss: 2.121238132317861 | Epoch(1151/3000) | Training loss: 2.1212381294795444 | Epoch(1152/3000) | Training loss: 2.121238132317861 | Epoch(1153/3000) | Training loss: 2.1212381096113297 | Epoch(1154/3000) | Training loss: 2.121238118126279 | Epoch(1155/3000) | Training loss: 2.121238103934697 | Epoch(1156/3000) | Training loss: 2.121238112449646 | Epoch(1157/3000) | Training loss: 2.1212380812281655 | Epoch(1158/3000) | Training loss: 2.121238078389849 | Epoch(1159/3000) | Training loss: 2.1212380698748996 | Epoch(1160/3000) | Training loss: 2.121238058521634 | Epoch(1161/3000) | Training loss: 2.1212380840664817 | Epoch(1162/3000) | Training loss: 2.1212380897431147 | Epoch(1163/3000) | Training loss: 2.1212380670365834 | Epoch(1164/3000) | Training loss: 2.1212380556833175 | Epoch(1165/3000) | Training loss: 2.121238050006685 | Epoch(1166/3000) | Training loss: 2.121238052845001 | Epoch(1167/3000) | Training loss: 2.121238058521634 | Epoch(1168/3000) | Training loss: 2.121238024461837 | Epoch(1169/3000) | Training loss: 2.1212380670365834 | Epoch(1170/3000) | Training loss: 2.121238024461837 | Epoch(1171/3000) | Training loss: 2.1212380273001537 | Epoch(1172/3000) | Training loss: 2.121238044330052 | Epoch(1173/3000) | Training loss: 2.121238064198267 | Epoch(1174/3000) | Training loss: 2.121238044330052 | Epoch(1175/3000) | Training loss: 2.121238032976786 | Epoch(1176/3000) | Training loss: 2.121238024461837 | Epoch(1177/3000) | Training loss: 2.1212380273001537 | Epoch(1178/3000) | Training loss: 2.1212380216235207 | Epoch(1179/3000) | Training loss: 2.1212379875637235 | Epoch(1180/3000) | Training loss: 2.1212380216235207 | Epoch(1181/3000) | Training loss: 2.121238018785204 | Epoch(1182/3000) | Training loss: 2.121238024461837 | Epoch(1183/3000) | Training loss: 2.1212380017553056 | Epoch(1184/3000) | Training loss: 2.1212379875637235 | Epoch(1185/3000) | Training loss: 2.121237962018876 | Epoch(1186/3000) | Training loss: 2.1212380045936223 | Epoch(1187/3000) | Training loss: 2.121237967695509 | Epoch(1188/3000) | Training loss: 2.1212379960786727 | Epoch(1189/3000) | Training loss: 2.121237967695509 | Epoch(1190/3000) | Training loss: 2.12123795066561 | Epoch(1191/3000) | Training loss: 2.121237942150661 | Epoch(1192/3000) | Training loss: 2.121237976210458 | Epoch(1193/3000) | Training loss: 2.1212379336357117 | Epoch(1194/3000) | Training loss: 2.1212380017553056 | Epoch(1195/3000) | Training loss: 2.1212379307973954 | Epoch(1196/3000) | Training loss: 2.121237956342243 | Epoch(1197/3000) | Training loss: 2.121237976210458 | Epoch(1198/3000) | Training loss: 2.1212379279590787 | Epoch(1199/3000) | Training loss: 2.121237947827294 | Epoch(1200/3000) | Training loss: 2.1212379166058133 | Epoch(1201/3000) | Training loss: 2.121237942150661 | Epoch(1202/3000) | Training loss: 2.1212379790487743 | Epoch(1203/3000) | Training loss: 2.1212379307973954 | Epoch(1204/3000) | Training loss: 2.121237962018876 | Epoch(1205/3000) | Training loss: 2.1212379449889776 | Epoch(1206/3000) | Training loss: 2.121237896737598 | Epoch(1207/3000) | Training loss: 2.1212378711927506 | Epoch(1208/3000) | Training loss: 2.121237936474028 | Epoch(1209/3000) | Training loss: 2.1212378910609653 | Epoch(1210/3000) | Training loss: 2.1212379307973954 | Epoch(1211/3000) | Training loss: 2.121237882546016 | Epoch(1212/3000) | Training loss: 2.121237896737598 | Epoch(1213/3000) | Training loss: 2.121237874031067 | Epoch(1214/3000) | Training loss: 2.1212378853843328 | Epoch(1215/3000) | Training loss: 2.121237902414231 | Epoch(1216/3000) | Training loss: 2.1212378570011685 | Epoch(1217/3000) | Training loss: 2.1212379137674966 | Epoch(1218/3000) | Training loss: 2.1212378513245356 | Epoch(1219/3000) | Training loss: 2.121237868354434 | Epoch(1220/3000) | Training loss: 2.121237862677801 | Epoch(1221/3000) | Training loss: 2.1212378853843328 | Epoch(1222/3000) | Training loss: 2.1212378484862193 | Epoch(1223/3000) | Training loss: 2.121237862677801 | Epoch(1224/3000) | Training loss: 2.1212378172647384 | Epoch(1225/3000) | Training loss: 2.121237825779688 | Epoch(1226/3000) | Training loss: 2.1212378286180043 | Epoch(1227/3000) | Training loss: 2.1212378286180043 | Epoch(1228/3000) | Training loss: 2.121237834294637 | Epoch(1229/3000) | Training loss: 2.1212378314563205 | Epoch(1230/3000) | Training loss: 2.1212378172647384 | Epoch(1231/3000) | Training loss: 2.121237811588106 | Epoch(1232/3000) | Training loss: 2.121237811588106 | Epoch(1233/3000) | Training loss: 2.121237854162852 | Epoch(1234/3000) | Training loss: 2.121237794558207 | Epoch(1235/3000) | Training loss: 2.121237820103055 | Epoch(1236/3000) | Training loss: 2.121237805911473 | Epoch(1237/3000) | Training loss: 2.1212377832049416 | Epoch(1238/3000) | Training loss: 2.121237825779688 | Epoch(1239/3000) | Training loss: 2.1212377832049416 | Epoch(1240/3000) | Training loss: 2.121237794558207 | Epoch(1241/3000) | Training loss: 2.121237780366625 | Epoch(1242/3000) | Training loss: 2.1212377775283087 | Epoch(1243/3000) | Training loss: 2.1212377661750432 | Epoch(1244/3000) | Training loss: 2.1212377434685115 | Epoch(1245/3000) | Training loss: 2.1212377775283087 | Epoch(1246/3000) | Training loss: 2.1212377718516757 | Epoch(1247/3000) | Training loss: 2.1212377633367265 | Epoch(1248/3000) | Training loss: 2.1212377690133595 | Epoch(1249/3000) | Training loss: 2.1212377434685115 | Epoch(1250/3000) | Training loss: 2.121237746306828 | Epoch(1251/3000) | Training loss: 2.121237726438613 | Epoch(1252/3000) | Training loss: 2.1212377434685115 | Epoch(1253/3000) | Training loss: 2.1212377491451444 | Epoch(1254/3000) | Training loss: 2.1212377491451444 | Epoch(1255/3000) | Training loss: 2.1212377008937655 | Epoch(1256/3000) | Training loss: 2.1212377150853476 | Epoch(1257/3000) | Training loss: 2.121237698055449 | Epoch(1258/3000) | Training loss: 2.1212377491451444 | Epoch(1259/3000) | Training loss: 2.1212377037320818 | Epoch(1260/3000) | Training loss: 2.1212376867021834 | Epoch(1261/3000) | Training loss: 2.121237698055449 | Epoch(1262/3000) | Training loss: 2.1212377576600936 | Epoch(1263/3000) | Training loss: 2.12123772076198 | Epoch(1264/3000) | Training loss: 2.121237698055449 | Epoch(1265/3000) | Training loss: 2.1212377094087147 | Epoch(1266/3000) | Training loss: 2.1212376923788163 | Epoch(1267/3000) | Training loss: 2.1212376838638667 | Epoch(1268/3000) | Training loss: 2.121237726438613 | Epoch(1269/3000) | Training loss: 2.121237698055449 | Epoch(1270/3000) | Training loss: 2.1212376838638667 | Epoch(1271/3000) | Training loss: 2.121237669672285 | Epoch(1272/3000) | Training loss: 2.1212376753489175 | Epoch(1273/3000) | Training loss: 2.12123764980407 | Epoch(1274/3000) | Training loss: 2.1212376810255504 | Epoch(1275/3000) | Training loss: 2.1212376668339683 | Epoch(1276/3000) | Training loss: 2.121237678187234 | Epoch(1277/3000) | Training loss: 2.1212376923788163 | Epoch(1278/3000) | Training loss: 2.1212376611573354 | Epoch(1279/3000) | Training loss: 2.1212376611573354 | Epoch(1280/3000) | Training loss: 2.121237663995652 | Epoch(1281/3000) | Training loss: 2.121237652642386 | Epoch(1282/3000) | Training loss: 2.1212376668339683 | Epoch(1283/3000) | Training loss: 2.1212376725106012 | Epoch(1284/3000) | Training loss: 2.1212376214209057 | Epoch(1285/3000) | Training loss: 2.121237629935855 | Epoch(1286/3000) | Training loss: 2.121237658319019 | Epoch(1287/3000) | Training loss: 2.1212376469657537 | Epoch(1288/3000) | Training loss: 2.1212376469657537 | Epoch(1289/3000) | Training loss: 2.121237658319019 | Epoch(1290/3000) | Training loss: 2.121237644127437 | Epoch(1291/3000) | Training loss: 2.121237635612488 | Epoch(1292/3000) | Training loss: 2.1212376185825894 | Epoch(1293/3000) | Training loss: 2.121237590199425 | Epoch(1294/3000) | Training loss: 2.1212376129059565 | Epoch(1295/3000) | Training loss: 2.1212375788461593 | Epoch(1296/3000) | Training loss: 2.1212376100676402 | Epoch(1297/3000) | Training loss: 2.121237624259222 | Epoch(1298/3000) | Training loss: 2.121237635612488 | Epoch(1299/3000) | Training loss: 2.1212376015526906 | Epoch(1300/3000) | Training loss: 2.121237590199425 | Epoch(1301/3000) | Training loss: 2.1212376072293235 | Epoch(1302/3000) | Training loss: 2.1212375731695268 | Epoch(1303/3000) | Training loss: 2.1212376072293235 | Epoch(1304/3000) | Training loss: 2.1212375788461593 | Epoch(1305/3000) | Training loss: 2.121237561816261 | Epoch(1306/3000) | Training loss: 2.1212376015526906 | Epoch(1307/3000) | Training loss: 2.121237595876058 | Epoch(1308/3000) | Training loss: 2.1212375391097296 | Epoch(1309/3000) | Training loss: 2.1212376043910073 | Epoch(1310/3000) | Training loss: 2.121237567492894 | Epoch(1311/3000) | Training loss: 2.1212375788461593 | Epoch(1312/3000) | Training loss: 2.1212375476246788 | Epoch(1313/3000) | Training loss: 2.1212375504629954 | Epoch(1314/3000) | Training loss: 2.121237522079831 | Epoch(1315/3000) | Training loss: 2.1212375362714133 | Epoch(1316/3000) | Training loss: 2.121237567492894 | Epoch(1317/3000) | Training loss: 2.121237556139628 | Epoch(1318/3000) | Training loss: 2.1212375731695268 | Epoch(1319/3000) | Training loss: 2.121237584522792 | Epoch(1320/3000) | Training loss: 2.121237576007843 | Epoch(1321/3000) | Training loss: 2.121237513564882 | Epoch(1322/3000) | Training loss: 2.1212375164031982 | Epoch(1323/3000) | Training loss: 2.121237556139628 | Epoch(1324/3000) | Training loss: 2.1212375107265653 | Epoch(1325/3000) | Training loss: 2.121237527756464 | Epoch(1326/3000) | Training loss: 2.121237561816261 | Epoch(1327/3000) | Training loss: 2.121237527756464 | Epoch(1328/3000) | Training loss: 2.1212375249181474 | Epoch(1329/3000) | Training loss: 2.121237541948046 | Epoch(1330/3000) | Training loss: 2.1212375391097296 | Epoch(1331/3000) | Training loss: 2.1212375107265653 | Epoch(1332/3000) | Training loss: 2.1212375192415145 | Epoch(1333/3000) | Training loss: 2.1212375447863625 | Epoch(1334/3000) | Training loss: 2.1212375164031982 | Epoch(1335/3000) | Training loss: 2.121237507888249 | Epoch(1336/3000) | Training loss: 2.121237496534983 | Epoch(1337/3000) | Training loss: 2.1212375050499324 | Epoch(1338/3000) | Training loss: 2.1212375050499324 | Epoch(1339/3000) | Training loss: 2.121237493696667 | Epoch(1340/3000) | Training loss: 2.1212375164031982 | Epoch(1341/3000) | Training loss: 2.121237488020034 | Epoch(1342/3000) | Training loss: 2.121237502211616 | Epoch(1343/3000) | Training loss: 2.1212375050499324 | Epoch(1344/3000) | Training loss: 2.1212374709901356 | Epoch(1345/3000) | Training loss: 2.1212374596368697 | Epoch(1346/3000) | Training loss: 2.121237453960237 | Epoch(1347/3000) | Training loss: 2.121237493696667 | Epoch(1348/3000) | Training loss: 2.1212374596368697 | Epoch(1349/3000) | Training loss: 2.1212374426069713 | Epoch(1350/3000) | Training loss: 2.1212374170621238 | Epoch(1351/3000) | Training loss: 2.1212374709901356 | Epoch(1352/3000) | Training loss: 2.1212374454452876 | Epoch(1353/3000) | Training loss: 2.121237434092022 | Epoch(1354/3000) | Training loss: 2.1212374653135027 | Epoch(1355/3000) | Training loss: 2.1212374511219205 | Epoch(1356/3000) | Training loss: 2.1212374766667685 | Epoch(1357/3000) | Training loss: 2.121237428415389 | Epoch(1358/3000) | Training loss: 2.1212374596368697 | Epoch(1359/3000) | Training loss: 2.121237434092022 | Epoch(1360/3000) | Training loss: 2.121237453960237 | Epoch(1361/3000) | Training loss: 2.121237431253706 | Epoch(1362/3000) | Training loss: 2.121237431253706 | Epoch(1363/3000) | Training loss: 2.121237453960237 | Epoch(1364/3000) | Training loss: 2.121237431253706 | Epoch(1365/3000) | Training loss: 2.1212374369303384 | Epoch(1366/3000) | Training loss: 2.1212374482836043 | Epoch(1367/3000) | Training loss: 2.1212374511219205 | Epoch(1368/3000) | Training loss: 2.1212374085471746 | Epoch(1369/3000) | Training loss: 2.1212373971939087 | Epoch(1370/3000) | Training loss: 2.1212373744873774 | Epoch(1371/3000) | Training loss: 2.121237428415389 | Epoch(1372/3000) | Training loss: 2.1212374170621238 | Epoch(1373/3000) | Training loss: 2.121237400032225 | Epoch(1374/3000) | Training loss: 2.1212374028705416 | Epoch(1375/3000) | Training loss: 2.1212374085471746 | Epoch(1376/3000) | Training loss: 2.12123741990044 | Epoch(1377/3000) | Training loss: 2.121237414223807 | Epoch(1378/3000) | Training loss: 2.1212373830023266 | Epoch(1379/3000) | Training loss: 2.1212373602957952 | Epoch(1380/3000) | Training loss: 2.121237365972428 | Epoch(1381/3000) | Training loss: 2.1212373886789595 | Epoch(1382/3000) | Training loss: 2.1212373915172757 | Epoch(1383/3000) | Training loss: 2.121237371649061 | Epoch(1384/3000) | Training loss: 2.1212374085471746 | Epoch(1385/3000) | Training loss: 2.1212373602957952 | Epoch(1386/3000) | Training loss: 2.121237400032225 | Epoch(1387/3000) | Training loss: 2.1212373631341115 | Epoch(1388/3000) | Training loss: 2.1212373688107444 | Epoch(1389/3000) | Training loss: 2.121237346104213 | Epoch(1390/3000) | Training loss: 2.1212373688107444 | Epoch(1391/3000) | Training loss: 2.121237365972428 | Epoch(1392/3000) | Training loss: 2.1212373688107444 | Epoch(1393/3000) | Training loss: 2.1212373602957952 | Epoch(1394/3000) | Training loss: 2.121237357457479 | Epoch(1395/3000) | Training loss: 2.12123734894253 | Epoch(1396/3000) | Training loss: 2.121237351780846 | Epoch(1397/3000) | Training loss: 2.1212373205593655 | Epoch(1398/3000) | Training loss: 2.121237346104213 | Epoch(1399/3000) | Training loss: 2.121237351780846 | Epoch(1400/3000) | Training loss: 2.121237317721049 | Epoch(1401/3000) | Training loss: 2.12123734042758 | Epoch(1402/3000) | Training loss: 2.121237337589264 | Epoch(1403/3000) | Training loss: 2.1212373290743147 | Epoch(1404/3000) | Training loss: 2.1212373035294667 | Epoch(1405/3000) | Training loss: 2.1212373063677834 | Epoch(1406/3000) | Training loss: 2.1212373205593655 | Epoch(1407/3000) | Training loss: 2.1212373063677834 | Epoch(1408/3000) | Training loss: 2.121237297852834 | Epoch(1409/3000) | Training loss: 2.121237323397682 | Epoch(1410/3000) | Training loss: 2.1212373006911505 | Epoch(1411/3000) | Training loss: 2.1212373148827326 | Epoch(1412/3000) | Training loss: 2.1212372666313533 | Epoch(1413/3000) | Training loss: 2.1212373205593655 | Epoch(1414/3000) | Training loss: 2.1212373035294667 | Epoch(1415/3000) | Training loss: 2.121237263793037 | Epoch(1416/3000) | Training loss: 2.1212372921762013 | Epoch(1417/3000) | Training loss: 2.1212372666313533 | Epoch(1418/3000) | Training loss: 2.121237297852834 | Epoch(1419/3000) | Training loss: 2.12123726946967 | Epoch(1420/3000) | Training loss: 2.121237277984619 | Epoch(1421/3000) | Training loss: 2.1212372808229354 | Epoch(1422/3000) | Training loss: 2.1212372864995683 | Epoch(1423/3000) | Training loss: 2.1212372524397716 | Epoch(1424/3000) | Training loss: 2.121237283661252 | Epoch(1425/3000) | Training loss: 2.121237283661252 | Epoch(1426/3000) | Training loss: 2.121237275146303 | Epoch(1427/3000) | Training loss: 2.121237275146303 | Epoch(1428/3000) | Training loss: 2.1212372212182906 | Epoch(1429/3000) | Training loss: 2.1212372808229354 | Epoch(1430/3000) | Training loss: 2.121237263793037 | Epoch(1431/3000) | Training loss: 2.1212372864995683 | Epoch(1432/3000) | Training loss: 2.121237255278088 | Epoch(1433/3000) | Training loss: 2.1212372382481894 | Epoch(1434/3000) | Training loss: 2.1212372666313533 | Epoch(1435/3000) | Training loss: 2.1212372382481894 | Epoch(1436/3000) | Training loss: 2.1212372382481894 | Epoch(1437/3000) | Training loss: 2.1212372183799744 | Epoch(1438/3000) | Training loss: 2.1212372297332402 | Epoch(1439/3000) | Training loss: 2.1212372382481894 | Epoch(1440/3000) | Training loss: 2.1212372354098727 | Epoch(1441/3000) | Training loss: 2.121237215541658 | Epoch(1442/3000) | Training loss: 2.1212372325715565 | Epoch(1443/3000) | Training loss: 2.121237258116404 | Epoch(1444/3000) | Training loss: 2.121237209865025 | Epoch(1445/3000) | Training loss: 2.1212372070267085 | Epoch(1446/3000) | Training loss: 2.1212372240566073 | Epoch(1447/3000) | Training loss: 2.121237255278088 | Epoch(1448/3000) | Training loss: 2.121237195673443 | Epoch(1449/3000) | Training loss: 2.1212371985117593 | Epoch(1450/3000) | Training loss: 2.1212372212182906 | Epoch(1451/3000) | Training loss: 2.1212372183799744 | Epoch(1452/3000) | Training loss: 2.1212372382481894 | Epoch(1453/3000) | Training loss: 2.12123718999681 | Epoch(1454/3000) | Training loss: 2.1212372127033414 | Epoch(1455/3000) | Training loss: 2.121237187158494 | Epoch(1456/3000) | Training loss: 2.1212372041883922 | Epoch(1457/3000) | Training loss: 2.121237161613646 | Epoch(1458/3000) | Training loss: 2.1212372410865057 | Epoch(1459/3000) | Training loss: 2.1212372297332402 | Epoch(1460/3000) | Training loss: 2.121237201350076 | Epoch(1461/3000) | Training loss: 2.121237215541658 | Epoch(1462/3000) | Training loss: 2.1212371644519625 | Epoch(1463/3000) | Training loss: 2.12123718999681 | Epoch(1464/3000) | Training loss: 2.1212371729669117 | Epoch(1465/3000) | Training loss: 2.121237181481861 | Epoch(1466/3000) | Training loss: 2.1212372041883922 | Epoch(1467/3000) | Training loss: 2.1212371729669117 | Epoch(1468/3000) | Training loss: 2.1212371985117593 | Epoch(1469/3000) | Training loss: 2.1212371985117593 | Epoch(1470/3000) | Training loss: 2.1212371729669117 | Epoch(1471/3000) | Training loss: 2.121237161613646 | Epoch(1472/3000) | Training loss: 2.1212371445837475 | Epoch(1473/3000) | Training loss: 2.121237215541658 | Epoch(1474/3000) | Training loss: 2.121237107685634 | Epoch(1475/3000) | Training loss: 2.121237175805228 | Epoch(1476/3000) | Training loss: 2.1212371701285955 | Epoch(1477/3000) | Training loss: 2.1212371530986966 | Epoch(1478/3000) | Training loss: 2.121237167290279 | Epoch(1479/3000) | Training loss: 2.121237113362267 | Epoch(1480/3000) | Training loss: 2.121237116200583 | Epoch(1481/3000) | Training loss: 2.1212371559370133 | Epoch(1482/3000) | Training loss: 2.1212371559370133 | Epoch(1483/3000) | Training loss: 2.1212371190389 | Epoch(1484/3000) | Training loss: 2.121237127553849 | Epoch(1485/3000) | Training loss: 2.1212371190389 | Epoch(1486/3000) | Training loss: 2.121237102009001 | Epoch(1487/3000) | Training loss: 2.1212371247155324 | Epoch(1488/3000) | Training loss: 2.1212371105239507 | Epoch(1489/3000) | Training loss: 2.1212371502603804 | Epoch(1490/3000) | Training loss: 2.1212371048473178 | Epoch(1491/3000) | Training loss: 2.121237121877216 | Epoch(1492/3000) | Training loss: 2.121237099170685 | Epoch(1493/3000) | Training loss: 2.1212371048473178 | Epoch(1494/3000) | Training loss: 2.1212370622725714 | Epoch(1495/3000) | Training loss: 2.1212371190389 | Epoch(1496/3000) | Training loss: 2.1212371190389 | Epoch(1497/3000) | Training loss: 2.1212371587753296 | Epoch(1498/3000) | Training loss: 2.121237107685634 | Epoch(1499/3000) | Training loss: 2.121237102009001 | Epoch(1500/3000) | Training loss: 2.1212371105239507 | Epoch(1501/3000) | Training loss: 2.1212371190389 | Epoch(1502/3000) | Training loss: 2.1212371048473178 | Epoch(1503/3000) | Training loss: 2.121237045242673 | Epoch(1504/3000) | Training loss: 2.1212370906557356 | Epoch(1505/3000) | Training loss: 2.1212370622725714 | Epoch(1506/3000) | Training loss: 2.121237102009001 | Epoch(1507/3000) | Training loss: 2.121237093494052 | Epoch(1508/3000) | Training loss: 2.1212370764641535 | Epoch(1509/3000) | Training loss: 2.121237045242673 | Epoch(1510/3000) | Training loss: 2.121237099170685 | Epoch(1511/3000) | Training loss: 2.121237099170685 | Epoch(1512/3000) | Training loss: 2.121237093494052 | Epoch(1513/3000) | Training loss: 2.1212370906557356 | Epoch(1514/3000) | Training loss: 2.1212370736258372 | Epoch(1515/3000) | Training loss: 2.1212370480809892 | Epoch(1516/3000) | Training loss: 2.1212370764641535 | Epoch(1517/3000) | Training loss: 2.1212370565959384 | Epoch(1518/3000) | Training loss: 2.1212370679492043 | Epoch(1519/3000) | Training loss: 2.1212370906557356 | Epoch(1520/3000) | Training loss: 2.1212370424043563 | Epoch(1521/3000) | Training loss: 2.121237059434255 | Epoch(1522/3000) | Training loss: 2.121237050919306 | Epoch(1523/3000) | Training loss: 2.121237033889407 | Epoch(1524/3000) | Training loss: 2.121237045242673 | Epoch(1525/3000) | Training loss: 2.1212370565959384 | Epoch(1526/3000) | Training loss: 2.1212369998296103 | Epoch(1527/3000) | Training loss: 2.121237033889407 | Epoch(1528/3000) | Training loss: 2.1212370480809892 | Epoch(1529/3000) | Training loss: 2.1212370111828758 | Epoch(1530/3000) | Training loss: 2.121237045242673 | Epoch(1531/3000) | Training loss: 2.121237036727724 | Epoch(1532/3000) | Training loss: 2.1212370140211925 | Epoch(1533/3000) | Training loss: 2.1212369884763445 | Epoch(1534/3000) | Training loss: 2.121237036727724 | Epoch(1535/3000) | Training loss: 2.121237025374458 | Epoch(1536/3000) | Training loss: 2.1212369998296103 | Epoch(1537/3000) | Training loss: 2.121237031051091 | Epoch(1538/3000) | Training loss: 2.1212370225361417 | Epoch(1539/3000) | Training loss: 2.121237053757622 | Epoch(1540/3000) | Training loss: 2.1212369941529774 | Epoch(1541/3000) | Training loss: 2.1212370168595087 | Epoch(1542/3000) | Training loss: 2.1212370140211925 | Epoch(1543/3000) | Training loss: 2.1212369884763445 | Epoch(1544/3000) | Training loss: 2.1212369941529774 | Epoch(1545/3000) | Training loss: 2.1212369969912936 | Epoch(1546/3000) | Training loss: 2.1212370083445595 | Epoch(1547/3000) | Training loss: 2.1212370026679266 | Epoch(1548/3000) | Training loss: 2.1212370111828758 | Epoch(1549/3000) | Training loss: 2.1212369827997115 | Epoch(1550/3000) | Training loss: 2.121236991314661 | Epoch(1551/3000) | Training loss: 2.1212369827997115 | Epoch(1552/3000) | Training loss: 2.1212369544165477 | Epoch(1553/3000) | Training loss: 2.121236985638028 | Epoch(1554/3000) | Training loss: 2.121237019697825 | Epoch(1555/3000) | Training loss: 2.1212369998296103 | Epoch(1556/3000) | Training loss: 2.121236977123079 | Epoch(1557/3000) | Training loss: 2.1212369544165477 | Epoch(1558/3000) | Training loss: 2.12123696009318 | Epoch(1559/3000) | Training loss: 2.121236971446446 | Epoch(1560/3000) | Training loss: 2.1212369884763445 | Epoch(1561/3000) | Training loss: 2.121236965769813 | Epoch(1562/3000) | Training loss: 2.121236977123079 | Epoch(1563/3000) | Training loss: 2.121236965769813 | Epoch(1564/3000) | Training loss: 2.1212369686081294 | Epoch(1565/3000) | Training loss: 2.1212369941529774 | Epoch(1566/3000) | Training loss: 2.1212369827997115 | Epoch(1567/3000) | Training loss: 2.121236945901598 | Epoch(1568/3000) | Training loss: 2.12123696009318 | Epoch(1569/3000) | Training loss: 2.1212369742847623 | Epoch(1570/3000) | Training loss: 2.1212369487399148 | Epoch(1571/3000) | Training loss: 2.1212369402249656 | Epoch(1572/3000) | Training loss: 2.1212369203567505 | Epoch(1573/3000) | Training loss: 2.1212369487399148 | Epoch(1574/3000) | Training loss: 2.121236971446446 | Epoch(1575/3000) | Training loss: 2.1212369146801175 | Epoch(1576/3000) | Training loss: 2.121236943063282 | Epoch(1577/3000) | Training loss: 2.121236971446446 | Epoch(1578/3000) | Training loss: 2.12123696009318 | Epoch(1579/3000) | Training loss: 2.1212369260333834 | Epoch(1580/3000) | Training loss: 2.12123696009318 | Epoch(1581/3000) | Training loss: 2.1212369345483326 | Epoch(1582/3000) | Training loss: 2.1212369345483326 | Epoch(1583/3000) | Training loss: 2.1212369317100164 | Epoch(1584/3000) | Training loss: 2.121236965769813 | Epoch(1585/3000) | Training loss: 2.1212369175184342 | Epoch(1586/3000) | Training loss: 2.1212369487399148 | Epoch(1587/3000) | Training loss: 2.121236957254864 | Epoch(1588/3000) | Training loss: 2.1212369203567505 | Epoch(1589/3000) | Training loss: 2.1212369402249656 | Epoch(1590/3000) | Training loss: 2.1212369487399148 | Epoch(1591/3000) | Training loss: 2.121236937386649 | Epoch(1592/3000) | Training loss: 2.1212369203567505 | Epoch(1593/3000) | Training loss: 2.1212369203567505 | Epoch(1594/3000) | Training loss: 2.121236897650219 | Epoch(1595/3000) | Training loss: 2.121236897650219 | Epoch(1596/3000) | Training loss: 2.121236903326852 | Epoch(1597/3000) | Training loss: 2.1212369090034846 | Epoch(1598/3000) | Training loss: 2.1212369004885354 | Epoch(1599/3000) | Training loss: 2.1212369260333834 | Epoch(1600/3000) | Training loss: 2.121236897650219 | Epoch(1601/3000) | Training loss: 2.121236877782004 | Epoch(1602/3000) | Training loss: 2.1212368919735862 | Epoch(1603/3000) | Training loss: 2.1212368919735862 | Epoch(1604/3000) | Training loss: 2.1212368919735862 | Epoch(1605/3000) | Training loss: 2.121236894811903 | Epoch(1606/3000) | Training loss: 2.121236897650219 | Epoch(1607/3000) | Training loss: 2.1212368721053716 | Epoch(1608/3000) | Training loss: 2.121236874943688 | Epoch(1609/3000) | Training loss: 2.121236894811903 | Epoch(1610/3000) | Training loss: 2.121236863590422 | Epoch(1611/3000) | Training loss: 2.1212368721053716 | Epoch(1612/3000) | Training loss: 2.1212368862969533 | Epoch(1613/3000) | Training loss: 2.1212368664287387 | Epoch(1614/3000) | Training loss: 2.121236874943688 | Epoch(1615/3000) | Training loss: 2.1212368522371565 | Epoch(1616/3000) | Training loss: 2.1212368522371565 | Epoch(1617/3000) | Training loss: 2.1212368664287387 | Epoch(1618/3000) | Training loss: 2.121236863590422 | Epoch(1619/3000) | Training loss: 2.1212368862969533 | Epoch(1620/3000) | Training loss: 2.121236869267055 | Epoch(1621/3000) | Training loss: 2.121236835207258 | Epoch(1622/3000) | Training loss: 2.1212368607521057 | Epoch(1623/3000) | Training loss: 2.1212368437222073 | Epoch(1624/3000) | Training loss: 2.1212368579137895 | Epoch(1625/3000) | Training loss: 2.1212368408838906 | Epoch(1626/3000) | Training loss: 2.1212368522371565 | Epoch(1627/3000) | Training loss: 2.1212368465605236 | Epoch(1628/3000) | Training loss: 2.1212368522371565 | Epoch(1629/3000) | Training loss: 2.121236801147461 | Epoch(1630/3000) | Training loss: 2.121236829530625 | Epoch(1631/3000) | Training loss: 2.12123684939884 | Epoch(1632/3000) | Training loss: 2.121236812500727 | Epoch(1633/3000) | Training loss: 2.121236835207258 | Epoch(1634/3000) | Training loss: 2.1212368380455744 | Epoch(1635/3000) | Training loss: 2.121236806824094 | Epoch(1636/3000) | Training loss: 2.1212368522371565 | Epoch(1637/3000) | Training loss: 2.121236821015676 | Epoch(1638/3000) | Training loss: 2.1212368238539923 | Epoch(1639/3000) | Training loss: 2.1212368266923085 | Epoch(1640/3000) | Training loss: 2.1212367841175626 | Epoch(1641/3000) | Training loss: 2.121236812500727 | Epoch(1642/3000) | Training loss: 2.121236795470828 | Epoch(1643/3000) | Training loss: 2.121236812500727 | Epoch(1644/3000) | Training loss: 2.121236815339043 | Epoch(1645/3000) | Training loss: 2.121236829530625 | Epoch(1646/3000) | Training loss: 2.121236815339043 | Epoch(1647/3000) | Training loss: 2.1212368238539923 | Epoch(1648/3000) | Training loss: 2.121236803985777 | Epoch(1649/3000) | Training loss: 2.1212368238539923 | Epoch(1650/3000) | Training loss: 2.1212367727642967 | Epoch(1651/3000) | Training loss: 2.121236795470828 | Epoch(1652/3000) | Training loss: 2.1212367841175626 | Epoch(1653/3000) | Training loss: 2.121236803985777 | Epoch(1654/3000) | Training loss: 2.1212367841175626 | Epoch(1655/3000) | Training loss: 2.1212367557343983 | Epoch(1656/3000) | Training loss: 2.121236789794195 | Epoch(1657/3000) | Training loss: 2.121236789794195 | Epoch(1658/3000) | Training loss: 2.1212367926325117 | Epoch(1659/3000) | Training loss: 2.1212367784409296 | Epoch(1660/3000) | Training loss: 2.121236795470828 | Epoch(1661/3000) | Training loss: 2.1212367926325117 | Epoch(1662/3000) | Training loss: 2.121236801147461 | Epoch(1663/3000) | Training loss: 2.1212367784409296 | Epoch(1664/3000) | Training loss: 2.1212367756026134 | Epoch(1665/3000) | Training loss: 2.1212367756026134 | Epoch(1666/3000) | Training loss: 2.121236795470828 | Epoch(1667/3000) | Training loss: 2.1212367983091447 | Epoch(1668/3000) | Training loss: 2.1212367756026134 | Epoch(1669/3000) | Training loss: 2.121236803985777 | Epoch(1670/3000) | Training loss: 2.121236733027867 | Epoch(1671/3000) | Training loss: 2.1212367784409296 | Epoch(1672/3000) | Training loss: 2.1212367585727145 | Epoch(1673/3000) | Training loss: 2.1212367443811324 | Epoch(1674/3000) | Training loss: 2.121236786955879 | Epoch(1675/3000) | Training loss: 2.1212367443811324 | Epoch(1676/3000) | Training loss: 2.121236752896082 | Epoch(1677/3000) | Training loss: 2.1212367670876637 | Epoch(1678/3000) | Training loss: 2.121236741542816 | Epoch(1679/3000) | Training loss: 2.121236781279246 | Epoch(1680/3000) | Training loss: 2.121236727351234 | Epoch(1681/3000) | Training loss: 2.1212367557343983 | Epoch(1682/3000) | Training loss: 2.121236741542816 | Epoch(1683/3000) | Training loss: 2.1212367557343983 | Epoch(1684/3000) | Training loss: 2.121236752896082 | Epoch(1685/3000) | Training loss: 2.1212367443811324 | Epoch(1686/3000) | Training loss: 2.121236733027867 | Epoch(1687/3000) | Training loss: 2.1212367614110312 | Epoch(1688/3000) | Training loss: 2.1212367500577654 | Epoch(1689/3000) | Training loss: 2.121236718836285 | Epoch(1690/3000) | Training loss: 2.1212367387045 | Epoch(1691/3000) | Training loss: 2.1212367727642967 | Epoch(1692/3000) | Training loss: 2.121236733027867 | Epoch(1693/3000) | Training loss: 2.1212367557343983 | Epoch(1694/3000) | Training loss: 2.1212367387045 | Epoch(1695/3000) | Training loss: 2.1212366876148043 | Epoch(1696/3000) | Training loss: 2.121236713159652 | Epoch(1697/3000) | Training loss: 2.1212367159979686 | Epoch(1698/3000) | Training loss: 2.1212367500577654 | Epoch(1699/3000) | Training loss: 2.121236727351234 | Epoch(1700/3000) | Training loss: 2.1212367301895503 | Epoch(1701/3000) | Training loss: 2.1212367301895503 | Epoch(1702/3000) | Training loss: 2.1212367387045 | Epoch(1703/3000) | Training loss: 2.121236718836285 | Epoch(1704/3000) | Training loss: 2.121236721674601 | Epoch(1705/3000) | Training loss: 2.1212367159979686 | Epoch(1706/3000) | Training loss: 2.1212367159979686 | Epoch(1707/3000) | Training loss: 2.121236721674601 | Epoch(1708/3000) | Training loss: 2.1212366932914373 | Epoch(1709/3000) | Training loss: 2.1212366762615384 | Epoch(1710/3000) | Training loss: 2.1212366904531206 | Epoch(1711/3000) | Training loss: 2.1212367046447027 | Epoch(1712/3000) | Training loss: 2.121236747219449 | Epoch(1713/3000) | Training loss: 2.1212367159979686 | Epoch(1714/3000) | Training loss: 2.121236713159652 | Epoch(1715/3000) | Training loss: 2.1212366876148043 | Epoch(1716/3000) | Training loss: 2.121236713159652 | Epoch(1717/3000) | Training loss: 2.1212367046447027 | Epoch(1718/3000) | Training loss: 2.1212366876148043 | Epoch(1719/3000) | Training loss: 2.1212366932914373 | Epoch(1720/3000) | Training loss: 2.1212367103213357 | Epoch(1721/3000) | Training loss: 2.1212366762615384 | Epoch(1722/3000) | Training loss: 2.1212366876148043 | Epoch(1723/3000) | Training loss: 2.121236664908273 | Epoch(1724/3000) | Training loss: 2.121236673423222 | Epoch(1725/3000) | Training loss: 2.121236653555007 | Epoch(1726/3000) | Training loss: 2.1212366677465893 | Epoch(1727/3000) | Training loss: 2.1212366677465893 | Epoch(1728/3000) | Training loss: 2.121236647878374 | Epoch(1729/3000) | Training loss: 2.121236679099855 | Epoch(1730/3000) | Training loss: 2.121236664908273 | Epoch(1731/3000) | Training loss: 2.121236656393324 | Epoch(1732/3000) | Training loss: 2.1212366762615384 | Epoch(1733/3000) | Training loss: 2.1212366876148043 | Epoch(1734/3000) | Training loss: 2.121236653555007 | Epoch(1735/3000) | Training loss: 2.121236647878374 | Epoch(1736/3000) | Training loss: 2.1212366904531206 | Epoch(1737/3000) | Training loss: 2.1212366762615384 | Epoch(1738/3000) | Training loss: 2.121236653555007 | Epoch(1739/3000) | Training loss: 2.1212366876148043 | Epoch(1740/3000) | Training loss: 2.1212366819381714 | Epoch(1741/3000) | Training loss: 2.1212366422017417 | Epoch(1742/3000) | Training loss: 2.12123665923164 | Epoch(1743/3000) | Training loss: 2.121236650716691 | Epoch(1744/3000) | Training loss: 2.1212366620699563 | Epoch(1745/3000) | Training loss: 2.121236639363425 | Epoch(1746/3000) | Training loss: 2.121236653555007 | Epoch(1747/3000) | Training loss: 2.121236656393324 | Epoch(1748/3000) | Training loss: 2.1212366422017417 | Epoch(1749/3000) | Training loss: 2.1212366365251087 | Epoch(1750/3000) | Training loss: 2.1212366081419445 | Epoch(1751/3000) | Training loss: 2.121236647878374 | Epoch(1752/3000) | Training loss: 2.1212365996269953 | Epoch(1753/3000) | Training loss: 2.12123665923164 | Epoch(1754/3000) | Training loss: 2.1212366422017417 | Epoch(1755/3000) | Training loss: 2.12123657975878 | Epoch(1756/3000) | Training loss: 2.1212366422017417 | Epoch(1757/3000) | Training loss: 2.1212366024653115 | Epoch(1758/3000) | Training loss: 2.1212366365251087 | Epoch(1759/3000) | Training loss: 2.1212366194952104 | Epoch(1760/3000) | Training loss: 2.1212366024653115 | Epoch(1761/3000) | Training loss: 2.1212366053036282 | Epoch(1762/3000) | Training loss: 2.1212366138185774 | Epoch(1763/3000) | Training loss: 2.1212366024653115 | Epoch(1764/3000) | Training loss: 2.121236591112046 | Epoch(1765/3000) | Training loss: 2.121236625171843 | Epoch(1766/3000) | Training loss: 2.1212366365251087 | Epoch(1767/3000) | Training loss: 2.1212365740821477 | Epoch(1768/3000) | Training loss: 2.1212366081419445 | Epoch(1769/3000) | Training loss: 2.121236630848476 | Epoch(1770/3000) | Training loss: 2.1212366081419445 | Epoch(1771/3000) | Training loss: 2.1212365740821477 | Epoch(1772/3000) | Training loss: 2.121236562728882 | Epoch(1773/3000) | Training loss: 2.1212366138185774 | Epoch(1774/3000) | Training loss: 2.121236596788679 | Epoch(1775/3000) | Training loss: 2.1212365882737294 | Epoch(1776/3000) | Training loss: 2.12123657975878 | Epoch(1777/3000) | Training loss: 2.1212366194952104 | Epoch(1778/3000) | Training loss: 2.1212366138185774 | Epoch(1779/3000) | Training loss: 2.1212365740821477 | Epoch(1780/3000) | Training loss: 2.1212366194952104 | Epoch(1781/3000) | Training loss: 2.121236562728882 | Epoch(1782/3000) | Training loss: 2.121236585435413 | Epoch(1783/3000) | Training loss: 2.121236562728882 | Epoch(1784/3000) | Training loss: 2.1212365456989835 | Epoch(1785/3000) | Training loss: 2.121236551375616 | Epoch(1786/3000) | Training loss: 2.121236562728882 | Epoch(1787/3000) | Training loss: 2.121236585435413 | Epoch(1788/3000) | Training loss: 2.121236596788679 | Epoch(1789/3000) | Training loss: 2.1212366138185774 | Epoch(1790/3000) | Training loss: 2.121236557052249 | Epoch(1791/3000) | Training loss: 2.121236596788679 | Epoch(1792/3000) | Training loss: 2.121236591112046 | Epoch(1793/3000) | Training loss: 2.1212365598905656 | Epoch(1794/3000) | Training loss: 2.121236591112046 | Epoch(1795/3000) | Training loss: 2.121236551375616 | Epoch(1796/3000) | Training loss: 2.1212365343457176 | Epoch(1797/3000) | Training loss: 2.121236562728882 | Epoch(1798/3000) | Training loss: 2.121236557052249 | Epoch(1799/3000) | Training loss: 2.121236551375616 | Epoch(1800/3000) | Training loss: 2.121236551375616 | Epoch(1801/3000) | Training loss: 2.1212365740821477 | Epoch(1802/3000) | Training loss: 2.1212365371840343 | Epoch(1803/3000) | Training loss: 2.1212365400223505 | Epoch(1804/3000) | Training loss: 2.121236551375616 | Epoch(1805/3000) | Training loss: 2.1212365456989835 | Epoch(1806/3000) | Training loss: 2.1212365400223505 | Epoch(1807/3000) | Training loss: 2.121236551375616 | Epoch(1808/3000) | Training loss: 2.1212365684055148 | Epoch(1809/3000) | Training loss: 2.1212365400223505 | Epoch(1810/3000) | Training loss: 2.1212365343457176 | Epoch(1811/3000) | Training loss: 2.121236500285921 | Epoch(1812/3000) | Training loss: 2.1212365456989835 | Epoch(1813/3000) | Training loss: 2.1212365456989835 | Epoch(1814/3000) | Training loss: 2.1212365428606668 | Epoch(1815/3000) | Training loss: 2.1212365456989835 | Epoch(1816/3000) | Training loss: 2.121236494609288 | Epoch(1817/3000) | Training loss: 2.121236522992452 | Epoch(1818/3000) | Training loss: 2.1212365343457176 | Epoch(1819/3000) | Training loss: 2.1212365400223505 | Epoch(1820/3000) | Training loss: 2.1212365343457176 | Epoch(1821/3000) | Training loss: 2.1212365201541354 | Epoch(1822/3000) | Training loss: 2.1212365116391863 | Epoch(1823/3000) | Training loss: 2.121236522992452 | Epoch(1824/3000) | Training loss: 2.121236488932655 | Epoch(1825/3000) | Training loss: 2.1212365201541354 | Epoch(1826/3000) | Training loss: 2.121236500285921 | Epoch(1827/3000) | Training loss: 2.1212365456989835 | Epoch(1828/3000) | Training loss: 2.121236514477503 | Epoch(1829/3000) | Training loss: 2.121236500285921 | Epoch(1830/3000) | Training loss: 2.121236517315819 | Epoch(1831/3000) | Training loss: 2.121236494609288 | Epoch(1832/3000) | Training loss: 2.1212365400223505 | Epoch(1833/3000) | Training loss: 2.1212365116391863 | Epoch(1834/3000) | Training loss: 2.1212365116391863 | Epoch(1835/3000) | Training loss: 2.1212365116391863 | Epoch(1836/3000) | Training loss: 2.121236500285921 | Epoch(1837/3000) | Training loss: 2.1212365286690846 | Epoch(1838/3000) | Training loss: 2.1212364719027565 | Epoch(1839/3000) | Training loss: 2.121236488932655 | Epoch(1840/3000) | Training loss: 2.121236488932655 | Epoch(1841/3000) | Training loss: 2.121236497447604 | Epoch(1842/3000) | Training loss: 2.121236500285921 | Epoch(1843/3000) | Training loss: 2.121236497447604 | Epoch(1844/3000) | Training loss: 2.1212364775793895 | Epoch(1845/3000) | Training loss: 2.121236494609288 | Epoch(1846/3000) | Training loss: 2.121236483256022 | Epoch(1847/3000) | Training loss: 2.121236454872858 | Epoch(1848/3000) | Training loss: 2.121236488932655 | Epoch(1849/3000) | Training loss: 2.1212364775793895 | Epoch(1850/3000) | Training loss: 2.121236483256022 | Epoch(1851/3000) | Training loss: 2.1212364775793895 | Epoch(1852/3000) | Training loss: 2.1212364719027565 | Epoch(1853/3000) | Training loss: 2.1212364719027565 | Epoch(1854/3000) | Training loss: 2.121236483256022 | Epoch(1855/3000) | Training loss: 2.1212364719027565 | Epoch(1856/3000) | Training loss: 2.121236474741073 | Epoch(1857/3000) | Training loss: 2.1212364662261236 | Epoch(1858/3000) | Training loss: 2.1212364775793895 | Epoch(1859/3000) | Training loss: 2.1212364605494907 | Epoch(1860/3000) | Training loss: 2.12123646906444 | Epoch(1861/3000) | Training loss: 2.1212364719027565 | Epoch(1862/3000) | Training loss: 2.1212364775793895 | Epoch(1863/3000) | Training loss: 2.1212364719027565 | Epoch(1864/3000) | Training loss: 2.1212364719027565 | Epoch(1865/3000) | Training loss: 2.1212364491962252 | Epoch(1866/3000) | Training loss: 2.1212364463579085 | Epoch(1867/3000) | Training loss: 2.1212364378429593 | Epoch(1868/3000) | Training loss: 2.1212364435195923 | Epoch(1869/3000) | Training loss: 2.1212364378429593 | Epoch(1870/3000) | Training loss: 2.1212364775793895 | Epoch(1871/3000) | Training loss: 2.121236454872858 | Epoch(1872/3000) | Training loss: 2.1212364520345415 | Epoch(1873/3000) | Training loss: 2.1212364577111744 | Epoch(1874/3000) | Training loss: 2.121236454872858 | Epoch(1875/3000) | Training loss: 2.1212364662261236 | Epoch(1876/3000) | Training loss: 2.121236420813061 | Epoch(1877/3000) | Training loss: 2.1212364662261236 | Epoch(1878/3000) | Training loss: 2.1212364605494907 | Epoch(1879/3000) | Training loss: 2.1212364321663264 | Epoch(1880/3000) | Training loss: 2.1212364378429593 | Epoch(1881/3000) | Training loss: 2.1212364605494907 | Epoch(1882/3000) | Training loss: 2.1212364435195923 | Epoch(1883/3000) | Training loss: 2.1212364321663264 | Epoch(1884/3000) | Training loss: 2.1212364719027565 | Epoch(1885/3000) | Training loss: 2.121236420813061 | Epoch(1886/3000) | Training loss: 2.121236426489694 | Epoch(1887/3000) | Training loss: 2.121236420813061 | Epoch(1888/3000) | Training loss: 2.1212364179747447 | Epoch(1889/3000) | Training loss: 2.121236435004643 | Epoch(1890/3000) | Training loss: 2.121236415136428 | Epoch(1891/3000) | Training loss: 2.1212364321663264 | Epoch(1892/3000) | Training loss: 2.121236409459795 | Epoch(1893/3000) | Training loss: 2.121236400944846 | Epoch(1894/3000) | Training loss: 2.12123642932801 | Epoch(1895/3000) | Training loss: 2.1212364122981118 | Epoch(1896/3000) | Training loss: 2.121236440681276 | Epoch(1897/3000) | Training loss: 2.121236409459795 | Epoch(1898/3000) | Training loss: 2.1212364122981118 | Epoch(1899/3000) | Training loss: 2.1212363952682134 | Epoch(1900/3000) | Training loss: 2.121236409459795 | Epoch(1901/3000) | Training loss: 2.1212364122981118 | Epoch(1902/3000) | Training loss: 2.121236423651377 | Epoch(1903/3000) | Training loss: 2.1212364179747447 | Epoch(1904/3000) | Training loss: 2.1212364122981118 | Epoch(1905/3000) | Training loss: 2.1212364122981118 | Epoch(1906/3000) | Training loss: 2.121236400944846 | Epoch(1907/3000) | Training loss: 2.121236406621479 | Epoch(1908/3000) | Training loss: 2.121236426489694 | Epoch(1909/3000) | Training loss: 2.1212363867532638 | Epoch(1910/3000) | Training loss: 2.1212363924298967 | Epoch(1911/3000) | Training loss: 2.1212363924298967 | Epoch(1912/3000) | Training loss: 2.1212363952682134 | Epoch(1913/3000) | Training loss: 2.121236400944846 | Epoch(1914/3000) | Training loss: 2.1212363895915805 | Epoch(1915/3000) | Training loss: 2.121236361208416 | Epoch(1916/3000) | Training loss: 2.1212363839149475 | Epoch(1917/3000) | Training loss: 2.1212364321663264 | Epoch(1918/3000) | Training loss: 2.1212363810766313 | Epoch(1919/3000) | Training loss: 2.121236366885049 | Epoch(1920/3000) | Training loss: 2.1212363498551503 | Epoch(1921/3000) | Training loss: 2.1212363839149475 | Epoch(1922/3000) | Training loss: 2.1212363555317832 | Epoch(1923/3000) | Training loss: 2.1212363753999983 | Epoch(1924/3000) | Training loss: 2.1212363952682134 | Epoch(1925/3000) | Training loss: 2.1212363810766313 | Epoch(1926/3000) | Training loss: 2.1212364122981118 | Epoch(1927/3000) | Training loss: 2.121236341340201 | Epoch(1928/3000) | Training loss: 2.1212363214719865 | Epoch(1929/3000) | Training loss: 2.1212363697233654 | Epoch(1930/3000) | Training loss: 2.1212363555317832 | Epoch(1931/3000) | Training loss: 2.1212363697233654 | Epoch(1932/3000) | Training loss: 2.1212363555317832 | Epoch(1933/3000) | Training loss: 2.1212363895915805 | Epoch(1934/3000) | Training loss: 2.1212363640467324 | Epoch(1935/3000) | Training loss: 2.1212363782383146 | Epoch(1936/3000) | Training loss: 2.1212363498551503 | Epoch(1937/3000) | Training loss: 2.1212363299869357 | Epoch(1938/3000) | Training loss: 2.1212363498551503 | Epoch(1939/3000) | Training loss: 2.121236366885049 | Epoch(1940/3000) | Training loss: 2.1212363782383146 | Epoch(1941/3000) | Training loss: 2.121236344178518 | Epoch(1942/3000) | Training loss: 2.1212363299869357 | Epoch(1943/3000) | Training loss: 2.1212363697233654 | Epoch(1944/3000) | Training loss: 2.1212363782383146 | Epoch(1945/3000) | Training loss: 2.1212363498551503 | Epoch(1946/3000) | Training loss: 2.121236341340201 | Epoch(1947/3000) | Training loss: 2.121236344178518 | Epoch(1948/3000) | Training loss: 2.121236344178518 | Epoch(1949/3000) | Training loss: 2.1212363555317832 | Epoch(1950/3000) | Training loss: 2.121236352693467 | Epoch(1951/3000) | Training loss: 2.121236332825252 | Epoch(1952/3000) | Training loss: 2.121236312957037 | Epoch(1953/3000) | Training loss: 2.121236332825252 | Epoch(1954/3000) | Training loss: 2.1212363157953535 | Epoch(1955/3000) | Training loss: 2.1212363299869357 | Epoch(1956/3000) | Training loss: 2.121236327148619 | Epoch(1957/3000) | Training loss: 2.121236327148619 | Epoch(1958/3000) | Training loss: 2.1212363214719865 | Epoch(1959/3000) | Training loss: 2.1212363157953535 | Epoch(1960/3000) | Training loss: 2.1212363214719865 | Epoch(1961/3000) | Training loss: 2.121236341340201 | Epoch(1962/3000) | Training loss: 2.1212363299869357 | Epoch(1963/3000) | Training loss: 2.1212363356635686 | Epoch(1964/3000) | Training loss: 2.12123631863367 | Epoch(1965/3000) | Training loss: 2.1212363214719865 | Epoch(1966/3000) | Training loss: 2.1212363356635686 | Epoch(1967/3000) | Training loss: 2.1212363072804044 | Epoch(1968/3000) | Training loss: 2.1212363299869357 | Epoch(1969/3000) | Training loss: 2.1212363044420877 | Epoch(1970/3000) | Training loss: 2.121236338501885 | Epoch(1971/3000) | Training loss: 2.1212363157953535 | Epoch(1972/3000) | Training loss: 2.121236327148619 | Epoch(1973/3000) | Training loss: 2.121236284573873 | Epoch(1974/3000) | Training loss: 2.1212363157953535 | Epoch(1975/3000) | Training loss: 2.1212363072804044 | Epoch(1976/3000) | Training loss: 2.1212363101187206 | Epoch(1977/3000) | Training loss: 2.121236338501885 | Epoch(1978/3000) | Training loss: 2.12123631863367 | Epoch(1979/3000) | Training loss: 2.1212362930888222 | Epoch(1980/3000) | Training loss: 2.1212363157953535 | Epoch(1981/3000) | Training loss: 2.1212363157953535 | Epoch(1982/3000) | Training loss: 2.1212363044420877 | Epoch(1983/3000) | Training loss: 2.1212362817355563 | Epoch(1984/3000) | Training loss: 2.1212362930888222 | Epoch(1985/3000) | Training loss: 2.1212362874121893 | Epoch(1986/3000) | Training loss: 2.121236284573873 | Epoch(1987/3000) | Training loss: 2.1212362874121893 | Epoch(1988/3000) | Training loss: 2.1212362874121893 | Epoch(1989/3000) | Training loss: 2.121236298765455 | Epoch(1990/3000) | Training loss: 2.1212363044420877 | Epoch(1991/3000) | Training loss: 2.1212362959271385 | Epoch(1992/3000) | Training loss: 2.1212362930888222 | Epoch(1993/3000) | Training loss: 2.1212362817355563 | Epoch(1994/3000) | Training loss: 2.1212363299869357 | Epoch(1995/3000) | Training loss: 2.1212363101187206 | Epoch(1996/3000) | Training loss: 2.1212362817355563 | Epoch(1997/3000) | Training loss: 2.1212363044420877 | Epoch(1998/3000) | Training loss: 2.121236298765455 | Epoch(1999/3000) | Training loss: 2.1212363016037714 | Epoch(2000/3000) | Training loss: 2.12123627889724 | Epoch(2001/3000) | Training loss: 2.121236253352392 | Epoch(2002/3000) | Training loss: 2.1212362561907088 | Epoch(2003/3000) | Training loss: 2.121236273220607 | Epoch(2004/3000) | Training loss: 2.121236270382291 | Epoch(2005/3000) | Training loss: 2.1212362817355563 | Epoch(2006/3000) | Training loss: 2.1212362959271385 | Epoch(2007/3000) | Training loss: 2.121236270382291 | Epoch(2008/3000) | Training loss: 2.121236267543974 | Epoch(2009/3000) | Training loss: 2.1212362618673417 | Epoch(2010/3000) | Training loss: 2.121236259029025 | Epoch(2011/3000) | Training loss: 2.121236253352392 | Epoch(2012/3000) | Training loss: 2.121236276058924 | Epoch(2013/3000) | Training loss: 2.121236270382291 | Epoch(2014/3000) | Training loss: 2.121236284573873 | Epoch(2015/3000) | Training loss: 2.121236270382291 | Epoch(2016/3000) | Training loss: 2.1212362476757596 | Epoch(2017/3000) | Training loss: 2.1212362817355563 | Epoch(2018/3000) | Training loss: 2.121236259029025 | Epoch(2019/3000) | Training loss: 2.1212362561907088 | Epoch(2020/3000) | Training loss: 2.1212362874121893 | Epoch(2021/3000) | Training loss: 2.1212362476757596 | Epoch(2022/3000) | Training loss: 2.1212362476757596 | Epoch(2023/3000) | Training loss: 2.121236264705658 | Epoch(2024/3000) | Training loss: 2.1212362476757596 | Epoch(2025/3000) | Training loss: 2.121236244837443 | Epoch(2026/3000) | Training loss: 2.1212362817355563 | Epoch(2027/3000) | Training loss: 2.1212362618673417 | Epoch(2028/3000) | Training loss: 2.1212362278075445 | Epoch(2029/3000) | Training loss: 2.1212362476757596 | Epoch(2030/3000) | Training loss: 2.121236259029025 | Epoch(2031/3000) | Training loss: 2.1212362334841774 | Epoch(2032/3000) | Training loss: 2.1212362306458608 | Epoch(2033/3000) | Training loss: 2.1212362136159624 | Epoch(2034/3000) | Training loss: 2.121236253352392 | Epoch(2035/3000) | Training loss: 2.121236210777646 | Epoch(2036/3000) | Training loss: 2.121236216454279 | Epoch(2037/3000) | Training loss: 2.1212362363224937 | Epoch(2038/3000) | Training loss: 2.1212362249692283 | Epoch(2039/3000) | Training loss: 2.121236259029025 | Epoch(2040/3000) | Training loss: 2.121236216454279 | Epoch(2041/3000) | Training loss: 2.121236250514076 | Epoch(2042/3000) | Training loss: 2.1212362306458608 | Epoch(2043/3000) | Training loss: 2.1212362249692283 | Epoch(2044/3000) | Training loss: 2.1212362221309116 | Epoch(2045/3000) | Training loss: 2.121236250514076 | Epoch(2046/3000) | Training loss: 2.1212362363224937 | Epoch(2047/3000) | Training loss: 2.1212362079393294 | Epoch(2048/3000) | Training loss: 2.1212362278075445 | Epoch(2049/3000) | Training loss: 2.1212362618673417 | Epoch(2050/3000) | Training loss: 2.121236205101013 | Epoch(2051/3000) | Training loss: 2.1212362249692283 | Epoch(2052/3000) | Training loss: 2.1212362221309116 | Epoch(2053/3000) | Training loss: 2.121236202262697 | Epoch(2054/3000) | Training loss: 2.121236210777646 | Epoch(2055/3000) | Training loss: 2.1212361994243802 | Epoch(2056/3000) | Training loss: 2.121236216454279 | Epoch(2057/3000) | Training loss: 2.1212362391608104 | Epoch(2058/3000) | Training loss: 2.1212362079393294 | Epoch(2059/3000) | Training loss: 2.121236196586064 | Epoch(2060/3000) | Training loss: 2.1212362136159624 | Epoch(2061/3000) | Training loss: 2.121236210777646 | Epoch(2062/3000) | Training loss: 2.1212362221309116 | Epoch(2063/3000) | Training loss: 2.1212361994243802 | Epoch(2064/3000) | Training loss: 2.1212362079393294 | Epoch(2065/3000) | Training loss: 2.1212361937477473 | Epoch(2066/3000) | Training loss: 2.1212361994243802 | Epoch(2067/3000) | Training loss: 2.121236196586064 | Epoch(2068/3000) | Training loss: 2.121236188071115 | Epoch(2069/3000) | Training loss: 2.1212362079393294 | Epoch(2070/3000) | Training loss: 2.1212361596879505 | Epoch(2071/3000) | Training loss: 2.121236205101013 | Epoch(2072/3000) | Training loss: 2.1212361937477473 | Epoch(2073/3000) | Training loss: 2.121236188071115 | Epoch(2074/3000) | Training loss: 2.121236190909431 | Epoch(2075/3000) | Training loss: 2.1212361596879505 | Epoch(2076/3000) | Training loss: 2.1212361653645835 | Epoch(2077/3000) | Training loss: 2.1212362136159624 | Epoch(2078/3000) | Training loss: 2.121236190909431 | Epoch(2079/3000) | Training loss: 2.1212361795561656 | Epoch(2080/3000) | Training loss: 2.121236176717849 | Epoch(2081/3000) | Training loss: 2.1212361795561656 | Epoch(2082/3000) | Training loss: 2.121236196586064 | Epoch(2083/3000) | Training loss: 2.121236216454279 | Epoch(2084/3000) | Training loss: 2.1212361653645835 | Epoch(2085/3000) | Training loss: 2.1212361795561656 | Epoch(2086/3000) | Training loss: 2.121236182394482 | Epoch(2087/3000) | Training loss: 2.121236188071115 | Epoch(2088/3000) | Training loss: 2.121236210777646 | Epoch(2089/3000) | Training loss: 2.121236188071115 | Epoch(2090/3000) | Training loss: 2.1212361795561656 | Epoch(2091/3000) | Training loss: 2.1212361682028997 | Epoch(2092/3000) | Training loss: 2.1212361596879505 | Epoch(2093/3000) | Training loss: 2.121236162526267 | Epoch(2094/3000) | Training loss: 2.121236188071115 | Epoch(2095/3000) | Training loss: 2.121236162526267 | Epoch(2096/3000) | Training loss: 2.121236171041216 | Epoch(2097/3000) | Training loss: 2.121236185232798 | Epoch(2098/3000) | Training loss: 2.1212361682028997 | Epoch(2099/3000) | Training loss: 2.1212361738795327 | Epoch(2100/3000) | Training loss: 2.121236162526267 | Epoch(2101/3000) | Training loss: 2.121236136981419 | Epoch(2102/3000) | Training loss: 2.1212361483346847 | Epoch(2103/3000) | Training loss: 2.121236176717849 | Epoch(2104/3000) | Training loss: 2.1212361738795327 | Epoch(2105/3000) | Training loss: 2.121236136981419 | Epoch(2106/3000) | Training loss: 2.1212361454963684 | Epoch(2107/3000) | Training loss: 2.1212361568496343 | Epoch(2108/3000) | Training loss: 2.1212361795561656 | Epoch(2109/3000) | Training loss: 2.1212361313047863 | Epoch(2110/3000) | Training loss: 2.121236182394482 | Epoch(2111/3000) | Training loss: 2.1212361653645835 | Epoch(2112/3000) | Training loss: 2.1212361256281533 | Epoch(2113/3000) | Training loss: 2.121236119951521 | Epoch(2114/3000) | Training loss: 2.1212361454963684 | Epoch(2115/3000) | Training loss: 2.1212361398197355 | Epoch(2116/3000) | Training loss: 2.1212361313047863 | Epoch(2117/3000) | Training loss: 2.1212361511730013 | Epoch(2118/3000) | Training loss: 2.1212361313047863 | Epoch(2119/3000) | Training loss: 2.1212361483346847 | Epoch(2120/3000) | Training loss: 2.1212361454963684 | Epoch(2121/3000) | Training loss: 2.1212361341431025 | Epoch(2122/3000) | Training loss: 2.1212361313047863 | Epoch(2123/3000) | Training loss: 2.1212361057599387 | Epoch(2124/3000) | Training loss: 2.1212361256281533 | Epoch(2125/3000) | Training loss: 2.1212361256281533 | Epoch(2126/3000) | Training loss: 2.121236122789837 | Epoch(2127/3000) | Training loss: 2.1212361341431025 | Epoch(2128/3000) | Training loss: 2.1212361398197355 | Epoch(2129/3000) | Training loss: 2.121236119951521 | Epoch(2130/3000) | Training loss: 2.121236136981419 | Epoch(2131/3000) | Training loss: 2.1212360972449895 | Epoch(2132/3000) | Training loss: 2.121236136981419 | Epoch(2133/3000) | Training loss: 2.121236114274888 | Epoch(2134/3000) | Training loss: 2.121236102921622 | Epoch(2135/3000) | Training loss: 2.121236142658052 | Epoch(2136/3000) | Training loss: 2.1212361313047863 | Epoch(2137/3000) | Training loss: 2.1212361341431025 | Epoch(2138/3000) | Training loss: 2.121236108598255 | Epoch(2139/3000) | Training loss: 2.121236102921622 | Epoch(2140/3000) | Training loss: 2.1212360972449895 | Epoch(2141/3000) | Training loss: 2.1212361568496343 | Epoch(2142/3000) | Training loss: 2.1212361057599387 | Epoch(2143/3000) | Training loss: 2.121236119951521 | Epoch(2144/3000) | Training loss: 2.121236119951521 | Epoch(2145/3000) | Training loss: 2.121236122789837 | Epoch(2146/3000) | Training loss: 2.121236119951521 | Epoch(2147/3000) | Training loss: 2.1212360915683566 | Epoch(2148/3000) | Training loss: 2.121236114274888 | Epoch(2149/3000) | Training loss: 2.121236114274888 | Epoch(2150/3000) | Training loss: 2.121236117113204 | Epoch(2151/3000) | Training loss: 2.1212360915683566 | Epoch(2152/3000) | Training loss: 2.1212360830534074 | Epoch(2153/3000) | Training loss: 2.1212360858917236 | Epoch(2154/3000) | Training loss: 2.121236108598255 | Epoch(2155/3000) | Training loss: 2.121236108598255 | Epoch(2156/3000) | Training loss: 2.121236111436571 | Epoch(2157/3000) | Training loss: 2.1212361341431025 | Epoch(2158/3000) | Training loss: 2.121236119951521 | Epoch(2159/3000) | Training loss: 2.12123608873004 | Epoch(2160/3000) | Training loss: 2.1212360688618253 | Epoch(2161/3000) | Training loss: 2.1212360915683566 | Epoch(2162/3000) | Training loss: 2.121236117113204 | Epoch(2163/3000) | Training loss: 2.1212360858917236 | Epoch(2164/3000) | Training loss: 2.12123608873004 | Epoch(2165/3000) | Training loss: 2.1212360802150907 | Epoch(2166/3000) | Training loss: 2.1212360745384577 | Epoch(2167/3000) | Training loss: 2.1212360745384577 | Epoch(2168/3000) | Training loss: 2.1212360717001415 | Epoch(2169/3000) | Training loss: 2.1212360858917236 | Epoch(2170/3000) | Training loss: 2.1212360830534074 | Epoch(2171/3000) | Training loss: 2.1212360518319264 | Epoch(2172/3000) | Training loss: 2.1212360745384577 | Epoch(2173/3000) | Training loss: 2.1212360773767744 | Epoch(2174/3000) | Training loss: 2.1212360802150907 | Epoch(2175/3000) | Training loss: 2.1212360688618253 | Epoch(2176/3000) | Training loss: 2.121236102921622 | Epoch(2177/3000) | Training loss: 2.1212360575085594 | Epoch(2178/3000) | Training loss: 2.121236060346876 | Epoch(2179/3000) | Training loss: 2.12123608873004 | Epoch(2180/3000) | Training loss: 2.1212360688618253 | Epoch(2181/3000) | Training loss: 2.1212360745384577 | Epoch(2182/3000) | Training loss: 2.1212360518319264 | Epoch(2183/3000) | Training loss: 2.121236031963712 | Epoch(2184/3000) | Training loss: 2.121236046155294 | Epoch(2185/3000) | Training loss: 2.1212360773767744 | Epoch(2186/3000) | Training loss: 2.1212360631851923 | Epoch(2187/3000) | Training loss: 2.1212360858917236 | Epoch(2188/3000) | Training loss: 2.1212360773767744 | Epoch(2189/3000) | Training loss: 2.1212360915683566 | Epoch(2190/3000) | Training loss: 2.121236034802028 | Epoch(2191/3000) | Training loss: 2.1212361000833058 | Epoch(2192/3000) | Training loss: 2.1212360858917236 | Epoch(2193/3000) | Training loss: 2.1212360745384577 | Epoch(2194/3000) | Training loss: 2.121236034802028 | Epoch(2195/3000) | Training loss: 2.1212360518319264 | Epoch(2196/3000) | Training loss: 2.1212360717001415 | Epoch(2197/3000) | Training loss: 2.1212360858917236 | Epoch(2198/3000) | Training loss: 2.1212360688618253 | Epoch(2199/3000) | Training loss: 2.121236034802028 | Epoch(2200/3000) | Training loss: 2.121236046155294 | Epoch(2201/3000) | Training loss: 2.1212360688618253 | Epoch(2202/3000) | Training loss: 2.121236040478661 | Epoch(2203/3000) | Training loss: 2.1212360858917236 | Epoch(2204/3000) | Training loss: 2.1212360660235086 | Epoch(2205/3000) | Training loss: 2.1212360915683566 | Epoch(2206/3000) | Training loss: 2.121236029125395 | Epoch(2207/3000) | Training loss: 2.1212360575085594 | Epoch(2208/3000) | Training loss: 2.1212360007422313 | Epoch(2209/3000) | Training loss: 2.1212360234487626 | Epoch(2210/3000) | Training loss: 2.1212360433169772 | Epoch(2211/3000) | Training loss: 2.1212360575085594 | Epoch(2212/3000) | Training loss: 2.1212360575085594 | Epoch(2213/3000) | Training loss: 2.1212360518319264 | Epoch(2214/3000) | Training loss: 2.1212360575085594 | Epoch(2215/3000) | Training loss: 2.1212360120954967 | Epoch(2216/3000) | Training loss: 2.1212360688618253 | Epoch(2217/3000) | Training loss: 2.121236006418864 | Epoch(2218/3000) | Training loss: 2.1212360177721297 | Epoch(2219/3000) | Training loss: 2.1212360120954967 | Epoch(2220/3000) | Training loss: 2.1212360234487626 | Epoch(2221/3000) | Training loss: 2.121236020610446 | Epoch(2222/3000) | Training loss: 2.1212360234487626 | Epoch(2223/3000) | Training loss: 2.1212360376403447 | Epoch(2224/3000) | Training loss: 2.1212360177721297 | Epoch(2225/3000) | Training loss: 2.1212359950655983 | Epoch(2226/3000) | Training loss: 2.121236006418864 | Epoch(2227/3000) | Training loss: 2.1212359950655983 | Epoch(2228/3000) | Training loss: 2.1212360575085594 | Epoch(2229/3000) | Training loss: 2.1212360120954967 | Epoch(2230/3000) | Training loss: 2.1212360234487626 | Epoch(2231/3000) | Training loss: 2.121236020610446 | Epoch(2232/3000) | Training loss: 2.121236034802028 | Epoch(2233/3000) | Training loss: 2.121236006418864 | Epoch(2234/3000) | Training loss: 2.1212359950655983 | Epoch(2235/3000) | Training loss: 2.121236014933813 | Epoch(2236/3000) | Training loss: 2.121236006418864 | Epoch(2237/3000) | Training loss: 2.1212360120954967 | Epoch(2238/3000) | Training loss: 2.1212360035805475 | Epoch(2239/3000) | Training loss: 2.1212359780357 | Epoch(2240/3000) | Training loss: 2.121235972359067 | Epoch(2241/3000) | Training loss: 2.121236006418864 | Epoch(2242/3000) | Training loss: 2.121236029125395 | Epoch(2243/3000) | Training loss: 2.1212359950655983 | Epoch(2244/3000) | Training loss: 2.121235961005801 | Epoch(2245/3000) | Training loss: 2.1212359893889654 | Epoch(2246/3000) | Training loss: 2.1212359979039146 | Epoch(2247/3000) | Training loss: 2.1212359922272817 | Epoch(2248/3000) | Training loss: 2.1212359979039146 | Epoch(2249/3000) | Training loss: 2.121236029125395 | Epoch(2250/3000) | Training loss: 2.1212359950655983 | Epoch(2251/3000) | Training loss: 2.1212359751973833 | Epoch(2252/3000) | Training loss: 2.1212359496525357 | Epoch(2253/3000) | Training loss: 2.121235952490852 | Epoch(2254/3000) | Training loss: 2.1212359780357 | Epoch(2255/3000) | Training loss: 2.1212359837123325 | Epoch(2256/3000) | Training loss: 2.1212359893889654 | Epoch(2257/3000) | Training loss: 2.1212359950655983 | Epoch(2258/3000) | Training loss: 2.1212359837123325 | Epoch(2259/3000) | Training loss: 2.121235961005801 | Epoch(2260/3000) | Training loss: 2.1212359751973833 | Epoch(2261/3000) | Training loss: 2.121235963844118 | Epoch(2262/3000) | Training loss: 2.12123593829927 | Epoch(2263/3000) | Training loss: 2.1212359780357 | Epoch(2264/3000) | Training loss: 2.1212359950655983 | Epoch(2265/3000) | Training loss: 2.1212359695207503 | Epoch(2266/3000) | Training loss: 2.1212359893889654 | Epoch(2267/3000) | Training loss: 2.121235955329168 | Epoch(2268/3000) | Training loss: 2.1212359837123325 | Epoch(2269/3000) | Training loss: 2.121235980874016 | Epoch(2270/3000) | Training loss: 2.121235961005801 | Epoch(2271/3000) | Training loss: 2.1212359837123325 | Epoch(2272/3000) | Training loss: 2.1212359780357 | Epoch(2273/3000) | Training loss: 2.1212359893889654 | Epoch(2274/3000) | Training loss: 2.1212359979039146 | Epoch(2275/3000) | Training loss: 2.12123593829927 | Epoch(2276/3000) | Training loss: 2.121235972359067 | Epoch(2277/3000) | Training loss: 2.1212359439759028 | Epoch(2278/3000) | Training loss: 2.121235961005801 | Epoch(2279/3000) | Training loss: 2.121235966682434 | Epoch(2280/3000) | Training loss: 2.1212359695207503 | Epoch(2281/3000) | Training loss: 2.1212359269460044 | Epoch(2282/3000) | Training loss: 2.121235961005801 | Epoch(2283/3000) | Training loss: 2.121235961005801 | Epoch(2284/3000) | Training loss: 2.121235946814219 | Epoch(2285/3000) | Training loss: 2.1212359496525357 | Epoch(2286/3000) | Training loss: 2.121235952490852 | Epoch(2287/3000) | Training loss: 2.1212360007422313 | Epoch(2288/3000) | Training loss: 2.121235972359067 | Epoch(2289/3000) | Training loss: 2.1212359127544222 | Epoch(2290/3000) | Training loss: 2.1212359496525357 | Epoch(2291/3000) | Training loss: 2.1212359411375865 | Epoch(2292/3000) | Training loss: 2.1212359354609536 | Epoch(2293/3000) | Training loss: 2.1212359411375865 | Epoch(2294/3000) | Training loss: 2.1212359070777893 | Epoch(2295/3000) | Training loss: 2.121235955329168 | Epoch(2296/3000) | Training loss: 2.121235918431055 | Epoch(2297/3000) | Training loss: 2.12123593829927 | Epoch(2298/3000) | Training loss: 2.1212359496525357 | Epoch(2299/3000) | Training loss: 2.1212359496525357 | Epoch(2300/3000) | Training loss: 2.1212359439759028 | Epoch(2301/3000) | Training loss: 2.121235955329168 | Epoch(2302/3000) | Training loss: 2.1212359439759028 | Epoch(2303/3000) | Training loss: 2.1212359155927385 | Epoch(2304/3000) | Training loss: 2.121235955329168 | Epoch(2305/3000) | Training loss: 2.1212359099161056 | Epoch(2306/3000) | Training loss: 2.1212359155927385 | Epoch(2307/3000) | Training loss: 2.1212359837123325 | Epoch(2308/3000) | Training loss: 2.1212359212693714 | Epoch(2309/3000) | Training loss: 2.121235892886207 | Epoch(2310/3000) | Training loss: 2.12123593829927 | Epoch(2311/3000) | Training loss: 2.1212359212693714 | Epoch(2312/3000) | Training loss: 2.1212359155927385 | Epoch(2313/3000) | Training loss: 2.121235890047891 | Epoch(2314/3000) | Training loss: 2.12123593829927 | Epoch(2315/3000) | Training loss: 2.12123593829927 | Epoch(2316/3000) | Training loss: 2.121235884371258 | Epoch(2317/3000) | Training loss: 2.1212358815329417 | Epoch(2318/3000) | Training loss: 2.12123589856284 | Epoch(2319/3000) | Training loss: 2.121235946814219 | Epoch(2320/3000) | Training loss: 2.121235890047891 | Epoch(2321/3000) | Training loss: 2.1212359099161056 | Epoch(2322/3000) | Training loss: 2.12123589856284 | Epoch(2323/3000) | Training loss: 2.1212359099161056 | Epoch(2324/3000) | Training loss: 2.1212359155927385 | Epoch(2325/3000) | Training loss: 2.1212359070777893 | Epoch(2326/3000) | Training loss: 2.1212359014011564 | Epoch(2327/3000) | Training loss: 2.1212359099161056 | Epoch(2328/3000) | Training loss: 2.1212359070777893 | Epoch(2329/3000) | Training loss: 2.121235904239473 | Epoch(2330/3000) | Training loss: 2.121235890047891 | Epoch(2331/3000) | Training loss: 2.1212358872095742 | Epoch(2332/3000) | Training loss: 2.121235963844118 | Epoch(2333/3000) | Training loss: 2.1212359127544222 | Epoch(2334/3000) | Training loss: 2.121235904239473 | Epoch(2335/3000) | Training loss: 2.1212358957245234 | Epoch(2336/3000) | Training loss: 2.12123589856284 | Epoch(2337/3000) | Training loss: 2.121235892886207 | Epoch(2338/3000) | Training loss: 2.1212359241076877 | Epoch(2339/3000) | Training loss: 2.1212358957245234 | Epoch(2340/3000) | Training loss: 2.1212359070777893 | Epoch(2341/3000) | Training loss: 2.121235892886207 | Epoch(2342/3000) | Training loss: 2.121235890047891 | Epoch(2343/3000) | Training loss: 2.1212359070777893 | Epoch(2344/3000) | Training loss: 2.1212359127544222 | Epoch(2345/3000) | Training loss: 2.121235904239473 | Epoch(2346/3000) | Training loss: 2.121235884371258 | Epoch(2347/3000) | Training loss: 2.121235864503043 | Epoch(2348/3000) | Training loss: 2.1212358815329417 | Epoch(2349/3000) | Training loss: 2.12123589856284 | Epoch(2350/3000) | Training loss: 2.1212358872095742 | Epoch(2351/3000) | Training loss: 2.1212358957245234 | Epoch(2352/3000) | Training loss: 2.1212358616647267 | Epoch(2353/3000) | Training loss: 2.121235904239473 | Epoch(2354/3000) | Training loss: 2.121235870179676 | Epoch(2355/3000) | Training loss: 2.1212359127544222 | Epoch(2356/3000) | Training loss: 2.121235875856309 | Epoch(2357/3000) | Training loss: 2.121235878694625 | Epoch(2358/3000) | Training loss: 2.1212358673413596 | Epoch(2359/3000) | Training loss: 2.1212359127544222 | Epoch(2360/3000) | Training loss: 2.121235864503043 | Epoch(2361/3000) | Training loss: 2.1212358872095742 | Epoch(2362/3000) | Training loss: 2.1212358815329417 | Epoch(2363/3000) | Training loss: 2.12123589856284 | Epoch(2364/3000) | Training loss: 2.1212358673413596 | Epoch(2365/3000) | Training loss: 2.1212358815329417 | Epoch(2366/3000) | Training loss: 2.1212358361198786 | Epoch(2367/3000) | Training loss: 2.1212358389581953 | Epoch(2368/3000) | Training loss: 2.121235873017992 | Epoch(2369/3000) | Training loss: 2.121235878694625 | Epoch(2370/3000) | Training loss: 2.1212358531497775 | Epoch(2371/3000) | Training loss: 2.121235884371258 | Epoch(2372/3000) | Training loss: 2.1212358957245234 | Epoch(2373/3000) | Training loss: 2.1212358559880937 | Epoch(2374/3000) | Training loss: 2.121235864503043 | Epoch(2375/3000) | Training loss: 2.1212358588264104 | Epoch(2376/3000) | Training loss: 2.1212358332815624 | Epoch(2377/3000) | Training loss: 2.1212358872095742 | Epoch(2378/3000) | Training loss: 2.1212358616647267 | Epoch(2379/3000) | Training loss: 2.121235864503043 | Epoch(2380/3000) | Training loss: 2.1212358957245234 | Epoch(2381/3000) | Training loss: 2.121235870179676 | Epoch(2382/3000) | Training loss: 2.121235824766613 | Epoch(2383/3000) | Training loss: 2.1212358446348283 | Epoch(2384/3000) | Training loss: 2.121235864503043 | Epoch(2385/3000) | Training loss: 2.1212358559880937 | Epoch(2386/3000) | Training loss: 2.1212358588264104 | Epoch(2387/3000) | Training loss: 2.1212358474731445 | Epoch(2388/3000) | Training loss: 2.121235870179676 | Epoch(2389/3000) | Training loss: 2.121235870179676 | Epoch(2390/3000) | Training loss: 2.121235873017992 | Epoch(2391/3000) | Training loss: 2.121235830443246 | Epoch(2392/3000) | Training loss: 2.1212358559880937 | Epoch(2393/3000) | Training loss: 2.1212358588264104 | Epoch(2394/3000) | Training loss: 2.121235864503043 | Epoch(2395/3000) | Training loss: 2.1212358588264104 | Epoch(2396/3000) | Training loss: 2.1212358417965116 | Epoch(2397/3000) | Training loss: 2.1212358134133473 | Epoch(2398/3000) | Training loss: 2.1212358531497775 | Epoch(2399/3000) | Training loss: 2.121235824766613 | Epoch(2400/3000) | Training loss: 2.1212358332815624 | Epoch(2401/3000) | Training loss: 2.1212358474731445 | Epoch(2402/3000) | Training loss: 2.1212358616647267 | Epoch(2403/3000) | Training loss: 2.1212358474731445 | Epoch(2404/3000) | Training loss: 2.1212358134133473 | Epoch(2405/3000) | Training loss: 2.1212358446348283 | Epoch(2406/3000) | Training loss: 2.121235816251664 | Epoch(2407/3000) | Training loss: 2.1212358134133473 | Epoch(2408/3000) | Training loss: 2.1212358190899803 | Epoch(2409/3000) | Training loss: 2.121235802060082 | Epoch(2410/3000) | Training loss: 2.1212358417965116 | Epoch(2411/3000) | Training loss: 2.121235802060082 | Epoch(2412/3000) | Training loss: 2.121235816251664 | Epoch(2413/3000) | Training loss: 2.1212358389581953 | Epoch(2414/3000) | Training loss: 2.1212358276049295 | Epoch(2415/3000) | Training loss: 2.1212358531497775 | Epoch(2416/3000) | Training loss: 2.121235804898398 | Epoch(2417/3000) | Training loss: 2.121235810575031 | Epoch(2418/3000) | Training loss: 2.1212358389581953 | Epoch(2419/3000) | Training loss: 2.1212358474731445 | Epoch(2420/3000) | Training loss: 2.121235804898398 | Epoch(2421/3000) | Training loss: 2.1212358190899803 | Epoch(2422/3000) | Training loss: 2.1212358389581953 | Epoch(2423/3000) | Training loss: 2.121235790706816 | Epoch(2424/3000) | Training loss: 2.1212358276049295 | Epoch(2425/3000) | Training loss: 2.121235816251664 | Epoch(2426/3000) | Training loss: 2.121235796383449 | Epoch(2427/3000) | Training loss: 2.121235816251664 | Epoch(2428/3000) | Training loss: 2.121235790706816 | Epoch(2429/3000) | Training loss: 2.121235824766613 | Epoch(2430/3000) | Training loss: 2.121235807736715 | Epoch(2431/3000) | Training loss: 2.121235810575031 | Epoch(2432/3000) | Training loss: 2.1212357992217656 | Epoch(2433/3000) | Training loss: 2.121235810575031 | Epoch(2434/3000) | Training loss: 2.121235816251664 | Epoch(2435/3000) | Training loss: 2.121235790706816 | Epoch(2436/3000) | Training loss: 2.1212358389581953 | Epoch(2437/3000) | Training loss: 2.121235790706816 | Epoch(2438/3000) | Training loss: 2.1212358190899803 | Epoch(2439/3000) | Training loss: 2.1212357736769176 | Epoch(2440/3000) | Training loss: 2.1212358276049295 | Epoch(2441/3000) | Training loss: 2.121235802060082 | Epoch(2442/3000) | Training loss: 2.121235824766613 | Epoch(2443/3000) | Training loss: 2.1212357935451327 | Epoch(2444/3000) | Training loss: 2.121235776515234 | Epoch(2445/3000) | Training loss: 2.121235830443246 | Epoch(2446/3000) | Training loss: 2.1212357708386014 | Epoch(2447/3000) | Training loss: 2.1212357793535506 | Epoch(2448/3000) | Training loss: 2.121235807736715 | Epoch(2449/3000) | Training loss: 2.121235821928297 | Epoch(2450/3000) | Training loss: 2.1212357793535506 | Epoch(2451/3000) | Training loss: 2.1212357736769176 | Epoch(2452/3000) | Training loss: 2.121235796383449 | Epoch(2453/3000) | Training loss: 2.121235782191867 | Epoch(2454/3000) | Training loss: 2.121235782191867 | Epoch(2455/3000) | Training loss: 2.121235810575031 | Epoch(2456/3000) | Training loss: 2.1212357850301835 | Epoch(2457/3000) | Training loss: 2.121235802060082 | Epoch(2458/3000) | Training loss: 2.1212357935451327 | Epoch(2459/3000) | Training loss: 2.1212357878684998 | Epoch(2460/3000) | Training loss: 2.121235802060082 | Epoch(2461/3000) | Training loss: 2.12123574813207 | Epoch(2462/3000) | Training loss: 2.1212357935451327 | Epoch(2463/3000) | Training loss: 2.121235796383449 | Epoch(2464/3000) | Training loss: 2.1212357878684998 | Epoch(2465/3000) | Training loss: 2.1212357878684998 | Epoch(2466/3000) | Training loss: 2.1212358134133473 | Epoch(2467/3000) | Training loss: 2.1212357708386014 | Epoch(2468/3000) | Training loss: 2.121235804898398 | Epoch(2469/3000) | Training loss: 2.121235782191867 | Epoch(2470/3000) | Training loss: 2.121235776515234 | Epoch(2471/3000) | Training loss: 2.1212357708386014 | Epoch(2472/3000) | Training loss: 2.1212357509703863 | Epoch(2473/3000) | Training loss: 2.121235776515234 | Epoch(2474/3000) | Training loss: 2.121235782191867 | Epoch(2475/3000) | Training loss: 2.1212357651619684 | Epoch(2476/3000) | Training loss: 2.1212357708386014 | Epoch(2477/3000) | Training loss: 2.1212357594853355 | Epoch(2478/3000) | Training loss: 2.1212357566470192 | Epoch(2479/3000) | Training loss: 2.1212357566470192 | Epoch(2480/3000) | Training loss: 2.12123574813207 | Epoch(2481/3000) | Training loss: 2.1212357793535506 | Epoch(2482/3000) | Training loss: 2.1212357651619684 | Epoch(2483/3000) | Training loss: 2.12123574813207 | Epoch(2484/3000) | Training loss: 2.121235739617121 | Epoch(2485/3000) | Training loss: 2.1212357452937534 | Epoch(2486/3000) | Training loss: 2.1212357566470192 | Epoch(2487/3000) | Training loss: 2.1212357736769176 | Epoch(2488/3000) | Training loss: 2.121235742455437 | Epoch(2489/3000) | Training loss: 2.1212357736769176 | Epoch(2490/3000) | Training loss: 2.121235733940488 | Epoch(2491/3000) | Training loss: 2.1212357651619684 | Epoch(2492/3000) | Training loss: 2.121235733940488 | Epoch(2493/3000) | Training loss: 2.12123574813207 | Epoch(2494/3000) | Training loss: 2.1212357736769176 | Epoch(2495/3000) | Training loss: 2.1212357452937534 | Epoch(2496/3000) | Training loss: 2.1212357509703863 | Epoch(2497/3000) | Training loss: 2.1212357566470192 | Epoch(2498/3000) | Training loss: 2.121235736778804 | Epoch(2499/3000) | Training loss: 2.121235736778804 | Epoch(2500/3000) | Training loss: 2.1212357452937534 | Epoch(2501/3000) | Training loss: 2.121235728263855 | Epoch(2502/3000) | Training loss: 2.121235762323652 | Epoch(2503/3000) | Training loss: 2.1212357538087026 | Epoch(2504/3000) | Training loss: 2.121235742455437 | Epoch(2505/3000) | Training loss: 2.1212357793535506 | Epoch(2506/3000) | Training loss: 2.1212357311021712 | Epoch(2507/3000) | Training loss: 2.1212357509703863 | Epoch(2508/3000) | Training loss: 2.121235736778804 | Epoch(2509/3000) | Training loss: 2.121235733940488 | Epoch(2510/3000) | Training loss: 2.121235762323652 | Epoch(2511/3000) | Training loss: 2.1212357509703863 | Epoch(2512/3000) | Training loss: 2.121235719748906 | Epoch(2513/3000) | Training loss: 2.1212357680002847 | Epoch(2514/3000) | Training loss: 2.121235728263855 | Epoch(2515/3000) | Training loss: 2.121235733940488 | Epoch(2516/3000) | Training loss: 2.121235714072273 | Epoch(2517/3000) | Training loss: 2.121235716910589 | Epoch(2518/3000) | Training loss: 2.121235736778804 | Epoch(2519/3000) | Training loss: 2.121235722587222 | Epoch(2520/3000) | Training loss: 2.121235733940488 | Epoch(2521/3000) | Training loss: 2.121235722587222 | Epoch(2522/3000) | Training loss: 2.12123574813207 | Epoch(2523/3000) | Training loss: 2.121235722587222 | Epoch(2524/3000) | Training loss: 2.12123570839564 | Epoch(2525/3000) | Training loss: 2.1212357509703863 | Epoch(2526/3000) | Training loss: 2.121235722587222 | Epoch(2527/3000) | Training loss: 2.121235716910589 | Epoch(2528/3000) | Training loss: 2.121235716910589 | Epoch(2529/3000) | Training loss: 2.12123570839564 | Epoch(2530/3000) | Training loss: 2.1212356828507923 | Epoch(2531/3000) | Training loss: 2.1212357311021712 | Epoch(2532/3000) | Training loss: 2.1212357509703863 | Epoch(2533/3000) | Training loss: 2.1212357112339566 | Epoch(2534/3000) | Training loss: 2.12123570839564 | Epoch(2535/3000) | Training loss: 2.121235714072273 | Epoch(2536/3000) | Training loss: 2.121235728263855 | Epoch(2537/3000) | Training loss: 2.1212357254255387 | Epoch(2538/3000) | Training loss: 2.121235714072273 | Epoch(2539/3000) | Training loss: 2.121235728263855 | Epoch(2540/3000) | Training loss: 2.1212357055573237 | Epoch(2541/3000) | Training loss: 2.1212356885274253 | Epoch(2542/3000) | Training loss: 2.1212356856891086 | Epoch(2543/3000) | Training loss: 2.121235716910589 | Epoch(2544/3000) | Training loss: 2.121235733940488 | Epoch(2545/3000) | Training loss: 2.121235722587222 | Epoch(2546/3000) | Training loss: 2.121235716910589 | Epoch(2547/3000) | Training loss: 2.1212357055573237 | Epoch(2548/3000) | Training loss: 2.121235728263855 | Epoch(2549/3000) | Training loss: 2.1212356970423745 | Epoch(2550/3000) | Training loss: 2.121235722587222 | Epoch(2551/3000) | Training loss: 2.1212357027190074 | Epoch(2552/3000) | Training loss: 2.1212356771741594 | Epoch(2553/3000) | Training loss: 2.121235719748906 | Epoch(2554/3000) | Training loss: 2.1212357027190074 | Epoch(2555/3000) | Training loss: 2.121235716910589 | Epoch(2556/3000) | Training loss: 2.12123566865921 | Epoch(2557/3000) | Training loss: 2.1212357027190074 | Epoch(2558/3000) | Training loss: 2.1212356913657415 | Epoch(2559/3000) | Training loss: 2.1212356998806907 | Epoch(2560/3000) | Training loss: 2.1212356942040578 | Epoch(2561/3000) | Training loss: 2.1212356856891086 | Epoch(2562/3000) | Training loss: 2.1212356714975265 | Epoch(2563/3000) | Training loss: 2.1212356885274253 | Epoch(2564/3000) | Training loss: 2.1212356828507923 | Epoch(2565/3000) | Training loss: 2.121235714072273 | Epoch(2566/3000) | Training loss: 2.1212356998806907 | Epoch(2567/3000) | Training loss: 2.1212356913657415 | Epoch(2568/3000) | Training loss: 2.1212356771741594 | Epoch(2569/3000) | Training loss: 2.12123566865921 | Epoch(2570/3000) | Training loss: 2.1212356942040578 | Epoch(2571/3000) | Training loss: 2.1212356998806907 | Epoch(2572/3000) | Training loss: 2.121235654467628 | Epoch(2573/3000) | Training loss: 2.1212356629825773 | Epoch(2574/3000) | Training loss: 2.1212356856891086 | Epoch(2575/3000) | Training loss: 2.121235654467628 | Epoch(2576/3000) | Training loss: 2.12123570839564 | Epoch(2577/3000) | Training loss: 2.1212356913657415 | Epoch(2578/3000) | Training loss: 2.1212356714975265 | Epoch(2579/3000) | Training loss: 2.121235714072273 | Epoch(2580/3000) | Training loss: 2.121235680012476 | Epoch(2581/3000) | Training loss: 2.1212356629825773 | Epoch(2582/3000) | Training loss: 2.121235674335843 | Epoch(2583/3000) | Training loss: 2.1212356828507923 | Epoch(2584/3000) | Training loss: 2.121235680012476 | Epoch(2585/3000) | Training loss: 2.1212356573059443 | Epoch(2586/3000) | Training loss: 2.1212357055573237 | Epoch(2587/3000) | Training loss: 2.121235665820894 | Epoch(2588/3000) | Training loss: 2.1212356771741594 | Epoch(2589/3000) | Training loss: 2.121235674335843 | Epoch(2590/3000) | Training loss: 2.1212356573059443 | Epoch(2591/3000) | Training loss: 2.1212356828507923 | Epoch(2592/3000) | Training loss: 2.1212356913657415 | Epoch(2593/3000) | Training loss: 2.1212356573059443 | Epoch(2594/3000) | Training loss: 2.121235660144261 | Epoch(2595/3000) | Training loss: 2.1212356828507923 | Epoch(2596/3000) | Training loss: 2.1212356998806907 | Epoch(2597/3000) | Training loss: 2.12123566865921 | Epoch(2598/3000) | Training loss: 2.121235654467628 | Epoch(2599/3000) | Training loss: 2.1212356913657415 | Epoch(2600/3000) | Training loss: 2.1212356885274253 | Epoch(2601/3000) | Training loss: 2.121235660144261 | Epoch(2602/3000) | Training loss: 2.121235680012476 | Epoch(2603/3000) | Training loss: 2.121235665820894 | Epoch(2604/3000) | Training loss: 2.1212356828507923 | Epoch(2605/3000) | Training loss: 2.121235660144261 | Epoch(2606/3000) | Training loss: 2.12123566865921 | Epoch(2607/3000) | Training loss: 2.1212356629825773 | Epoch(2608/3000) | Training loss: 2.1212356629825773 | Epoch(2609/3000) | Training loss: 2.121235645952679 | Epoch(2610/3000) | Training loss: 2.1212356828507923 | Epoch(2611/3000) | Training loss: 2.1212356856891086 | Epoch(2612/3000) | Training loss: 2.121235651629312 | Epoch(2613/3000) | Training loss: 2.1212356771741594 | Epoch(2614/3000) | Training loss: 2.1212356629825773 | Epoch(2615/3000) | Training loss: 2.121235645952679 | Epoch(2616/3000) | Training loss: 2.1212356317610968 | Epoch(2617/3000) | Training loss: 2.12123566865921 | Epoch(2618/3000) | Training loss: 2.121235665820894 | Epoch(2619/3000) | Training loss: 2.121235648790995 | Epoch(2620/3000) | Training loss: 2.1212356289227805 | Epoch(2621/3000) | Training loss: 2.1212356771741594 | Epoch(2622/3000) | Training loss: 2.121235645952679 | Epoch(2623/3000) | Training loss: 2.1212356885274253 | Epoch(2624/3000) | Training loss: 2.121235654467628 | Epoch(2625/3000) | Training loss: 2.1212356573059443 | Epoch(2626/3000) | Training loss: 2.121235640276046 | Epoch(2627/3000) | Training loss: 2.121235640276046 | Epoch(2628/3000) | Training loss: 2.12123566865921 | Epoch(2629/3000) | Training loss: 2.12123566865921 | Epoch(2630/3000) | Training loss: 2.121235648790995 | Epoch(2631/3000) | Training loss: 2.121235674335843 | Epoch(2632/3000) | Training loss: 2.121235648790995 | Epoch(2633/3000) | Training loss: 2.1212356374377297 | Epoch(2634/3000) | Training loss: 2.121235654467628 | Epoch(2635/3000) | Training loss: 2.1212356374377297 | Epoch(2636/3000) | Training loss: 2.121235634599413 | Epoch(2637/3000) | Training loss: 2.1212356573059443 | Epoch(2638/3000) | Training loss: 2.121235640276046 | Epoch(2639/3000) | Training loss: 2.121235645952679 | Epoch(2640/3000) | Training loss: 2.121235640276046 | Epoch(2641/3000) | Training loss: 2.121235640276046 | Epoch(2642/3000) | Training loss: 2.121235634599413 | Epoch(2643/3000) | Training loss: 2.1212356232461476 | Epoch(2644/3000) | Training loss: 2.121235654467628 | Epoch(2645/3000) | Training loss: 2.1212356175695146 | Epoch(2646/3000) | Training loss: 2.1212356118928817 | Epoch(2647/3000) | Training loss: 2.1212356005396162 | Epoch(2648/3000) | Training loss: 2.1212356175695146 | Epoch(2649/3000) | Training loss: 2.1212356232461476 | Epoch(2650/3000) | Training loss: 2.1212356431143626 | Epoch(2651/3000) | Training loss: 2.1212356118928817 | Epoch(2652/3000) | Training loss: 2.121235660144261 | Epoch(2653/3000) | Training loss: 2.121235648790995 | Epoch(2654/3000) | Training loss: 2.1212355977012995 | Epoch(2655/3000) | Training loss: 2.1212356090545654 | Epoch(2656/3000) | Training loss: 2.1212355948629833 | Epoch(2657/3000) | Training loss: 2.1212356232461476 | Epoch(2658/3000) | Training loss: 2.121235626084464 | Epoch(2659/3000) | Training loss: 2.1212356204078313 | Epoch(2660/3000) | Training loss: 2.1212355891863504 | Epoch(2661/3000) | Training loss: 2.121235606216249 | Epoch(2662/3000) | Training loss: 2.1212355977012995 | Epoch(2663/3000) | Training loss: 2.1212356289227805 | Epoch(2664/3000) | Training loss: 2.121235626084464 | Epoch(2665/3000) | Training loss: 2.1212356289227805 | Epoch(2666/3000) | Training loss: 2.1212356204078313 | Epoch(2667/3000) | Training loss: 2.121235665820894 | Epoch(2668/3000) | Training loss: 2.1212356118928817 | Epoch(2669/3000) | Training loss: 2.121235592024667 | Epoch(2670/3000) | Training loss: 2.1212356033779325 | Epoch(2671/3000) | Training loss: 2.121235606216249 | Epoch(2672/3000) | Training loss: 2.121235626084464 | Epoch(2673/3000) | Training loss: 2.1212356289227805 | Epoch(2674/3000) | Training loss: 2.1212356005396162 | Epoch(2675/3000) | Training loss: 2.1212356175695146 | Epoch(2676/3000) | Training loss: 2.1212356147311984 | Epoch(2677/3000) | Training loss: 2.121235606216249 | Epoch(2678/3000) | Training loss: 2.1212356204078313 | Epoch(2679/3000) | Training loss: 2.1212356118928817 | Epoch(2680/3000) | Training loss: 2.1212356005396162 | Epoch(2681/3000) | Training loss: 2.1212356118928817 | Epoch(2682/3000) | Training loss: 2.121235606216249 | Epoch(2683/3000) | Training loss: 2.1212355948629833 | Epoch(2684/3000) | Training loss: 2.1212355891863504 | Epoch(2685/3000) | Training loss: 2.1212355948629833 | Epoch(2686/3000) | Training loss: 2.121235583509718 | Epoch(2687/3000) | Training loss: 2.121235592024667 | Epoch(2688/3000) | Training loss: 2.121235606216249 | Epoch(2689/3000) | Training loss: 2.1212356232461476 | Epoch(2690/3000) | Training loss: 2.1212356118928817 | Epoch(2691/3000) | Training loss: 2.1212356147311984 | Epoch(2692/3000) | Training loss: 2.1212355948629833 | Epoch(2693/3000) | Training loss: 2.121235586348034 | Epoch(2694/3000) | Training loss: 2.1212356175695146 | Epoch(2695/3000) | Training loss: 2.1212356090545654 | Epoch(2696/3000) | Training loss: 2.1212356232461476 | Epoch(2697/3000) | Training loss: 2.1212355948629833 | Epoch(2698/3000) | Training loss: 2.1212355948629833 | Epoch(2699/3000) | Training loss: 2.1212355948629833 | Epoch(2700/3000) | Training loss: 2.1212356033779325 | Epoch(2701/3000) | Training loss: 2.121235577833085 | Epoch(2702/3000) | Training loss: 2.1212355891863504 | Epoch(2703/3000) | Training loss: 2.1212356033779325 | Epoch(2704/3000) | Training loss: 2.1212355948629833 | Epoch(2705/3000) | Training loss: 2.121235583509718 | Epoch(2706/3000) | Training loss: 2.121235580671401 | Epoch(2707/3000) | Training loss: 2.1212355948629833 | Epoch(2708/3000) | Training loss: 2.121235586348034 | Epoch(2709/3000) | Training loss: 2.121235626084464 | Epoch(2710/3000) | Training loss: 2.1212355948629833 | Epoch(2711/3000) | Training loss: 2.1212355891863504 | Epoch(2712/3000) | Training loss: 2.1212355891863504 | Epoch(2713/3000) | Training loss: 2.121235580671401 | Epoch(2714/3000) | Training loss: 2.121235577833085 | Epoch(2715/3000) | Training loss: 2.121235586348034 | Epoch(2716/3000) | Training loss: 2.1212356005396162 | Epoch(2717/3000) | Training loss: 2.1212355977012995 | Epoch(2718/3000) | Training loss: 2.121235577833085 | Epoch(2719/3000) | Training loss: 2.121235580671401 | Epoch(2720/3000) | Training loss: 2.1212356090545654 | Epoch(2721/3000) | Training loss: 2.121235586348034 | Epoch(2722/3000) | Training loss: 2.121235572156452 | Epoch(2723/3000) | Training loss: 2.1212355749947682 | Epoch(2724/3000) | Training loss: 2.121235577833085 | Epoch(2725/3000) | Training loss: 2.1212356033779325 | Epoch(2726/3000) | Training loss: 2.121235583509718 | Epoch(2727/3000) | Training loss: 2.121235592024667 | Epoch(2728/3000) | Training loss: 2.1212355608031865 | Epoch(2729/3000) | Training loss: 2.121235592024667 | Epoch(2730/3000) | Training loss: 2.121235572156452 | Epoch(2731/3000) | Training loss: 2.121235566479819 | Epoch(2732/3000) | Training loss: 2.1212355494499207 | Epoch(2733/3000) | Training loss: 2.1212355551265536 | Epoch(2734/3000) | Training loss: 2.1212355608031865 | Epoch(2735/3000) | Training loss: 2.121235552288237 | Epoch(2736/3000) | Training loss: 2.121235566479819 | Epoch(2737/3000) | Training loss: 2.1212355551265536 | Epoch(2738/3000) | Training loss: 2.121235580671401 | Epoch(2739/3000) | Training loss: 2.1212355693181357 | Epoch(2740/3000) | Training loss: 2.1212355551265536 | Epoch(2741/3000) | Training loss: 2.1212355608031865 | Epoch(2742/3000) | Training loss: 2.121235572156452 | Epoch(2743/3000) | Training loss: 2.121235566479819 | Epoch(2744/3000) | Training loss: 2.1212355608031865 | Epoch(2745/3000) | Training loss: 2.1212355352583385 | Epoch(2746/3000) | Training loss: 2.121235552288237 | Epoch(2747/3000) | Training loss: 2.1212355466116044 | Epoch(2748/3000) | Training loss: 2.1212355295817056 | Epoch(2749/3000) | Training loss: 2.1212355380966548 | Epoch(2750/3000) | Training loss: 2.1212355494499207 | Epoch(2751/3000) | Training loss: 2.1212355749947682 | Epoch(2752/3000) | Training loss: 2.1212355437732877 | Epoch(2753/3000) | Training loss: 2.121235552288237 | Epoch(2754/3000) | Training loss: 2.1212355749947682 | Epoch(2755/3000) | Training loss: 2.1212355693181357 | Epoch(2756/3000) | Training loss: 2.121235577833085 | Epoch(2757/3000) | Training loss: 2.121235572156452 | Epoch(2758/3000) | Training loss: 2.121235566479819 | Epoch(2759/3000) | Training loss: 2.1212355380966548 | Epoch(2760/3000) | Training loss: 2.12123555796487 | Epoch(2761/3000) | Training loss: 2.1212355324200223 | Epoch(2762/3000) | Training loss: 2.1212355380966548 | Epoch(2763/3000) | Training loss: 2.1212355324200223 | Epoch(2764/3000) | Training loss: 2.1212355380966548 | Epoch(2765/3000) | Training loss: 2.1212355494499207 | Epoch(2766/3000) | Training loss: 2.1212355494499207 | Epoch(2767/3000) | Training loss: 2.1212355324200223 | Epoch(2768/3000) | Training loss: 2.1212355551265536 | Epoch(2769/3000) | Training loss: 2.1212355466116044 | Epoch(2770/3000) | Training loss: 2.1212355466116044 | Epoch(2771/3000) | Training loss: 2.1212355295817056 | Epoch(2772/3000) | Training loss: 2.1212355551265536 | Epoch(2773/3000) | Training loss: 2.121235572156452 | Epoch(2774/3000) | Training loss: 2.1212355380966548 | Epoch(2775/3000) | Training loss: 2.121235592024667 | Epoch(2776/3000) | Training loss: 2.1212355380966548 | Epoch(2777/3000) | Training loss: 2.12123555796487 | Epoch(2778/3000) | Training loss: 2.121235512551807 | Epoch(2779/3000) | Training loss: 2.1212355324200223 | Epoch(2780/3000) | Training loss: 2.1212355267433893 | Epoch(2781/3000) | Training loss: 2.121235563641503 | Epoch(2782/3000) | Training loss: 2.1212355068751743 | Epoch(2783/3000) | Training loss: 2.1212355352583385 | Epoch(2784/3000) | Training loss: 2.1212355551265536 | Epoch(2785/3000) | Training loss: 2.12123555796487 | Epoch(2786/3000) | Training loss: 2.1212355494499207 | Epoch(2787/3000) | Training loss: 2.121235495521909 | Epoch(2788/3000) | Training loss: 2.1212355295817056 | Epoch(2789/3000) | Training loss: 2.1212355551265536 | Epoch(2790/3000) | Training loss: 2.1212355437732877 | Epoch(2791/3000) | Training loss: 2.1212355153901234 | Epoch(2792/3000) | Training loss: 2.1212355409349715 | Epoch(2793/3000) | Training loss: 2.1212355409349715 | Epoch(2794/3000) | Training loss: 2.12123555796487 | Epoch(2795/3000) | Training loss: 2.121235523905073 | Epoch(2796/3000) | Training loss: 2.1212355267433893 | Epoch(2797/3000) | Training loss: 2.12123551822844 | Epoch(2798/3000) | Training loss: 2.121235566479819 | Epoch(2799/3000) | Training loss: 2.1212355153901234 | Epoch(2800/3000) | Training loss: 2.121235492683592 | Epoch(2801/3000) | Training loss: 2.12123551822844 | Epoch(2802/3000) | Training loss: 2.1212355324200223 | Epoch(2803/3000) | Training loss: 2.12123551822844 | Epoch(2804/3000) | Training loss: 2.1212355466116044 | Epoch(2805/3000) | Training loss: 2.1212355352583385 | Epoch(2806/3000) | Training loss: 2.1212355210667564 | Epoch(2807/3000) | Training loss: 2.121235504036858 | Epoch(2808/3000) | Training loss: 2.12123547849201 | Epoch(2809/3000) | Training loss: 2.1212355324200223 | Epoch(2810/3000) | Training loss: 2.1212355210667564 | Epoch(2811/3000) | Training loss: 2.121235523905073 | Epoch(2812/3000) | Training loss: 2.1212355352583385 | Epoch(2813/3000) | Training loss: 2.1212355153901234 | Epoch(2814/3000) | Training loss: 2.1212355267433893 | Epoch(2815/3000) | Training loss: 2.1212355267433893 | Epoch(2816/3000) | Training loss: 2.1212355153901234 | Epoch(2817/3000) | Training loss: 2.1212355068751743 | Epoch(2818/3000) | Training loss: 2.1212355409349715 | Epoch(2819/3000) | Training loss: 2.1212354870069596 | Epoch(2820/3000) | Training loss: 2.1212355068751743 | Epoch(2821/3000) | Training loss: 2.12123551822844 | Epoch(2822/3000) | Training loss: 2.1212355295817056 | Epoch(2823/3000) | Training loss: 2.1212355068751743 | Epoch(2824/3000) | Training loss: 2.1212355267433893 | Epoch(2825/3000) | Training loss: 2.1212354870069596 | Epoch(2826/3000) | Training loss: 2.1212354557854787 | Epoch(2827/3000) | Training loss: 2.1212355409349715 | Epoch(2828/3000) | Training loss: 2.1212355409349715 | Epoch(2829/3000) | Training loss: 2.1212354330789474 | Epoch(2830/3000) | Training loss: 2.1212355068751743 | Epoch(2831/3000) | Training loss: 2.1212354813303267 | Epoch(2832/3000) | Training loss: 2.121235492683592 | Epoch(2833/3000) | Training loss: 2.1212354813303267 | Epoch(2834/3000) | Training loss: 2.1212354870069596 | Epoch(2835/3000) | Training loss: 2.121235492683592 | Epoch(2836/3000) | Training loss: 2.1212355153901234 | Epoch(2837/3000) | Training loss: 2.121235504036858 | Epoch(2838/3000) | Training loss: 2.121235495521909 | Epoch(2839/3000) | Training loss: 2.1212355153901234 | Epoch(2840/3000) | Training loss: 2.1212354472705295 | Epoch(2841/3000) | Training loss: 2.1212354756536937 | Epoch(2842/3000) | Training loss: 2.121235484168643 | Epoch(2843/3000) | Training loss: 2.12123547849201 | Epoch(2844/3000) | Training loss: 2.1212354813303267 | Epoch(2845/3000) | Training loss: 2.121235509713491 | Epoch(2846/3000) | Training loss: 2.121235504036858 | Epoch(2847/3000) | Training loss: 2.121235489845276 | Epoch(2848/3000) | Training loss: 2.121235495521909 | Epoch(2849/3000) | Training loss: 2.121235484168643 | Epoch(2850/3000) | Training loss: 2.121235498360225 | Epoch(2851/3000) | Training loss: 2.1212354614621116 | Epoch(2852/3000) | Training loss: 2.1212354728153775 | Epoch(2853/3000) | Training loss: 2.1212354813303267 | Epoch(2854/3000) | Training loss: 2.121235495521909 | Epoch(2855/3000) | Training loss: 2.1212354586237954 | Epoch(2856/3000) | Training loss: 2.1212354586237954 | Epoch(2857/3000) | Training loss: 2.121235498360225 | Epoch(2858/3000) | Training loss: 2.1212355153901234 | Epoch(2859/3000) | Training loss: 2.1212354671387446 | Epoch(2860/3000) | Training loss: 2.121235489845276 | Epoch(2861/3000) | Training loss: 2.1212354643004283 | Epoch(2862/3000) | Training loss: 2.121235484168643 | Epoch(2863/3000) | Training loss: 2.121235489845276 | Epoch(2864/3000) | Training loss: 2.1212354643004283 | Epoch(2865/3000) | Training loss: 2.121235469977061 | Epoch(2866/3000) | Training loss: 2.121235484168643 | Epoch(2867/3000) | Training loss: 2.1212354756536937 | Epoch(2868/3000) | Training loss: 2.1212354813303267 | Epoch(2869/3000) | Training loss: 2.1212354614621116 | Epoch(2870/3000) | Training loss: 2.1212354728153775 | Epoch(2871/3000) | Training loss: 2.121235492683592 | Epoch(2872/3000) | Training loss: 2.1212354614621116 | Epoch(2873/3000) | Training loss: 2.1212354756536937 | Epoch(2874/3000) | Training loss: 2.12123547849201 | Epoch(2875/3000) | Training loss: 2.1212354614621116 | Epoch(2876/3000) | Training loss: 2.1212354643004283 | Epoch(2877/3000) | Training loss: 2.1212354387555803 | Epoch(2878/3000) | Training loss: 2.1212354813303267 | Epoch(2879/3000) | Training loss: 2.121235469977061 | Epoch(2880/3000) | Training loss: 2.1212354671387446 | Epoch(2881/3000) | Training loss: 2.121235469977061 | Epoch(2882/3000) | Training loss: 2.1212354756536937 | Epoch(2883/3000) | Training loss: 2.1212354813303267 | Epoch(2884/3000) | Training loss: 2.1212354472705295 | Epoch(2885/3000) | Training loss: 2.1212354557854787 | Epoch(2886/3000) | Training loss: 2.1212354557854787 | Epoch(2887/3000) | Training loss: 2.121235469977061 | Epoch(2888/3000) | Training loss: 2.1212354529471624 | Epoch(2889/3000) | Training loss: 2.1212354813303267 | Epoch(2890/3000) | Training loss: 2.1212354671387446 | Epoch(2891/3000) | Training loss: 2.1212354614621116 | Epoch(2892/3000) | Training loss: 2.1212354586237954 | Epoch(2893/3000) | Training loss: 2.1212354529471624 | Epoch(2894/3000) | Training loss: 2.1212354472705295 | Epoch(2895/3000) | Training loss: 2.1212354557854787 | Epoch(2896/3000) | Training loss: 2.1212354330789474 | Epoch(2897/3000) | Training loss: 2.1212354529471624 | Epoch(2898/3000) | Training loss: 2.1212354444322132 | Epoch(2899/3000) | Training loss: 2.1212354472705295 | Epoch(2900/3000) | Training loss: 2.12123547849201 | Epoch(2901/3000) | Training loss: 2.121235469977061 | Epoch(2902/3000) | Training loss: 2.1212354444322132 | Epoch(2903/3000) | Training loss: 2.1212354614621116 | Epoch(2904/3000) | Training loss: 2.1212354671387446 | Epoch(2905/3000) | Training loss: 2.1212354671387446 | Epoch(2906/3000) | Training loss: 2.1212354529471624 | Epoch(2907/3000) | Training loss: 2.1212354330789474 | Epoch(2908/3000) | Training loss: 2.1212354529471624 | Epoch(2909/3000) | Training loss: 2.121235430240631 | Epoch(2910/3000) | Training loss: 2.1212354387555803 | Epoch(2911/3000) | Training loss: 2.121235450108846 | Epoch(2912/3000) | Training loss: 2.121235450108846 | Epoch(2913/3000) | Training loss: 2.1212354728153775 | Epoch(2914/3000) | Training loss: 2.1212354557854787 | Epoch(2915/3000) | Training loss: 2.1212354671387446 | Epoch(2916/3000) | Training loss: 2.1212354557854787 | Epoch(2917/3000) | Training loss: 2.121235421725682 | Epoch(2918/3000) | Training loss: 2.1212354387555803 | Epoch(2919/3000) | Training loss: 2.1212354387555803 | Epoch(2920/3000) | Training loss: 2.121235450108846 | Epoch(2921/3000) | Training loss: 2.1212354529471624 | Epoch(2922/3000) | Training loss: 2.1212354728153775 | Epoch(2923/3000) | Training loss: 2.121235430240631 | Epoch(2924/3000) | Training loss: 2.121235410372416 | Epoch(2925/3000) | Training loss: 2.121235450108846 | Epoch(2926/3000) | Training loss: 2.1212354444322132 | Epoch(2927/3000) | Training loss: 2.12123547849201 | Epoch(2928/3000) | Training loss: 2.121235427402315 | Epoch(2929/3000) | Training loss: 2.1212354387555803 | Epoch(2930/3000) | Training loss: 2.121235435917264 | Epoch(2931/3000) | Training loss: 2.1212354444322132 | Epoch(2932/3000) | Training loss: 2.1212354387555803 | Epoch(2933/3000) | Training loss: 2.121235435917264 | Epoch(2934/3000) | Training loss: 2.121235401857467 | Epoch(2935/3000) | Training loss: 2.121235416049049 | Epoch(2936/3000) | Training loss: 2.121235435917264 | Epoch(2937/3000) | Training loss: 2.121235450108846 | Epoch(2938/3000) | Training loss: 2.1212354586237954 | Epoch(2939/3000) | Training loss: 2.121235427402315 | Epoch(2940/3000) | Training loss: 2.1212354330789474 | Epoch(2941/3000) | Training loss: 2.121235441593897 | Epoch(2942/3000) | Training loss: 2.1212354444322132 | Epoch(2943/3000) | Training loss: 2.121235416049049 | Epoch(2944/3000) | Training loss: 2.121235416049049 | Epoch(2945/3000) | Training loss: 2.121235424563998 | Epoch(2946/3000) | Training loss: 2.121235421725682 | Epoch(2947/3000) | Training loss: 2.121235416049049 | Epoch(2948/3000) | Training loss: 2.1212354557854787 | Epoch(2949/3000) | Training loss: 2.1212354387555803 | Epoch(2950/3000) | Training loss: 2.121235418887365 | Epoch(2951/3000) | Training loss: 2.121235401857467 | Epoch(2952/3000) | Training loss: 2.121235441593897 | Epoch(2953/3000) | Training loss: 2.121235418887365 | Epoch(2954/3000) | Training loss: 2.1212354529471624 | Epoch(2955/3000) | Training loss: 2.121235421725682 | Epoch(2956/3000) | Training loss: 2.121235418887365 | Epoch(2957/3000) | Training loss: 2.121235427402315 | Epoch(2958/3000) | Training loss: 2.1212354046957835 | Epoch(2959/3000) | Training loss: 2.1212354529471624 | Epoch(2960/3000) | Training loss: 2.1212353933425176 | Epoch(2961/3000) | Training loss: 2.121235427402315 | Epoch(2962/3000) | Training loss: 2.121235410372416 | Epoch(2963/3000) | Training loss: 2.121235396180834 | Epoch(2964/3000) | Training loss: 2.1212354387555803 | Epoch(2965/3000) | Training loss: 2.121235421725682 | Epoch(2966/3000) | Training loss: 2.121235396180834 | Epoch(2967/3000) | Training loss: 2.1212354046957835 | Epoch(2968/3000) | Training loss: 2.1212353848275685 | Epoch(2969/3000) | Training loss: 2.1212353876658847 | Epoch(2970/3000) | Training loss: 2.1212354330789474 | Epoch(2971/3000) | Training loss: 2.121235416049049 | Epoch(2972/3000) | Training loss: 2.121235410372416 | Epoch(2973/3000) | Training loss: 2.1212354330789474 | Epoch(2974/3000) | Training loss: 2.121235430240631 | Epoch(2975/3000) | Training loss: 2.121235410372416 | Epoch(2976/3000) | Training loss: 2.1212353990191506 | Epoch(2977/3000) | Training loss: 2.1212353990191506 | Epoch(2978/3000) | Training loss: 2.1212353763126193 | Epoch(2979/3000) | Training loss: 2.121235418887365 | Epoch(2980/3000) | Training loss: 2.121235421725682 | Epoch(2981/3000) | Training loss: 2.1212354046957835 | Epoch(2982/3000) | Training loss: 2.121235416049049 | Epoch(2983/3000) | Training loss: 2.1212353763126193 | Epoch(2984/3000) | Training loss: 2.121235410372416 | Epoch(2985/3000) | Training loss: 2.1212354046957835 | Epoch(2986/3000) | Training loss: 2.121235410372416 | Epoch(2987/3000) | Training loss: 2.1212353990191506 | Epoch(2988/3000) | Training loss: 2.1212353990191506 | Epoch(2989/3000) | Training loss: 2.1212353990191506 | Epoch(2990/3000) | Training loss: 2.121235410372416 | Epoch(2991/3000) | Training loss: 2.1212353905042014 | Epoch(2992/3000) | Training loss: 2.1212353706359863 | Epoch(2993/3000) | Training loss: 2.1212353905042014 | Epoch(2994/3000) | Training loss: 2.1212353706359863 | Epoch(2995/3000) | Training loss: 2.1212353933425176 | Epoch(2996/3000) | Training loss: 2.1212353706359863 | Epoch(2997/3000) | Training loss: 2.1212353990191506 | Epoch(2998/3000) | Training loss: 2.1212354046957835 | Epoch(2999/3000) | Training loss: 2.121235381989252 | Epoch(3000/3000) | Training loss: 2.121235410372416 | "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqNpkYO6V9YI"
      },
      "source": [
        "- Has the accuracy of the model increased?\n",
        "- Now plot the training loss vs the test loss over 30 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89a8FdTi-cNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "f34b59d0-192c-4051-b5c4-aeb32d18c1ac"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(train_tracker, label='Training loss')\n",
        "%plt.plot(test_tracker, label='Test loss')\n",
        "plt.legend()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%plt.plot` not found.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFgoHaXdTwQI"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHtKTHKKjG3r"
      },
      "source": [
        "- Now add the accuracy to the mix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJgyMHm2Pvx5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "643621e4-d479-4d61-ba6b-8b72564a31f7"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(train_tracker, label='Training loss')\n",
        "%plt.plot(test_tracker, label='Test loss')\n",
        "%plt.plot(accuracy_tracker, label='Test accuracy')\n",
        "plt.legend()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%plt.plot` not found.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD4CAYAAAAQP7oXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAe50lEQVR4nO3dfZRdVZ3m8e9TLyliihgihWBeDDQoHTFNIGJmELqnXUJ0GMG2bXExgDqrGRQHsoRxCYy2Y+tabcswq9VuY1yI2h2kezpkpMcwEEawG4dEilCSN14CyoSQmECABGJeKvWbP+6u1Lkv595zKgn1wvNZ6646tc/e5+xdt1JP9j7n3quIwMzMrIi2ke6AmZmNHQ4NMzMrzKFhZmaFOTTMzKwwh4aZmRXWMdIdONKOPfbYmDVr1kh3w8xsTHn44Yefj4ie2vJxHxqzZs2it7d3pLthZjamSHqmUbmXp8zMrDCHhpmZFdYyNCTNkHSfpPWS1km6pkGdUyU9KGmvpOuKtJX055IeldQn6R5Jb0nlkvQNSRvT/jMybS6X9GR6XH7owzczszKKzDT6gWsjYjYwH7hK0uyaOjuAq4GbSrT9ekTMiYjTgf8FfDGVvx84JT2uAL4NIGkq8GfAu4GzgD+TdEzhkZqZ2SFrGRoRsSUiVqftXcAGYFpNnW0R8RCwv2jbiNiZqToJGHwTrAuBH0bFSmCKpBOA84EVEbEjIl4EVgALyg7YzMyGr9TdU5JmAXOBVWVP1KitpK8ClwEvA/8mFU8DNmWaPpvK8sobnesKKrMUZs6cWbarZmaWo/CFcEndwFJgYc0sYdhtI+LGiJgBLAE+U+aYzUTE4oiYFxHzenrqbjM2M7NhKhQakjqp/NFfEhF3lDlBwbZLgA+n7c3AjMy+6aksr/yI+MH//TX/9MvnjtThzczGpCJ3Twm4BdgQETeXOXiztpJOyXx7IfBY2r4TuCzdRTUfeDkitgB3A+dJOiZdAD8vlR0Rf7fyGe5au+VIHd7MbEwqck3jbOBSYI2kvlR2AzATICIWSToe6AUmAwOSFgKzgTmN2kbEcuAvJL0dGACeAa5M+5cDHwA2AruBT6Tz7JD058BDqd6XI2LH8IZdjD+fysysWsvQiIgHALWos5XKclGt3LYR8eGc8gCuytn3PeB7zfpyuEgODTOzWn5FeA41z0kzs9clh0YTgacaZmZZDo0cXp4yM6vn0DAzs8IcGk14omFmVs2hkUOSl6fMzGo4NHL43ikzs3oOjaY81TAzy3Jo5PDdU2Zm9RwaOeT1KTOzOg6NJjzRMDOr5tDIIUR4fcrMrIpDI4eXp8zM6jk0mvA8w8ysmkMjh/DdU2ZmtRwaebw+ZWZWx6HRhCcaZmbVHBo5KstTjg0zsyyHRg6vTpmZ1XNomJlZYS1DQ9IMSfdJWi9pnaRrGtQ5VdKDkvZKuq5IW0lfl/SYpEclLZM0JZVfIqkv8xiQdHrad7+kxzP7jjs8P4YG4z5SBzYzG8OKzDT6gWsjYjYwH7hK0uyaOjuAq4GbSrRdAZwWEXOAJ4DrASJiSUScHhGnA5cCv4qIvswxLxncHxHbig+1PF/SMDOr1jI0ImJLRKxO27uADcC0mjrbIuIhYH/RthFxT0T0p6orgekNTv8x4PZSIzpMJBG+f8rMrEqpaxqSZgFzgVVlT9Si7SeBuxqUfxT4UU3ZrWlp6gtS48vVkq6Q1Cupd/v27WW7WjnGsFqZmY1vhUNDUjewFFgYETvLnKRZW0k3UlnGWlJT/m5gd0SszRRfEhHvBM5Jj0sbnS8iFkfEvIiY19PTU6arNccZdlMzs3GpUGhI6qTyR39JRNxR5gTN2kr6OHABlTCo/RN9MTWzjIjYnL7uAm4DzirTlzL8IUxmZvU6WlVIS0C3ABsi4uYyB2/WVtIC4HPA70fE7pp9bcCfUJlNDJZ1AFMi4vkURBcA95bpT6m+e4HKzKxOy9AAzqayDLRG0uBdTDcAMwEiYpGk44FeYDIwIGkhMBuY06htRCwHvgV0ASvSpYmVEXFlqnMusCkins70owu4OwVGO5XA+O4wxlyYL4SbmVVrGRoR8QAtrgtHxFYa3/2U2zYiTm5yvPup3KKbLXsVOLNFdw8fL0+ZmdXxK8JzeHHKzKyeQ6MJTzTMzKo5NHJIODXMzGo4NHL47ikzs3oOjSZ895SZWTWHRg6/uM/MrJ5DI4c/hMnMrJ5DowlPNMzMqjk0cgj5M8LNzGo4NHJ4ecrMrJ5DownPM8zMqjk0mvDqlJlZNYdGjpwPBTQze11zaDThiYaZWTWHRg6B16fMzGo4NHJ4dcrMrJ5DownPM8zMqjk0cniiYWZWz6HRhC9pmJlVaxkakmZIuk/SeknrJF3ToM6pkh6UtFfSdUXaSvq6pMckPSppmaQpqXyWpN9K6kuPRZk2Z0paI2mjpG/oCN4XK8lvjW5mVqPITKMfuDYiZgPzgaskza6pswO4GripRNsVwGkRMQd4Arg+0+6piDg9Pa7MlH8b+FPglPRYUKD/w+LlKTOzei1DIyK2RMTqtL0L2ABMq6mzLSIeAvYXbRsR90REf6q6EpjerB+STgAmR8TKqLyT4A+Bi1oPcfi8PGVmVq3UNQ1Js4C5wKqyJ2rR9pPAXZnvT5T0iKSfSTonlU0Dns3UeZaa8Mqc6wpJvZJ6t2/fXrar6RgODTOzWoVDQ1I3sBRYGBE7y5ykWVtJN1JZxlqSirYAMyNiLvBZ4DZJk8ucLyIWR8S8iJjX09NTpmm2Z8NsZ2Y2fnUUqSSpk8of/SURcUeZEzRrK+njwAXAe9OSExGxF9ibth+W9BTwNmAz1UtY01PZEeOJhplZtSJ3Twm4BdgQETeXOXiztpIWAJ8DPhgRuzPlPZLa0/ZJVC54Px0RW4Cdkuan414G/LhMf8r1HX8Ik5lZjSIzjbOBS4E1kvpS2Q3ATICIWCTpeKAXmAwMSFoIzAbmNGobEcuBbwFdwIp05+zKdKfUucCXJe0HBoArI2JHavtp4PvARCrXQLLXQQ4rL06ZmdVrGRoR8QAt/oZGxFYa3/2U2zYiTs4pX0plOavRvl7gtGZ9MTOzI8evCM/hu6fMzOo5NHLIC1RmZnUcGk34bUTMzKo5NHJ4ecrMrJ5DI4c/hMnMrJ5DowlPNMzMqjk0cgj5xX1mZjUcGnm8PGVmVseh0YTnGWZm1RwaOQRODTOzGg6NHEfwk2TNzMYsh0YTnmiYmVVzaOTwPMPMrJ5DownfcmtmVs2hkUPy8pSZWS2HRg4vT5mZ1XNoNOHVKTOzag6NHJL81uhmZjUcGjm8PGVmVq9laEiaIek+SeslrZN0TYM6p0p6UNJeSdcVaSvp65Iek/SopGWSpqTy90l6WNKa9PUPM23ul/S4pL70OO7QfwT5vDxlZlatyEyjH7g2ImYD84GrJM2uqbMDuBq4qUTbFcBpETEHeAK4PpU/D/y7iHgncDnwtzXHvCQiTk+PbQX6Pzz+ECYzszotQyMitkTE6rS9C9gATKupsy0iHgL2F20bEfdERH+quhKYnsofiYjnUvk6YKKkrmGOb9j8GeFmZvVKXdOQNAuYC6wqe6IWbT8J3NWg/MPA6ojYmym7NS1NfUF+gygzs9dU4dCQ1A0sBRZGxM4yJ2nWVtKNVJaxltSUvwP4GvAfM8WXpGWrc9Lj0pzzXSGpV1Lv9u3by3Q1cwy/ItzMrFah0JDUSeWP/pKIuKPMCZq1lfRx4AIqYRCZ8unAMuCyiHhqsDwiNqevu4DbgLManTMiFkfEvIiY19PTU6a7Q30bViszs/GtyN1TAm4BNkTEzWUO3qytpAXA54APRsTuTPkU4CfA5yPi55nyDknHpu1OKmGztkx/yvI8w8ysWkeBOmdTWQZaI6kvld0AzASIiEWSjgd6gcnAgKSFwGxgTqO2EbEc+BbQBaxIlyZWRsSVwGeAk4EvSvpianMe8CpwdwqMduBe4LvDHnkL8t1TZmZ1WoZGRDxAi9WaiNhKuvupRm7biDg5p/wrwFdyTnVms34cTr57ysysnl8R3oTfRsTMrJpDI4eXp8zM6jk0cvgVIGZm9RwaTXiiYWZWzaGRS16eMjOr4dDI4eUpM7N6Do2mPNUwM8tyaOTwRMPMrJ5Dowlf0zAzq+bQyCF5ccrMrJZDI4ffRsTMrJ5Dowl/noaZWTWHRg4vT5mZ1XNo5PDilJlZPYdGE16dMjOr5tDIIcnXNMzMajg0zMysMIdGE55nmJlVc2jkkHBqmJnVcGjk8Iv7zMzqtQwNSTMk3SdpvaR1kq5pUOdUSQ9K2ivpuiJtJX1d0mOSHpW0TNKUzL7rJW2U9Lik8zPlC1LZRkmfP7Sht+aJhplZtSIzjX7g2oiYDcwHrpI0u6bODuBq4KYSbVcAp0XEHOAJ4HqAtP9i4B3AAuBvJLVLagf+Gng/MBv4WIN+HDaVzwh3bJiZZbUMjYjYEhGr0/YuYAMwrabOtoh4CNhftG1E3BMR/anqSmB62r4QuD0i9kbEr4CNwFnpsTEino6IfcDtqe4R4cUpM7N6pa5pSJoFzAVWlT1Ri7afBO5K29OATZl9z6ayvPJG57pCUq+k3u3bt5ft6kGeZ5iZVSscGpK6gaXAwojYWeYkzdpKupHKMtaSMsdsJiIWR8S8iJjX09MzrGNUlqcOV4/MzMaHjiKVJHVS+aO/JCLuKHOCZm0lfRy4AHhvDF1A2AzMyFSbnspoUn7YyR8SbmZWp8jdUwJuATZExM1lDt6sraQFwOeAD0bE7syuO4GLJXVJOhE4BfgF8BBwiqQTJU2gcrH8zjL9KSu8QGVmVqXITONs4FJgjaS+VHYDMBMgIhZJOh7oBSYDA5IWUrnDaU6jthGxHPgW0AWsSP+rXxkRV0bEOkn/AKynsmx1VUQcAJD0GeBuoB34XkSsO7Th5xNenjIzq9UyNCLiAVrcTBQRWxm6+ykrt21EnNzkeF8FvtqgfDmwvFlfDhuvTpmZ1fErwpvwRMPMrJpDI4fwR/eZmdVyaOSofNyrU8PMLMuhkaPNr9MwM6vj0MjRJjHg1DAzq+LQyCGJAWeGmVkVh0aOtnTLrd/p1sxsiEMjR1t6GxHPNszMhjg0cgzONHxdw8xsiEMjhw7ONBwaZmaDHBo5BpennBlmZkMcGjnk5SkzszoOjRxDd0+NbD/MzEYTh0aONl/TMDOr49DIId9ya2ZWx6GRwy/uMzOr59DI4Rf3mZnVc2jk8Iv7zMzqOTRy+MV9Zmb1WoaGpBmS7pO0XtI6Sdc0qHOqpAcl7ZV0XZG2kj6SygYkzcuUXyKpL/MYkHR62ne/pMcz+4479B9BY35xn5lZvY4CdfqBayNitaSjgYclrYiI9Zk6O4CrgYtKtF0L/BHwnWyDiFgCLAGQ9E7gf0ZEX6bKJRHRW2KMw+LlKTOzei1nGhGxJSJWp+1dwAZgWk2dbRHxELC/aNuI2BARj7c4/ceA2wuO5bAaekX4SJzdzGx0KnVNQ9IsYC6wquyJhtn2o8CPaspuTUtTX9DghYf6c10hqVdS7/bt28t2dfAYgG+5NTPLKhwakrqBpcDCiNhZ5iTDaSvp3cDuiFibKb4kIt4JnJMelzZqGxGLI2JeRMzr6ekp09WDfE3DzKxeodCQ1Enlj/6SiLijzAkOoe3F1MwyImJz+roLuA04q0xfyvA1DTOzekXunhJwC7AhIm4uc/DhtpXUBvwJmesZkjokHZu2O4ELqFxMPyL84j4zs3pF7p46m8oy0BpJg3cx3QDMBIiIRZKOB3qBycCApIXAbGBOo7YRsVzSh4BvAj3ATyT1RcT5qc65wKaIeDrTjy7g7hQY7cC9wHeHNeoC/NboZmb1WoZGRDwANLzgnKmzFZjeYFdu24hYBizL2Xc/ML+m7FXgzFb9PVzafCHczKyOXxGew8tTZmb1HBo5fCHczKyeQyPHwWsaAyPbDzOz0cShkcNvWGhmVs+hkaOt8YvNzcxe1xwaOXxNw8ysnkMjh++eMjOr59DI4Rf3mZnVc2jk8Iv7zMzqOTRyeHnKzKyeQyPHwQvhTg0zs4McGjnkmYaZWR2HRo7BC+G+pmFmNsShkePghfAR7oeZ2Wji0MjhF/eZmdVzaORoS6nR74saZmYHOTRydLZVfjT9BxwaZmaDHBo5OjvSTOOA3xvdzGyQQyNHR5pp7HNomJkd5NDI0dk+ONPw8pSZ2aCWoSFphqT7JK2XtE7SNQ3qnCrpQUl7JV1XpK2kj6SyAUnzMuWzJP1WUl96LMrsO1PSGkkbJX1DOnIfetHRnq5p+KP7zMwO6ihQpx+4NiJWSzoaeFjSiohYn6mzA7gauKhE27XAHwHfaXDOpyLi9Abl3wb+FFgFLAcWAHcVGENpnenuqf2eaZiZHdRyphERWyJiddreBWwAptXU2RYRDwH7i7aNiA0R8XjRjko6AZgcESuj8jLtH1IfUodNZ5pp7Pc1DTOzg0pd05A0C5hL5X/6pZRse6KkRyT9TNI5qWwa8GymzrPUhFfmXFdI6pXUu3379rJdBaDD1zTMzOoUDg1J3cBSYGFE7CxzkpJttwAzI2Iu8FngNkmTy5wvIhZHxLyImNfT01Om6UEHZxq+pmFmdlCRaxpI6qTyR39JRNxR5gRl20bEXmBv2n5Y0lPA24DNwPRM1emp7IjoaPNMw8ysVpG7pwTcAmyIiJvLHHw4bSX1SGpP2ycBpwBPR8QWYKek+em4lwE/LtOfMtrbhORrGmZmWUVmGmcDlwJrJPWlshuAmQARsUjS8UAvMBkYkLQQmA3MadQ2IpZL+hDwTaAH+Imkvog4HzgX+LKk/cAAcGVE7EhtPw18H5hI5a6pI3LnFFQ+T6Ozrc13T5mZZbQMjYh4AGj6eoiI2Er10tGg3LYRsQxY1qB8KZXlrEZteoHTWnT5sOlol99GxMwsw68Ib6KjTV6eMjPLcGg0MXFCO3v2OzTMzAY5NJro7urglb39I90NM7NRw6HRhEPDzKyaQ6OJ7qMcGmZmWQ6NJrq7Onhlj0PDzGyQQ6OJ7q5Odu7Z37qimdnrhEOjiRPeeBTbdu31azXMzBKHRhPTj5nIgYFgy8t7RrorZmajgkOjibcffzQAfZteGuGemJmNDg6NJuZMn8KUN3Tyk0e3jHRXzMxGBYdGE+1t4rL5b+V/r9vKvzw5vA9zMjMbTxwaLXzqD07mbW/u5prb+/jV86+OdHfMzEaUQ6OFiRPaWfTvzwTg0ltWsWnH7hHukZnZyHFoFHBSTzff/8S72LWnnw/9zc955P+9ONJdMjMbEQ6NguZMn8LST/1rJk5o5yOLHuSv7n3Sb5tuZq87Do0STj6umzuveg8feOcJ/Pd7n+B9N/+Mf/rlcxwY8Kf7mdnrg0OjpGMmTeAbH5vLrR9/F10d7fynHz3CuX95H9/66ZNs2+UXAZrZ+KaI8f2/5Hnz5kVvb+8ROfaBgeCedVv5u1XP8PONL9AmmPfWqZx/2vH8wdt7OPFNk2hra/pJuWZmo5KkhyNiXl15q9CQNAP4IfBmIIDFEfFXNXVOBW4FzgBujIibWrWV9BHgS8DvAmelz/9G0vuAvwAmAPuA/xwRP0377gdOAH6bTn1eRGxr1v8jGRpZT29/hR/3Pcfd67by2NZdAEw+qoPfmzGFuTOm8HvpcWx31xHvi5nZoTqU0DgBOCEiVks6GngYuCgi1mfqHAe8FbgIeDETGrltJf0uMAB8B7guExpzgd9ExHOSTgPujohpad/92bpFvFahkfXr519l1a9eoG/TS/RtepnHt+5k8LLHtCkT+Z3juplxzERmTn0DM6a+genHTOTY7i6mTprAUZ3tr2lfzcwayQuNjlYNI2ILsCVt75K0AZgGrM/U2QZsk/Rvi7aNiA2pY7XneyTz7TpgoqSuiNhbZKCjwaxjJzHr2El89F0zAdi9r5+1m3fSt+lFHn32ZZ55YTePPvsSL+2uf9v1SRPaeVMKkDdNmsDUSROY2j2BKRMn0N3VzqSuDrrTY1JXB0d1tnNUZ1vla0c7XZ1tdHW01f1czcwOh5ahkSVpFjAXWFX2RMNs+2FgdU1g3CrpALAU+Eo0mCpJugK4AmDmzJllu3rYvWFCB2edOJWzTpxaVb5zz3427djNsy/+lhde2ceOV/fywqv72JEeW17ew7rndrLj1X3sK3F7rwRHdbQzoaONzvZKiHS2i/a2wUcb7W3QrmxZZp84WKejrY22NlWVVe2T6GgXbdLQvkxZR5toaxv62t6grKNtqG57pl6bAEGbhIC2tspXSQxmog6OWZltGPwum51DbYb2Zcvqjzn0XXbfYCAr55h152vRt8xpcvqhmj6lY2ioLFu3aN8a9SPbfuhn5P+A2JDCoSGpm8of6oURsbPMSYbTVtI7gK8B52WKL4mIzWmpaylwKZVrJlUiYjGwGCrLU2X6+lqafFQn73jLG3nHW97YtF5EsGf/AK/s7eeVvf28mvm6Z/8Ae/YfYE//AX677wB7+9P3+w+wr3+AfQeCff0D7D8wwIEIDhyIyteBmkcE+/cPcGDgQF15o7p1ZZlyG79KB1ptOfkhR6PwahFoNDxmg343qNd4fPl7G/2HoPb4eecf6mnxcyr3m+ayVZdfcw5dHYd3ybtQaEjqpPJHeklE3FHmBMNpK2k6sAy4LCKeGiyPiM3p6y5JtwFn0SA0xhtJTJzQzsQJ7fQcPbovpEcEA0F1kKSg6h8YYGCAmvAa4MAAB/f1DwwwEMGBgaFjBUEERMBABANpcnkwnlKdyvkrj+z+iMhsDzXK1hvazhwn056a9pFtX3WcFn1rdpzMjmy92mOW6ltVm6iql20/7L7VHLPuZ9nouSjbt9pjNuxvTT9i6I93o/E0Uv87kt0X9RUbfJt3jTjvtHn9KXLMIudRmbQpqGVoqBKDtwAbIuLmMgcfTltJU4CfAJ+PiJ9nyjuAKRHxfAqiC4B7y/THjjxpcCnLSxpm41GRu6feA/wLsIbK3U4ANwAzASJikaTjgV5gcqrzCjAbmNOobUQsl/Qh4JtAD/AS0BcR50v6L8D1wJOZbpwHvAr8M9AJtFMJjM9GxIFm/R+Ju6fMzMa6Yd9yO9Y5NMzMyssLDb+NiJmZFebQMDOzwhwaZmZWmEPDzMwKc2iYmVlhDg0zMyts3N9yK2k78Mwwmx8LPH8YuzOSxstYxss4wGMZrcbLWA51HG+NiJ7awnEfGodCUm+j+5THovEylvEyDvBYRqvxMpYjNQ4vT5mZWWEODTMzK8yh0dzike7AYTRexjJexgEey2g1XsZyRMbhaxpmZlaYZxpmZlaYQ8PMzApzaDQgaYGkxyVtlPT5ke5PEZJ+LWmNpD5JvalsqqQVkp5MX49J5ZL0jTS+RyWdMcJ9/56kbZLWZspK913S5an+k5IuH0Vj+ZKkzem56ZP0gcy+69NYHpd0fqZ8RH8HJc2QdJ+k9ZLWSbomlY+556XJWMbi83KUpF9I+mUay39N5SdKWpX69feSJqTyrvT9xrR/VqsxtlT5+EU/Bh9UPuDpKeAkYALwS2D2SPerQL9/DRxbU/aXVD4BEeDzwNfS9geAu6h8nPB8YNUI9/1c4Axg7XD7DkwFnk5fj0nbx4ySsXwJuK5B3dnp96sLODH93rWPht9B4ATgjLR9NPBE6u+Ye16ajGUsPi8CutN2J7Aq/bz/Abg4lS8CPpW2Pw0sStsXA3/fbIxF+uCZRr2zgI0R8XRE7ANuBy4c4T4N14XAD9L2D4CLMuU/jIqVwBRJJ4xEBwEi4p+BHTXFZft+PrAiInZExIvACmDBke99tZyx5LkQuD0i9kbEr4CNVH7/Rvx3MCK2RMTqtL0L2ABMYww+L03Gkmc0Py8REa+kbzvTI4A/BP4xldc+L4PP1z8C75Uk8sfYkkOj3jRgU+b7Z2n+CzZaBHCPpIclXZHK3hwRW9L2VuDNaXssjLFs30f7mD6Tlm2+N7ikwxgZS1rSmEvlf7Vj+nmpGQuMwedFUrukPmAblRB+CngpIvob9Otgn9P+l4E3cQhjcWiMH++JiDOA9wNXSTo3uzMqc9IxeX/1WO578m3gd4DTgS3AfxvZ7hQnqRtYCiyMiJ3ZfWPteWkwljH5vETEgYg4HZhOZXZw6mt5fodGvc3AjMz301PZqBYRm9PXbcAyKr9Mvxlcdkpft6XqY2GMZfs+ascUEb9J/9AHgO8ytAwwqsciqZPKH9klEXFHKh6Tz0ujsYzV52VQRLwE3Af8KyrLgR0N+nWwz2n/G4EXOISxODTqPQScku5GmEDl4tGdI9ynpiRNknT04DZwHrCWSr8H71a5HPhx2r4TuCzd8TIfeDmz5DBalO373cB5ko5JywznpbIRV3O96ENUnhuojOXidIfLicApwC8YBb+Dad37FmBDRNyc2TXmnpe8sYzR56VH0pS0PRF4H5VrNPcBf5yq1T4vg8/XHwM/TTPEvDG29lpe+R8rDyp3gjxBZa3wxpHuT4H+nkTlTohfAusG+0xl7fL/AE8C9wJTU7mAv07jWwPMG+H+/4jK8sB+Kmur/2E4fQc+SeWC3kbgE6NoLH+b+vpo+sd6Qqb+jWksjwPvHy2/g8B7qCw9PQr0pccHxuLz0mQsY/F5mQM8kvq8FvhiKj+Jyh/9jcD/ALpS+VHp+41p/0mtxtjq4bcRMTOzwrw8ZWZmhTk0zMysMIeGmZkV5tAwM7PCHBpmZlaYQ8PMzApzaJiZWWH/HwrD8Sf+0ApWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APhVglVTk-og"
      },
      "source": [
        "## Further challenges and experiments\n",
        "- Can you get better accuracy from a model if you :\n",
        "    - Add more layers?\n",
        "    - Change the number of nodes in the layers?\n",
        "    - Train over fewer/higher epochs?\n",
        "    \n",
        "- Can you improve on your results if you add additional layers like [Dropout](https://pytorch.org/docs/master/nn.html#torch.nn.Dropout)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8FzAY7jG3v"
      },
      "source": [
        "output = open(\"uin2pressure\", mode=\"wb\")\n",
        "torch.save(model.state_dict(), output)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ImOG4oXjG3x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwHV3ig7jG3y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg3aU2vJjG30"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UfGdMuCjG32"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxOzS2-TjG34"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kdUCpCyjG35"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMhC9JJujG37"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}